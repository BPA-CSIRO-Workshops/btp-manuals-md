{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Some welcome text should appear here.</p> <p>Perhaps the logos should be added here as well?</p>"},{"location":"preamble/","title":"Workshop Information","text":""},{"location":"preamble/#providing-feedback","title":"Providing Feedback","text":"<p>While we endeavour to deliver a workshop with quality content and documentation in a venue conducive to an exciting, well run hands-on workshop with a bunch of knowledgeable and likable trainers, we know there are things we could do better.</p> <p>Whilst we want to know what didn\u2019t quite hit the mark for you, what would be most helpful and least depressing, would be for you to provide ways to improve the workshop. i.e. constructive feedback. After all, if we knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put it into the workshop in the first place! Remember, we\u2019re experts in the field of bioinformatics not experts in the field of biology!</p> <p>Clearly, we also want to know what we did well! This gives us that \u201cfeel good\u201d factor which will see us through those long days and nights in the lead up to such hands-on workshops!</p> <p>With that in mind, we\u2019ll provide three really high tech mechanism through which you can provide anonymous feedback during the workshop:</p> <ol> <li> <p>A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a \u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your mission is to see how many comments you can stick on the \u201chappy\u201d side!</p> </li> <li> <p>Some empty ruled pages at the back of this handout. Use them for your own personal notes or for write specific comments/feedback about the workshop as it progresses.</p> </li> <li> <p>An online post-workshop evaluation survey. We\u2019ll ask you to complete this before you leave. If you\u2019ve used the blank pages at the back of this handout to make feedback notes, you\u2019ll be able to provide more specific and helpful feedback with the least amount of brain-drain!</p> </li> </ol>"},{"location":"preamble/#document-structure","title":"Document Structure","text":"<p>We have provided you with an electronic copy of the workshop\u2019s hands-on tutorial documents. We have done this for two reasons: 1) you will have something to take away with you at the end of the workshop, and 2) you can save time (mis)typing commands on the command line by using copy-and-paste.</p>  <p>While you could fly through the hands-on sessions doing copy-and-paste you will learn more if you take the time, saved from not having to type all those commands, to understand what each command is doing!</p>  <p>The commands to enter at a terminal look something like this:</p> <pre><code>tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n</code></pre> <p>The following styled code is not to be entered at a terminal, it is simply to show you the syntax of the command. You must use your own judgement to substitute in the correct arguments, options, filenames etc</p> <pre><code>tophat [options]* &lt;index_base&gt; &lt;reads_1&gt; &lt;reads_2&gt;\n</code></pre> <p>The following is an example how of R commands are styled:</p> <pre><code>R --no-save\nlibrary(plotrix)\ndata &lt;- read.table(\"run_25/stats.txt\", header=TRUE)\nweighted.hist(data$short1_cov+data$short2_cov, data$lgth, breaks=0:70)\nq()\n</code></pre> <p>The following icons are used throughout the documentation to help you navigate around the document more easily:</p>  <p>Question</p> <p>Questions to answer.</p>   <p>Answer</p> <p>Answers will be provided at the end of the workskop.</p>   <p>Important</p> <p>This is important. </p>   <p>STOP</p> <p>Warning - STOP and read.</p>   <p>Bonus exercise</p> <p>Bonus exercise for fast learners.</p>   <p>Advanced exercise</p> <p>Advanced exercise for super-fast learners</p>"},{"location":"preamble/#resources-used","title":"Resources Used","text":"<p>We have provided you with an environment which contains all the tools and data you need for the duration of this workshop. However, we also provide details about the tools and data used by each module at the start of the respective module documentation.</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/","title":"ChIP-seq","text":""},{"location":"modules/btp-module-chip-seq/chip-seq/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched     areas using the chosen R package: ChIP-seq processing pipeline (SPP)</p> </li> <li> <p>Visualize the peak regions through a genome browser, e.g. IGV or     Ensembl, and identify the real peak regions</p> </li> <li> <p>Perform functional annotation using biomaRt R package and detect     potential binding sites (motif) in the predicted binding regions     using motif discovery tool, e.g. Trawler or MEME.</p> </li> </ul>"},{"location":"modules/btp-module-chip-seq/chip-seq/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/btp-module-chip-seq/chip-seq/#tools-used","title":"Tools Used","text":"<p>SPP:  http://compbio.med.harvard.edu/Supplements/ChIP-seq/</p> <p>IGV: http://software.broadinstitute.org/software/igv/</p> <p>Ensembl: http://www.ensembl.org</p> <p>Trawler: https://trawler.erc.monash.edu.au/index.html</p> <p>RSAT peak-motifs:   http://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi</p> <p>MEME-ChIP: http://meme-suite.org/tools/meme-chip</p> <p>TOMTOM: http://meme.ebi.edu/meme/cgi-bin/tomtom.cgi</p> <p>DAVID: http://david.abcc.ncifcrf.gov</p> <p>GOstat: http://gostat.wehi.edu.au</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#sources-of-data","title":"Sources of Data","text":"<p>http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#author-information","title":"Author Information","text":"<p>Primary Author(s):     Sonika Tyagi sonika.tyagi@agrf.org.au      Sean Li sean.li@anu.edu.au </p> <p>Contributor(s):     Mirana Ramialison mirana.ramialison@monash.edu      Markus Tondl markus.tondl@monash.edu </p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to perform some basic tasks in the analysis of ChIP-seq data. In fact, you already performed the first step, alignment of the reads to the genome, in the previous session. We start from the aligned reads and we will find immuno-enriched areas using SPP. We will visualize the identified regions in a genome browser and perform functional annotation and motif analysis on the predicted binding regions.</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#prepare-the-environment","title":"Prepare the Environment","text":"<p>The material for this practical can be found in the <code>ChIPseq</code> directory on your desktop. Please make sure that this directory also contains the SAM/BAM files you produced during the alignment practical.</p> <p>If you didn\u2019t have time to align the control file called <code>gfp.fastq</code> during the alignment practical, please do it now. Follow the same steps, from the bowtie alignment step, as for the <code>Oct4.fastq</code> file.</p> <p>In ChIP-seq analysis (unlike in other applications such as RNA-seq) it can be useful to exclude all reads that map to more than one location in the genome. When using Bowtie, this can be done using the <code>-m 1</code> option, which tells it to report only unique matches (See <code>bowtie \u2013help</code> for more details).</p> <p>Open the Terminal and go to the <code>chipseq</code> directory:</p> <pre><code>cd /home/trainee/chipseq\nls\nR\n</code></pre>"},{"location":"modules/btp-module-chip-seq/chip-seq/#finding-enriched-areas-using-spp","title":"Finding enriched areas using SPP","text":"<p>Terminology used in the tutorial: </p> <ul> <li><code>fragment:</code> overlapping fragments     obtaining in the IP (immuno precipitation) experiments. </li> <li><code>tag:</code> sequenced part of the fragment which could be from one end (in case of single end     sequencing ) or both ends in the paired end data. </li> <li><code>alignment:</code> a process to determine the position of the tags, which typically should be around     the binding site. </li> <li><code>peaks:</code> spatial distribution of the tags densities     around the binding sites on the genome. You would see two separate peaks     of tags on the positive and negative strand around the binding site. The     distance between the two peaks should reflect the size of the protected     region.</li> </ul> <p>SPP is a Chip-seq processing pipeline implemented using R.</p> <p>The main functions of SPP include locating quality tag alignment by screening overall DNA-binding signals, removing or restricting certain positions with extremely high number of tags, estimating significant enrichment regions through genome-wide profiling, providing appropriate outputs for visualization, and determining statistically significant binding positions with saturation criteria assessment. Moreover, the processing of ChIP-seq data can require considerable amount of CPU time, it is often necessary to make use of parallel processing. SPP supports parallel processing if the cluster option is configured. Since our example data is relatively small, we will use single CPU and omit the cluster parameters for simplicity. The following steps will work you through the SPP pipeline.</p> <p>In your R terminal, load spp and biomaRt packages and make sure to set your working directory correctly:</p> <pre><code>library(spp);\nlibrary(biomaRt);\nsetwd('/home/trainee/chipseq');\n</code></pre> <p>The first stage in SPP are: 1.    load input data;  2.    choose alignment quality and  3.    remove anomalies. </p> <p>SPP can read output from the following aligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format and BAM format (Note: because BAM standard doesn\u2019t specify a flag for a uniquely-mapped read, the aligner has to generate a BAM file that would contain only unique reads.)</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#step-1-loading-data-and-quality-filter-the-informative-tags","title":"STEP 1. Loading data and quality filter the informative tags","text":"<p>First load Oct4 and gfp bam files. Here GFP are the control or input samples, these are usually mock IP DNA where you do not expect to see any binding peaks.</p> <pre><code>oct4.data&lt;-read.bam.tags(\"Oct4.sorted.bam\");\ngfp.data&lt;-read.bam.tags(\"gfp.sorted.bam\");\n</code></pre> <p>The statistical significance of tags clustering observed for a putative protein binding site depends on the expected background. Therefore, use of a input or control DNA is highly recommended in the experiment design. This provides an experimental assessment of the background tag distribution.</p> <p>The next step uses cross-correlation profile to calculate binding peak separation distance, and assess whether inclusion of tags with non-perfect alignment quality improves the cross-correlation peak. This is done by shifting the strands relative to each other by increasing distance within a given range. cross-correlation of the positive and negative strand tag densities is plotted. The cross-correlation plot should show the predominant size of the protected region.</p> <pre><code>binding.characteristics &lt;- get.binding.characteristics(oct4.data,srange=c(50,500),bin=5);\n</code></pre> <p>The <code>binding.characteristics</code> provides the estimate of the binding peak</p> <p>separation distance, cross-correlation profile itself and tag quality bin acceptance information. The <code>srange</code> parameter defines the possible range for the size of the protected region. It is supposed to be higher than tag length. However, the upper boundary (500) cannot be too high, which will increase the running time. The bin parameter tags within the specified number of base pairs to speed up calculation. The increase of bin size will decrease the accuracy of the determined parameters.</p> <p>Then, print out binding peak separation distance and we can plot cross-correlation profile:</p>  <p>STOP</p> <p>DO NOT run this command, we dont have enough data to generate this plot.</p> <p>print(paste(\u201cbinding peak separation distance=\u201d,binding.characteristics$peak$x)); pdf(file=\u201doct4.crosscorrelation.pdf\u201d,width=5,height=5); par(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8); plot(binding.characteristics$cross.correlation,type=\u2019l\u2019,xlab=\u201dstrand shift\u201d,ylab=\u201dcross-correlation\u201d); abline(v=binding.characteristics$peak$x,lty=2,col=2); dev.off();</p>  <p>A set of tags informative about the binding positions should increase cross correlation magnitude whereas a randonmly mapped set of tags should decrease it. The following calls will select tags with acceptable alignment quality based on the binding characteristics:</p> <pre><code>chip.data &lt;- select.informative.tags(oct4.data`binding.characteristics);\ngfpcontrol.data &lt;- select.informative.tags(gfp.data,binding.characteristics);\n</code></pre> <p>The last step below will scan along the chromosomes calculating local density of region (can be specified using <code>window.size</code> parameter, default is 200bp), removing or restricting singular positions with extremely high tag count relative to the neighborhood:</p> <pre><code>chip.data &lt;- remove.local.tag.anomalies(chip.data);\ngfpcontrol.data &lt;- remove.local.tag.anomalies(gfpcontrol.data);\n</code></pre>"},{"location":"modules/btp-module-chip-seq/chip-seq/#step-2-calculating-genome-wide-tag-density-and-tag-enrichmentdepletion-profiles","title":"STEP 2. Calculating genome-wide tag density and tag enrichment/depletion profiles","text":"<p>The following commands will calculate smoothed tag density and output it into a WIG file that can be read with genome browsers, such as IGV (Note: the tags are shifted by half of the peak separation distance):</p> <pre><code>tag.shift &lt;- round(binding.characteristics$peak$x/2)\nsmoothed.density &lt;- get.smoothed.tag.density(chip.data,control.tags=gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift);\nwritewig(smoothed.density,\"oct4.density.wig\",\"Smoothed, background-subtracted tag density\");\nrm(smoothed.density);\n</code></pre> <p>To provide a rough estimate of the enrichment profile (i.e. ChIP signal over input), we can use the get.smoothed.enrichment.mle() method:</p> <pre><code>smoothed.enrichment.estimate &lt;- get.smoothed.enrichment.mle(chip.data,gfpcontrol.data,bandwidth=200,step=100,tag.shift=tag.shift);\nwritewig(smoothed.enrichment.estimate,\"oct4.enrichment.wig\",\"Smoothed maximum likelihood log2 enrichment estimate\");\n</code></pre> <p>Next, we will scan ChIP and signal tag density to estimate lower bounds of tag enrichment (and upper bound of tag depletion if it is significant) along the genome. The resulting profile gives conservative statistical estimates of log2 fold-enrichment ratios along the genome. The example below uses a window of 500bp (and background windows of 1, 5, 25 and 50 times that size) and a confidence interval corresponding to 1%.</p> <pre><code>enrichment.estimates &lt;- get.conservative.fold.enrichment.profile(chip.data,gfpcontrol.data,fws=500,step=100,alpha=0.01);\nwritewig(enrichment.estimates,\"oct4.Enrichment.estimates.wig\",\"Conservative fold-enrichment/depletion estimates shown on log2 scale\");\nrm(enrichment.estimates);\n</code></pre> <p>Also, broad regions of enrichment for a specified scale can be quickly identified and output in broadPeak format using the following commands:</p> <pre><code>broad.clusters &lt;- get.broad.enrichment.clusters(chip.data,gfpcontrol.data,window.size=1e3,z.thr=3,tag.shift=round(binding.characteristics$peak$x/2));\nwrite.broadpeak.info(broad.clusters,\"oct4.broadPeak\");\n</code></pre> <p>write out in bed format</p> <pre><code>write.table(cbind(rep(\"1\", length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0(\"oct4\",\"_enrich_broad_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\");\n</code></pre> <p>The tasks below will use window tag density (WTD) method to call binding positions, using FDR of 1% and a window size estimated by the binding.characteristics.</p> <p>We set the binding detection parameters: FDR (1%) (Note: we can use an E-value to the method calls below instead of the fdr), the binding.characteristics contains the optimized half-size for binding detection window:</p> <pre><code>fdr &lt;- 1e-2;\ndetection.window.halfsize &lt;- binding.characteristics$whs;\n</code></pre> <p>Identify binding positions using WTD method and write narrow peaks in BED format:</p> <pre><code>bp &lt;- find.binding.positions(signal.data=chip.data,control.data=gfpcontrol.data,fdr=fdr,whs=detection.window.halfsize);\nprint(paste(\"detected\",sum(unlist(lapply(bp$npl,function(d) length(d$x)))),\"peaks\"));\nbp.short &lt;- add.broad.peak.regions(chip.data,gfpcontrol.data,bp,window.size=500,z.thr=3);\nwrite.table(na.omit(data.frame(cbind(rep(\"1\", length(bp.short$npl$chr1$rs)), bp.short$npl$chr1$rs, bp.short$npl$chr1$re))), file = paste0(\"oct4\",\"_enrich_narrow_chr1.bed\"),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \"\\t\");\n</code></pre>"},{"location":"modules/btp-module-chip-seq/chip-seq/#step-3-comparing-binding-sites-to-annotations-using-the-biomart-package","title":"STEP 3. Comparing Binding Sites to Annotations Using the biomaRt package","text":"<p>In order to biologically interpret the results of ChIP-seq experiments, it is usually recommended to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. This can be easily done using the R biomaRt package, which serves as an interface to perform comprehensive data analysis from gene annotation to data mining through wealth number of biological databases integrated by the BioMart software suite (http://www.biomart.org). It provides fast access to large amount of data without touching the underlying database or using complex database queries. These major databases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP mapped to Emsembl.</p> <p>you should make sure that ensembl has the same version of reference as you used in bowtie aligner.</p> <p>We will download the ENSEMBLE mouse genome annotations and generate a list of ENSEMBLE gene information on chromosome 1 including start position, end position, strand and description</p> <pre><code>ensembl = useMart(host=\"asia.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset = \"mmusculus_gene_ensembl\");\ngenes.chr1 = getBM(attributes = c(\"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"description\"), filters = \"chromosome_name\", values= \"1\", mart = ensembl);\n</code></pre> <p>Next, we\u2019re going to take our binding sites from the bp list and use it to determine the set of genes that contain significantly enriched Pol II within 2kb of their TSS.</p> <p>In order to compare PolII sites to TSS sites, we need to write an overlap function where bs represents a binding site position, ts is the annotated TSS and l is the allowed distance of the binding site from the TSS.</p> <pre><code>overlap = function(bs, ts, l)\n{\n    if ((bs &gt; ts - l) &amp;&amp; (bs &lt; ts + l)) {\n        TRUE;\n    } else {\n        FALSE;\n    }\n}\n</code></pre> <p>Now we\u2019ll write a function that takes a vector of binding site values, start positions, end positions and strands of the genes on chromosome X as well as our distance cutoff. l and outputs a logical vector of the genes that contain a Pol II site within l bp (i.e., TRUE value) or do not contain a Pol II site (i.e., FALSE value).</p> <pre><code>fivePrimeGenes = function(bs, ts, te, s, l) {\n    fivePrimeVec = logical();\n    for (i in 1:length(ts)) {\n            fivePrime = FALSE;\n            for (j in 1:length(bs)) {\n                if (s[i] == 1) {\n                    fivePrime = fivePrime || overlap(bs[j], ts[i], l);\n                } else {\n                    fivePrime = fivePrime || overlap(bs[j], te[i], l);\n                }\n             }\n            fivePrimeVec = c(fivePrimeVec, fivePrime);\n    }\n     fivePrimeVec;\n}\n</code></pre> <p>Using the fivePrimeGenes function, generate a vector of the TSSs and genes that contain Pol II within .2kb of their TSS (i.e., l = 2000).</p> <pre><code>fivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000);\n</code></pre> <p>Find the gene located on the plus strand</p> <pre><code>fivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical &amp; genes.chr1$strand == 1];\n</code></pre> <p>Find the gene located on the minus strand</p> <pre><code>fivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical &amp; genes.chr1$strand == -1];\n</code></pre> <p>Combine the start positions together</p> <pre><code>fivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus))\n</code></pre> <p>Get all the gene names</p> <pre><code>fivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]\n</code></pre>"},{"location":"modules/btp-module-chip-seq/chip-seq/#viewing-results-with-the-genome-browser","title":"Viewing results with the Genome browser","text":"<p>It is often instructive to look at your data in a genome browser, which will allow you to get a \u2018feel\u2019 for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. Well known web-based genome browsers, like Ensembl or the UCSC browser do not only allow for more polished and flexible visualization, but also provide access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis, even though could be relatively slow. In this section, we will guide you though using IGV, a stand-alone browser, which has the advantage of being installed locally, easy to use and fast access to visualize your in-house data. We alo provide the workflow of how to use Ensembl for visualization. You can practise after the workshop.</p> <p>IGV Visualization</p> <p>Double click the IGV 2.3 icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest. On the top left of your screen choose from the drop down menu Mouse (mm10). If it doesn\u2019t appear in list, click More .., type mm10 in the Filter section, choose the mouse genome and press OK.</p> <p>We have generated bigWig files in advance for you. Instead of choosing the \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to upload to IGV. The first file is at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw</p> <p>To visualise the data:</p> <ul> <li> <p>Select chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019     box.</p> </li> <li> <p>Click File then choose Load from URL\u2026</p> </li> <li> <p>Paste the location above in the field File URL.</p> </li> <li> <p>Click OK and close the window to return to the genome browser.</p> </li> <li> <p>You should see Oct4.bw has been loaded in the track region below the     genome region.</p> </li> <li> <p>Move the mouse to track region over Oct4.bw.</p> </li> <li> <p>Right click the mouse, Change the track colour on your own     perference.</p> </li> <li> <p>Right click again, in the Windowing Function, choose Maxmum     and set to Autoscale.</p> </li> </ul> <p>Repeat the process for the gfp control sample, located at:</p> <p>http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw.</p> <p>Go to a region on chromosome 1 (e.g. <code>1:34823162-35323161</code>), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis.</p>  <p>Question</p> <p>What can you say about the profile of Oct4 peaks in this region?</p>   Answer  <p>Answer</p> <p>There are no significant Oct4 peaks over the selected region.</p>   <p>Compare it with H3K4me3 histone modification wig file we have generated at http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw.</p> <p>H3K4me3 has a region that contains relatively high peaks than Oct4.</p>  <p>Question</p> <p>Jump to <code>1:36066594-36079728</code> for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4?</p>   Answer  <p>Answer</p> <p>Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed.</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#advanced-session","title":"Advanced Session","text":"<p>Advanced exercise</p> <p>Ensembl Visualization</p> <p>Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html. Choose the genome of interest (in this case, mouse) on the left side of the page, browse to any location in the genome or click one of the demo links provided on the web page. Click on the Add your data link on the left, then choose Attach remote file.</p> <p>Wig files are large so are inconvenient for uploading directly to the Ensemble Genome browser. Instead, we will convert it to an indexed binary format and put this into a web accessible place such as on a HTTP, HTTPS, or FTP server. This makes all the browsing process much faster. Detailed instructions for generating a bigWig from a wig type file can be found at:</p> <p>http://genome.ucsc.edu/goldenPath/help/bigWig.html.</p> <p>We have generated bigWig files in advance for you to upload to the Ensembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw</p> <p>To visualise the data:</p> <ul> <li> <p>Paste the location above in the field File URL.</p> </li> <li> <p>Choose data format bigWig.</p> </li> <li> <p>Choose some informative name and in the next window choose the     colour of your preference.</p> </li> <li> <p>Click Save and close the window to return to the genome browser.</p> </li> </ul> <p>Repeat the process for the gfp control sample, located at:</p> <p>http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw.</p> <p>If can not see your tracks: Click on \u2019Configure this page\u2019 in left panel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left panel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw files by selecting \u2019wiggle plot in the pop up menu.</p> <p>After uploading, choose Configure this page, and under Your data tick both boxes. Closing the window will save these changes.</p> <p>Go to a region on chromosome 1 (e.g. <code>1:34823162-35323161</code>), and zoom in and out to view the signal and peak regions. Be aware that the y-axis of each track is auto-scaled independently of each other, so bigger-looking peaks may not actually be bigger! Always look at the values on the left hand side axis.</p> <p>MACS generates its peak files in a file format called bed file. This is a simple text format containing genomic locations, specified by chromosome, begin and end positions, and some more optional information.</p> <p>See http://genome.ucsc.edu/FAQ/FAQformat.html#format1 for details.</p> <p>Bed files can also be uploaded to the Ensembl browser.</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#motif-analysis","title":"Motif analysis","text":"<p>It is often interesting to find out whether we can identify transcription factor binding sites (TFBSs) from the input DNA sequences. TFBSs which share a similar sequence pattern (motif) are presumed to have biological functions. Here we introduce three motif discovery tools named Trawler, RSAT peak-motifs, and MEME-ChIP, which use different searching algorithms. This might lead to varying results. Hence, it is generally a good idea to use several tools to identify TFBSs. The more tools confirm the same result, the better, which is also called an orthogonal approach. Eventually, you probably want to validate your in silico findings in vivo or in vitro.</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#motif-discovery-with-trawler","title":"Motif discovery with Trawler","text":"<p>Trawler is a fast, yet accurate motif discovery tool that accepts both, BED and FASTA files as input file formats. BED files are generated when you process and analyse your NGS data. Thus, it is handy to use them directly in Trawler. Other tools do not accept BED files as input. With Trawler, BED files can be converted into FASTA files that can then be used for other motif discovery tools (e.g. RSAT peak-motifs and MEME ChIP).</p> <p>Running Trawler</p> <ul> <li> <p>Go to the website https://trawler.erc.monash.edu.au</p> </li> <li> <p>Run Trawler with BED file as input, and wait for the results      </p> </li> <li> <p>Download both sample and background files in FASTA format. Right     click and choose: \u2019Save the link as\u2026\u2019 </p> </li> </ul>  <p>Question</p> <p>Which motif was found to be the most similar to your motif?</p>   Answer  <p>Answer</p> <p>Sox2</p>"},{"location":"modules/btp-module-chip-seq/chip-seq/#optional-motif-discovery-with-rsat-peak-motif","title":"Optional: Motif discovery with RSAT peak-motif","text":"<p>The motif discovery tool RSAT peak-motifs uses FASTA files as input. An optional background can be uploaded in FASTA format. RSAT peak-motifs automatically outputs motifs of 6 and 7 nucleotides length (two separate files). While still accurate, the running time is longer compared to Trawler (up to 20 minutes depending on the size of the files).</p> <p>Running RSAT peak-motifs</p> <ul> <li> <p>Go to the website     http://floresta.eead.csic.es/rsat/peak-motifs_form.cgi</p> </li> <li> <p>Upload input and background FASTA files just downloaded from Trawler     </p> </li> <li> <p>Wait until the discovery finishes.</p> </li> </ul>"},{"location":"modules/btp-module-chip-seq/chip-seq/#optional-motif-discovery-with-meme-chip","title":"Optional: Motif discovery with MEME-ChIP","text":"<p>MEME-ChIP is a popular motif discovery tool and part of MEME Suite. MEME-ChIP accepts input files in FASTA format. It is not necessary to upload your own background because MEME-ChIP uses its own. Although MEME-ChIP is one of the most popular motif discovery tools, the identified motifs are not very accurate and the motif search might take up to one hour. MEME-ChIP outputs the three motifs with the lowest E-Value.</p> <p>Running MEME-ChIP</p> <ul> <li> <p>Go to the website http://meme-suite.org/tools/meme-chip</p> </li> <li> <p>Upload input and background FASTA files just downloaded from Trawler     </p> </li> <li> <p>Wait until the discovery finishes.</p> </li> </ul>"},{"location":"modules/btp-module-chip-seq/chip-seq/#reference","title":"Reference","text":"<p>Chen, X et al.: Integration of external signaling pathways with the core transcriptional network in embryonic stem cells. Cell 133:6, 1106-17 (2008).</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/","title":"Pacbio reads: assembly with command line tools","text":"<p>This tutorial demonstrates how to use long PacBio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#resources","title":"Resources","text":"<p>Tools (and versions) used in this tutorial include:</p> <ul> <li>canu 1.5 (requires java 1.8)</li> <li>infoseq and sizeseq (part of EMBOSS) 6.6.0.0</li> <li>circlator 1.5.1</li> <li>bwa 0.7.15</li> <li>samtools 1.3.1</li> <li>makeblastdb and blastn (part of blast) 2.4.0+</li> <li>pilon 1.20</li> <li>spades 3.10.1</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#learning-objectives","title":"Learning objectives","text":"<p>At the end of this tutorial, be able to:</p> <ol> <li>Assemble and circularise a bacterial genome from PacBio sequence data.</li> <li>Recover small plasmids missed by long read sequencing, using Illumina data</li> <li>Explore the effect of polishing assembled sequences with a different data set.</li> </ol>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#overview","title":"Overview","text":"<p>Simplified version of workflow:</p>    <p>Question<p>How do long- and short-read assembly methods differ?</p> </p>    Answer <p>Short reads are usually assembled using De Bruijn graphs. With long reads, there is a move back towards simpler overlap-layout-consensus methods.</p>    <p>Question</p> <p>Where can we find out the what the approximate genome size should be for the species being assembled?</p>    Answer <p>Go to https://www.ncbi.nlm.nih.gov/genome/ - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column.</p>    <p>Question</p> <p>Where could you view the output filename.gfa and what would it show?</p>    Answer <p>This is the assembly graph. You can view it using the tool \u201cBandage\u201d, https://rrwick.github.io/Bandage/, to see how the contigs are connected (including ambiguities).</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#get-data","title":"Get data","text":"<pre><code>cd /home/trainee/long_reads/workshop_files\nls -l\n</code></pre> <p>The files we need are:</p> <ul> <li>pacbio.fastq.gz - the PacBio reads</li> <li>illumina_R1.fastq.gz - the Illumina forward reads</li> <li>illumina_R2.fastq.gz - the Illumina reverse reads</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#pre-computed-data","title":"Pre-computed data","text":"<p>Some of these tools will take too long to run in this workshop. For these tools, we have pre-computed the output files. In this workshop, we will still enter in the commands and set the tool running, but will sometimes then stop the run and move on to pre-computed output files.</p> <p>In your directory, along with the PacBio and Illumina files, you may also see folders of pre-computed data.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#sample-information","title":"Sample information","text":"<p>The sample used in this tutorial is a gram-positive bacteria called Staphylococcus aureus (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant Staphylococcus aureus. It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#assemble","title":"Assemble","text":"<ul> <li> <p>We will use the assembly software called Canu, https://github.com/marbl/canu.</p> </li> <li> <p>Type in:</p> </li> </ul> <pre><code>canu -p canu -d canu_outdir_NGS genomeSize=2.8m -pacbio-raw pacbio.fastq.gz    \n</code></pre> <ul> <li>the first <code>canu</code> tells the program to run    </li> <li><code>-p canu</code> names prefix for output files (\u201ccanu\u201d)    </li> <li><code>-d canu_outdir_NGS</code> names output directory    </li> <li> <p><code>genomeSize</code> only has to be approximate.    </p> <ul> <li>e.g. Staphylococcus aureus, 2.8m    </li> <li>e.g. Streptococcus pyogenes, 1.8m    </li> </ul> </li> <li> <p>Canu will correct, trim and assemble the reads.    </p> </li> <li> <p>Various output will be displayed on the screen.    </p> </li> <li> <p>As we don\u2019t have time for Canu to complete, stop the run by typing Ctrl-C. We will look at pre-computed data in the folder canu_outdir.</p> </li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#canu-output","title":"Canu output","text":"<p>Move into canu_outdir and <code>ls</code> to see the output files.</p> <pre><code>cd canu_outdir\nls -l\n</code></pre> <ul> <li>The canu.contigs.fasta are the assembled sequences.</li> <li>The canu.unassembled.fasta are the reads that could not be assembled.</li> <li>The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly.</li> <li>The canu.contigs.gfa is the graph of the assembly.</li> <li>Display summary information about the contigs: (<code>infoseq</code> is a tool from EMBOSS)</li> </ul> <pre><code>infoseq canu.contigs.fasta\n</code></pre>  <p>Question</p> <p>How long is the assembled contig ?</p>    Answer <p>tig00000001       2851805 This looks like a chromosome of approximately 2.8 million bases.</p>   <p>This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.  </p>  <p>Try it later</p> <p>Change Canu parameters if required: If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.    </p> <pre><code>canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz    \n</code></pre>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#trim-and-circularise","title":"Trim and circularise","text":""},{"location":"modules/btp-module-denovo-canu/denovo_canu/#run-circlator","title":"Run Circlator","text":"<p>Circlator (https://github.com/sanger-pathogens/circlator) identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.</p> <p>Overhangs are shown in blue:</p> <p> Adapted from Figure 1. Hunt et al. Genome Biology 2015</p> <p>Run Circlator:</p> <pre><code>```text\ncd /home/trainee/long_reads/workshop_files\ncirclator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir_NGS    \n```\n\n`--threads` is the number of cores &lt;!-- change this to an appropriate number--&gt;    \n`--verbose` prints progress information to the screen    \n`canu_outdir/canu.contigs.fasta` is the file path to the input Canu assembly    \n`canu_outdir/canu.correctedReads.fasta.gz` is the file path to the corrected Pacbio reads - note, fastA not fastQ    \n`circlator_outdir_NGS` is the name of the output directory.\n</code></pre> <p>Stop the run by typing Ctrl-C. We will look at pre-computed data in the folder circlator_outdir.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#circlator-output","title":"Circlator output","text":"<p>Move into the circlator_outdir directory and <code>ls</code> to list files.</p> <pre><code>cd /home/trainee/long_reads/workshop_files/circlator_outdir\nls -ltr\n</code></pre>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#questions","title":"Questions","text":"<p>Question</p> <p>Were all the contigs circularised? Why/why not?</p>    Hint <p><pre><code>less 04.merge.circularise.log\n</code></pre> Type \u201cq\u201d to exit.</p>     Answer <ul> <li>Yes, the contig was circularised (last column). In this example, the contig could be circularized because it contained the entire sequence, with overhangs that were trimmed.</li> </ul>    <p>Question</p> <p>Where were the contigs oriented (which gene)?</p>    Hint <p><pre><code>less 06.fixstart.log\n</code></pre> Type \u201cq\u201d to exit.</p>     Answer <ul> <li>Look in the \u201cgene_name\u201d column.</li> <li>The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA.  This is typically used as the start of bacterial chromosome sequences.</li> </ul>    <p>Question</p> <p>What are the trimmed contig sizes?</p>    Hint <pre><code>infoseq 06.fixstart.fasta\n</code></pre>     Answer <ul> <li>tig00000001 2823331 (28564 bases trimmed) -This trimmed part is the overlap.</li> </ul>    <p>Question</p> <p>Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs?</p>    Answer <p>Circlator uses dnaA for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as a rep gene. It is possible to provide a file to Circlator to do this.</p>   <p>Re-name the contigs file:</p> <ul> <li>The trimmed contigs are in the file called 06.fixstart.fasta.</li> <li>Re-name it contig1.fasta:</li> </ul> <pre><code>cp 06.fixstart.fasta contig1.fasta\n</code></pre> <p>Open this file in a text editor (e.g. nano: <code>nano contig1.fasta</code>) and change the header to \u201c&gt;chromosome\u201d.</p> <p>Move the file back into the main folder (<code>mv contig1.fasta ../</code>).</p>  <p>Tips</p> <p>If all the contigs have not circularised with Circlator, an option is to change the <code>--b2r_length_cutoff</code> setting to approximately 2X the average read depth.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#find-smaller-plasmids","title":"Find smaller plasmids","text":"<p>Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.</p> <p>This section involves several steps:</p> <ol> <li>Use the Canu+Circlator output of a trimmed assembly contig.</li> <li>Map all the Illumina reads against this PacBio-assembled contig.</li> <li>Extract any reads that didn\u2019t map and assemble them together: this could be a plasmid, or part of a plasmid.</li> <li>Look for overhang: if found, trim.</li> </ol>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#align-illumina-reads-to-the-pacbio-contig","title":"Align Illumina reads to the PacBio contig","text":"<ul> <li>Index the contigs file:</li> </ul> <pre><code>bwa index contig1.fasta\n</code></pre> <ul> <li> <p>Align Illumina reads using using bwa mem:</p> <pre><code>bwa mem -t 4 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort &gt; aln_NGS.bam     \n</code></pre> <ul> <li><code>bwa mem</code> is the alignment tool     </li> <li><code>-t 4</code> is the number of cores  </li> <li><code>contig1.fasta</code> is the input assembly file     </li> <li><code>illumina_R1.fastq.gz illumina_R2.fastq.gz</code> are the Illumina reads     </li> <li><code>| samtools sort</code> pipes the output to samtools to sort     </li> <li><code>&gt; aln_NGS.bam</code> sends the alignment to the file aln_NGS.bam </li> </ul> </li> </ul> <p>Stop the run by typing Ctrl-C. We will use the pre-computed file called aln.bam.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#extract-unmapped-illumina-reads","title":"Extract unmapped Illumina reads","text":"<ul> <li>Index the alignment file:</li> </ul> <pre><code>samtools index aln.bam\n</code></pre> <ul> <li>Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \u201cunmapped\u201d files:</li> </ul> <pre><code>samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam\n</code></pre> <ul> <li><code>fastq</code> is a command that coverts a .bam file into fastq format</li> <li><code>-f 4</code> : only output unmapped reads</li> <li><code>-1</code> : put R1 reads into a file called unmapped.R1.fastq</li> <li><code>-2</code> : put R2 reads into a file called unmapped.R2.fastq</li> <li><code>-s</code> : put singleton reads into a file called unmapped.RS.fastq</li> <li><code>aln.bam</code> : input alignment file</li> </ul> <p>We now have three files of the unampped reads:  unmapped.R1.fastq,  unmapped.R2.fastq,  unmapped.RS.fastq.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#assemble-the-unmapped-reads","title":"Assemble the unmapped reads","text":"<ul> <li> <p>Assemble with Spades (http://cab.spbu.ru/software/spades/):</p> <pre><code>spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly_NGS\n</code></pre> <ul> <li><code>-1</code> is input file forward</li> <li><code>-2</code> is input file reverse</li> <li><code>-s</code> is unpaired</li> <li><code>--careful</code> minimizes mismatches and short indels</li> <li><code>--cov-cutoff auto</code> computes the coverage threshold (rather than the default setting, \u201coff\u201d)</li> <li><code>-o</code> is the output directory</li> </ul> </li> </ul> <p>Stop the run by typing Ctrl-C. We will use the pre-computed file in the folder spades_assembly.</p> <p>Move into the output directory (spades_assembly) and look at the contigs:</p> <pre><code>infoseq contigs.fasta\n</code></pre> <ul> <li>78 contigs were assembled, with the max length of 2250 (the first contig).  </li> <li>All other nodes are &lt; 650kb so we will disregard as they are unlikely to be plasmids.</li> <li>We will extract the first sequence (NODE_1):</li> </ul> <pre><code>samtools faidx contigs.fasta\n</code></pre> <pre><code>samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 &gt; contig2.fasta\n</code></pre> <ul> <li>This is now saved as contig2.fasta</li> <li>Open in nano and change header to \u201c&gt;plasmid\u201d.</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#trim-the-plasmid","title":"Trim the plasmid","text":"<p>To trim any overhang on this plasmid, we will blast the start of contig2 against itself.</p> <ul> <li>Take the start of the contig:</li> </ul> <pre><code>head -n 10 contig2.fasta &gt; contig2.fa.head\n</code></pre> <ul> <li>We want to see if it matches the end (overhang).</li> <li>Format the assembly file for blast:</li> </ul> <pre><code>makeblastdb -in contig2.fasta -dbtype nucl\n</code></pre> <ul> <li>Blast the start of the assembly (.head file) against all of the assembly:</li> </ul> <pre><code>blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls\n</code></pre> <ul> <li>Look at contig2.bls to see hits:</li> </ul> <pre><code>less contig2.bls\n</code></pre> <ul> <li>The first hit is at start, as expected.</li> <li>The second hit is at 2474 all the way to the end - 2550.</li> <li>This is the overhang.</li> <li>Trim to position 2473.</li> <li>Type \u2018q\u2019 to exit.</li> <li>Index the plasmid.fa file:</li> </ul> <pre><code>samtools faidx contig2.fasta\n</code></pre> <ul> <li>Trim</li> </ul> <pre><code>samtools faidx contig2.fasta plasmid:1-2473 &gt; plasmid.fa.trimmed\n</code></pre> <ul> <li> <p><code>plasmid</code> is the name of the contig, and we want the sequence from 1-2473.</p> </li> <li> <p>Open this file in nano (<code>nano plasmid.fa.trimmed</code>) and change the header to \u201c&gt;plasmid\u201d, save.</p> </li> <li>(Use the side scroll bar to see the top of the file.)</li> <li>We now have a trimmed plasmid.</li> <li>Move file back into main folder:</li> </ul> <pre><code>cp plasmid.fa.trimmed ../\n</code></pre> <ul> <li>Move into the main folder.</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#plasmid-contig-orientation","title":"Plasmid contig orientation","text":"<p>The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator:</p> <pre><code>circlator fixstart plasmid.fa.trimmed plasmid_fixstart\n</code></pre> <ul> <li><code>fixstart</code> is an option in Circlator just to orient a sequence.</li> <li><code>plasmid.fa.trimmed</code> is our small plasmid.</li> <li><code>plasmid_fixstart</code> is the prefix for the output files.</li> </ul> <p>View the output:</p> <pre><code>less plasmid_fixstart.log\n</code></pre> <ul> <li>The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200.</li> <li>Change the file name:</li> </ul> <pre><code>cp plasmid_fixstart.fasta contig2.fasta\n</code></pre>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#collect-contigs","title":"Collect contigs","text":"<pre><code>cat contig1.fasta contig2.fasta &gt; genome.fasta\n</code></pre> <ul> <li>See the contigs and sizes:</li> </ul> <pre><code>infoseq genome.fasta\n</code></pre> <ul> <li>chromosome: 2823331</li> <li>plasmid: 2473</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#questions_1","title":"Questions","text":"<p>Question</p> <p>Why is this section so complicated?</p>    Answer <p>Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744</p>    <p>Question</p> <p>Why can PacBio sequencing miss small plasmids?</p>    Answer <p>Library prep size selection</p>    <p>Question</p> <p>We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing?</p>    Answer <p>Repeats that have mapped to the PacBio assembly.</p>    <p>Question</p> <p>How do you find a plasmid in a Bandage graph?</p>    Answer <p>It is probably circular, matches the size of a known plasmid, and has a rep gene.</p>    <p>Question</p> <p>Are there easier ways to find plasmids?</p>    Answer <p>Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#correct","title":"Correct","text":"<p>We will correct the Pacbio assembly with Illumina reads, using the tool Pilon (https://github.com/broadinstitute/pilon/wiki).</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#make-an-alignment-file","title":"Make an alignment file","text":"<ul> <li> <p>Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. genome.fasta:</p> <pre><code>bwa index genome.fasta    \nbwa mem -t 4 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort &gt; aln_illumina_pacbio_NGS.bam    \n</code></pre> <ul> <li><code>-t</code> is the number of cores  </li> </ul> </li> </ul> <p>Stop the run by typing Ctrl-C. We will use the pre-computed file called aln_illumina_pacbio.bam. - We will use the pre-computed file called aln_illumina_pacbio.bam.</p> <ul> <li>Index the files:</li> </ul> <pre><code>samtools index aln_illumina_pacbio.bam\nsamtools faidx genome.fasta\n</code></pre> <ul> <li>Now we have an alignment file to use in Pilon: aln_illumina_pacbio.bam</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#run-pilon","title":"Run Pilon","text":"<p>Pilon is a software tool which can be used to:</p> <ul> <li>Automatically improve draft assemblies    </li> <li> <p>Find variation among strains, including large event detection    </p> <pre><code>pilon --genome genome.fasta --frags aln_illumina_pacbio.bam --output pilon1_NGS --fix all --mindepth 0.5 --changes --verbose --threads 4\n</code></pre> <ul> <li><code>--genome</code> is the name of the input assembly to be corrected     </li> <li><code>--frags</code> is the alignment of the reads against the assembly     </li> <li><code>--output</code> is the name of the output prefix     </li> <li><code>--fix</code> is an option for types of corrections     </li> <li><code>--mindepth</code> gives a minimum read depth to use     </li> <li><code>--changes</code> produces an output file of the changes made     </li> <li><code>--verbose</code> prints information to the screen during the run     </li> <li><code>--threads</code>: the number of cores     </li> </ul> </li> </ul> <p>Stop the run by typing Ctrl-C. We will use the pre-computed files called with the prefixes pilon1. - We will use the pre-computed files called with the prefixes pilon1._</p> <p>Look at the changes file:</p> <pre><code>less pilon1.changes\n</code></pre> <p>Example:</p>  <p>Look at the details of the fasta file:</p> <pre><code>infoseq pilon1.fasta\n</code></pre> <ul> <li>chromosome - 2823340 (net +9 bases)</li> <li>plasmid - 2473 (no change)</li> </ul> <p>Option:</p> <p>If there are many changes, run Pilon again, using the pilon1.fasta file as the input assembly, and the Illumina reads to correct.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#genome-output","title":"Genome output","text":"<ul> <li>Change the file name:</li> </ul> <pre><code>cp pilon1.fasta assembly.fasta\n</code></pre> <ul> <li>We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.  </li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#questions_2","title":"Questions","text":"<p>Question</p> <p>Why don\u2019t we correct earlier in the assembly process?</p>    Answer <p>We need to circularise the contigs and trim overhangs first.</p>    <p>Question</p> <p>Why can we use some reads (Illumina) to correct other reads (PacBio) ?</p>    Answer <p>Illumina reads have higher accuracy</p>    <p>Question</p> <p>Could we just use PacBio reads to assemble the genome?</p>    Answer <p>Yes, if accuracy adequate.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#short-read-assembly-a-comparison","title":"Short-read assembly: a comparison","text":"<p>So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads.</p> <p>If we only had Illumina reads, we could also assemble these using the tool Spades.</p> <p>You can try this here, or try it later on your own data.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#get-data_1","title":"Get data","text":"<p>We will use the same Illumina data as we used above:</p> <ul> <li>illumina_R1.fastq.gz: the Illumina forward reads</li> <li>illumina_R2.fastq.gz: the Illumina reverse reads</li> </ul> <p>This is from Sample 25747.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#assemble_1","title":"Assemble","text":"<p>Run Spades:</p> <pre><code>spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina\n</code></pre> <ul> <li><code>-1</code> is input file of forward reads</li> <li><code>-2</code> is input file of reverse reads</li> <li><code>--careful</code> minimizes mismatches and short indels</li> <li><code>--cov-cutoff auto</code> computes the coverage threshold (rather than the default setting, \u201coff\u201d)</li> <li><code>-o</code> is the output directory</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#results","title":"Results","text":"<p>Move into the output directory and look at the contigs:</p> <pre><code>infoseq contigs.fasta\n</code></pre>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#questions_3","title":"Questions","text":"<p>How many contigs were found by Spades?</p> <ul> <li>many</li> </ul> <p>How does this compare to the number of contigs found by assembling the long read data with Canu?</p> <ul> <li>many more.</li> </ul> <p>Does it matter that an assembly is in many contigs?</p> <ul> <li> <p>Yes</p> </li> <li> <p>broken genes =&gt; missing/incorrect annotations</p> </li> <li> <p>less information about structure: e.g. number of plasmids</p> </li> <li> <p>No</p> </li> <li> <p>Many or all genes may still be annotated</p> </li> <li>Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes)</li> </ul> <p>How can we get more information about the assembly from Spades?</p> <ul> <li>Look at the assembly graph assembly_graph.fastg, e.g. in the program Bandage. This shows how contigs are related, albeit with ambiguity in some places.</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#comparative-genomics","title":"Comparative genomics","text":"<p>We will compare the genomes assembled by:</p> <ul> <li>pacbio pre-pilon</li> <li>pacbio post pilon</li> <li>illumina only</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#do-the-annotations-differ","title":"Do the annotations differ","text":"<p>Run prokka on the three - how do they differ</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#align-the-short-read-and-long-read-assemblies","title":"Align the short-read and long-read assemblies","text":"<p>Mauve: align the illumina contigs to the polished pacbio contigs.</p>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#further-analyses","title":"Further analyses","text":"<ul> <li>Annotate genomes, e.g. with Prokka, https://github.com/tseemann/prokka</li> <li>Comparative genomics, e.g. with Roary, https://sanger-pathogens.github.io/Roary/</li> </ul>"},{"location":"modules/btp-module-denovo-canu/denovo_canu/#links","title":"Links","text":"<ul> <li>Canu manual and gitub repository</li> <li>Circlator article and github repository</li> <li>Pilon article and github repository</li> <li>Notes on finishing and evaluating assemblies.</li> </ul>"},{"location":"modules/btp-module-ngs-bio/biol/","title":"NGS Biological Insight","text":""},{"location":"modules/btp-module-ngs-bio/biol/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Find gene ontology enrichment in a list of differentially expressed     genes using R-based packages.</p> </li> <li> <p>Running GO enrichment analysis using the web tool DAVID and the web     tool Gorilla</p> </li> <li> <p>To run webtools such as REVIGO and STRING</p> </li> </ul>"},{"location":"modules/btp-module-ngs-bio/biol/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/btp-module-ngs-bio/biol/#tools-used","title":"Tools Used","text":"<p>Goana from Limma: https://bioconductor.org/packages/release/bioc/html/limma.html</p> <p>DAVID: http://david.abcc.ncifcrf.gov</p> <p>GOrilla: http://cbl-gorilla.cs.technion.ac.il</p> <p>REVIGO: http://revigo.irb.hr</p> <p>STRING: http://string-db.org</p>"},{"location":"modules/btp-module-ngs-bio/biol/#author-information","title":"Author Information","text":"<p>Primary Author(s):     Susan M Corley s.corley@unsw.edu.au</p> <p>Contributor(s):     Sonika Tyagi sonika.tyagi@agrf.org.au      Nandan Deshpande n.deshpane@unsw.edu.au</p>"},{"location":"modules/btp-module-ngs-bio/biol/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to allow you to develop some familiarity with commonly used, freely available R based packages and web tools which can be used to gain biological insight from a differential expression experiment. We will use the differentially expressed genes (DEGs) identified in the last session. First, we will look at whether these genes are enriched for gene ontology terms which gives us some insight as to whether the DEGs are involved in particular functions. Then we will use a tool that constructs an interaction network from these genes. This will allow us to identify clusters of DEGs that are known to interact.</p> <p>In using any database tools it is always advisable to check whether they are regularly updated. We suggest that you experiment with more than one tool.</p>"},{"location":"modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-goana","title":"Gene ontology analysis with GOana","text":"<p>First we will go back to the R environment and use the function GOana associated with the limma package. To use this function we need to have our DEGs annotated with the entrez gene identifier for each gene.We did this early on in our data processing. We use the fit object (fit_v) generated using the voom function in limma for this analysis. To obtain more information regarding the goana function type ?goana within your R session.</p> <p>Open the Terminal and go to the <code>rnaseq/edgeR</code> working directory:</p> <pre><code>cd /home/trainee/rnaseq/edgeR\n</code></pre>  <p>STOP</p> <p>All commands entered into the terminal for this tutorial should be from within the <code>/home/trainee/rnaseq/edgeR</code> directory.</p>  <p>R (press enter)</p> <p>Check that the directory you are in contains the above-mentioned fit_v file by typing:</p> <pre><code>ls()\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(gplots)\nlibrary(org.Hs.eg.db)\n</code></pre> <p>We will use the goana function to obtain the gene ontology terms associated with the DEGs.</p> <pre><code>DE_GOana&lt;-goana(fit_v, coef=2, geneid=fit_v$genes$Entrez, FDR=0.05, species = \"Hs\", trend=F, plot=F )\n</code></pre> <p>Now we will look at the most significant biological process (BP)ontology terms</p> <pre><code>DE_GOana_top_BP&lt;- topGO(DE_GOana, ontology=c(\"BP\"), number=150L, truncate.term=50)\nhead(DE_GOana_top_BP, 20)      \nDE_GOana_top_BP_down&lt;- topGO(DE_GOana, ontology=c(\"BP\"), sort = \"down\", number=150L, truncate.term=50)\nhead(DE_GOana_top_BP_down, 10)\nDE_GOana_top_BP_up&lt;- topGO(DE_GOana, ontology=c(\"BP\"), sort = \"up\", number=150L, truncate.term=50)\nhead(DE_GOana_top_BP_up, 10)\n</code></pre> <p>Rather than looking at biological process (BP) let\u2019s now look at molecular function (MF) terms</p> <pre><code>DE_GOana_top_MF_down&lt;- topGO(DE_GOana, ontology=c(\"MF\"), sort = \"down\", number=150L, truncate.term=50)\nhead(DE_GOana_top_MF_down, 10)\nDE_GOana_top_MF_up&lt;- topGO(DE_GOana, ontology=c(\"MF\"), sort = \"up\", number=150L, truncate.term=50)\nhead(DE_GOana_top_MF_up, 10)\n</code></pre>"},{"location":"modules/btp-module-ngs-bio/biol/#questions-and-answers","title":"Questions and Answers","text":"<p>Based on the above section:</p>  <p>Question</p> <p>What is the general theme emerging when we look at biological process in the down-regulated genes and the up-regulated genes?</p>    Answer <p>We see terms involving the cell cycle and cell division are enriched in the down-regulated genes and terms involving ER stress and protein folding are enriched in the up-regulated genes. </p>    <p>Question</p> <p>Looking at the biological process (BP) term \u201ccell division: GO:0051301\u201d, how many down-regulated and up-regulated DEGs are annotated with this term, and what statistical significance is associated with this enrichment?</p>    Answer <p>Look for the row for GO:0051301 in the top 20 BP ontology terms. </p>   <p>There are a number of tools and packages available with the R-bioconductor repositories that you can use with your R code to run ontologies and pathway analysis. </p>"},{"location":"modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-david","title":"Gene ontology analysis with DAVID","text":"<p>Click on your Firefox web browser. Go to the DAVID website: http://david.abcc.ncifcrf.gov. Go to your edgeR folder and open the file <code>voom_res_sig_lfc.txt</code> using LibreOffice Calc. For the separator options in LibreOffice Calc choose Separated by tab. Once you have opened this file copy the Ensembl Gene Ids (Column A). This list can then be pasted into DAVID.  </p> <p>Screenshots of the DAVID website and the steps to move through the website are provided in the presentation prepared for this session. Use that material to work through this exercise.  </p> <p>We will use the Functional Annotation Clustering tool in DAVID. First we will uncheck all the defaults and look only at the GO terms involving biological process. After unchecking all the defaults, expand Gene Ontology and select GOTERM_BP_5. Then select the button Functional Annotation Clustering.  </p> <p>This will bring up a screen where GO terms are clustered. Statistical testing is performed to assess whether the GO terms are more enriched in the list of DEGs than would be expected by chance. You will see a column of P_Value and also adjusted P values. Have a look at the brief description of the statistical test used in DAVID (https://david.ncifcrf.gov/helps/functional_annotation.html).</p>"},{"location":"modules/btp-module-ngs-bio/biol/#questions-and-answers_1","title":"Questions and Answers","text":"<p>Based on the above section:</p>  <p>Question</p> <p>What functional themes emerge in Cluster 1 and Cluster 2?</p>    Answer <p>Cluster 1 is enriched for cell singling and Cluster 2 is enriched for cell cycle. Note that answers may vary based on the genes entered and the options selected. </p>    <p>Question</p> <p>How many of the DEGs are annotated with the term \u201cintracellular signal transduction\u201d and name five genes?</p>    Answer <p>Looking at the first row of the DAVID results we see that 181 DEGs are annotated with \u201cIntracellular signal transduction\u201d. The first 5 genes listed are DHCR24, HMGCR, ADAP12, ARL5A, ARFGEF3.</p>    <p>Question</p> <p>Does this seem to be sensible in an experiment that looks at the response of cancer cells to a stimulant?</p>"},{"location":"modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-gorilla","title":"Gene ontology analysis with GOrilla","text":"<p>Click on your Firefox web browser. Go to the GOrilla website: http://cbl-gorilla.cs.technion.ac.il </p> <p>For this tool we will use a background list of genes. Open the file <code>voom_res.txt</code> using LibreOffice Calc. Copy the Ensembl Gene Ids (Column A) and paste this into GOrilla as the background set.</p> <p>As in the previous exercise we will use the DEGs found by voom (<code>voom_res_sig_lfc.txt</code>) as the Target set.  </p> <p>We will firstly look for enriched GO process terms. Screenshots of the website showing the steps you need to follow are in the presentation for this session.  </p> <p>GOrilla will display the GO term hierarchy. This shows you which terms are parent and child terms and how the terms are related.Under this you will find a table of the most significantly enriched GO terms. Have a look at the DEGs associated with the most enriched clusters.</p>"},{"location":"modules/btp-module-ngs-bio/biol/#questions-and-answers_2","title":"Questions and Answers","text":"<p>Based on the above section:</p>  <p>Question</p> <p>Find the GO term regulation of cell proliferation how far can you trace this back to the parent terms. </p>    Answer <p>regulation of cell proliferation \u2013 regulation of cellular process \u2013 regulation of biological process \u2013 biological regulation \u2013 biological process</p>    <p>Question</p> <p>What are the direct child terms of regulation of cell proliferation?. </p>    Answer <p>regulation of stem cell proliferation, regulation of sooth muscle cell proliferation, positive regulation of cell proliferation, regulation of mesenchymal cell proliferation</p>    <p>Question</p> <p>What is the enrichment score for regulation of cell proliferation ? How is this calculated (Hint: scroll down the page for the heading Enrichment). </p>    Answer <p>1.58 ((104/870)/(919/12137))</p>"},{"location":"modules/btp-module-ngs-bio/biol/#revigo-to-reduce-redundancy-and-visualise","title":"REVIGO to reduce redundancy and visualise","text":"<p>We can use the results generated by the GOrilla web tool as input to REVIGO which will summarise the GO data and allow us to visualize the simplified data. Click on the link Visualize output in REViGO. Follow the screen shots in the presentation.Go to the treemap view.</p>  <p>Question</p> <p>What are the main functional categories emerging in this analysis? </p>"},{"location":"modules/btp-module-ngs-bio/biol/#string","title":"STRING","text":"<p>Using STRING to look at networks that may be formed by the DEGsClick on your Firefox web browser. Go to the STRING website: http://string-db.org. For this exercise we will only use the top 500 DEGs. Go to the file (<code>voom_res_sig_lfc.txt</code>) copy only the top 500. These will be pasted as input to the STRING website. Follow the screen shots in the presentation.  </p> <p>You will see a large interaction network being built from the 500 DEGs.We will refine this by clicking on the Data settings tab and selecting high confidence (see the screen shot in the presentation). Look at the gene clusters that are generated.</p> <p>Find the gene CDK1. Look at the cluster generated around this gene. What other DEGs are interaction partners of CDK1.  </p> <p>Click on CDK1 and find what functions it is involved in. Click on the interaction partners of CDK1 and find their functions. </p>  <p>Question</p> <p>Does this help in further explaining some of the gene ontology results? </p>  <p>Explore other clusters that are formed in this analysis.</p>"},{"location":"modules/btp-module-ngs-cli/commandline/","title":"Introduction to Command Line","text":""},{"location":"modules/btp-module-ngs-cli/commandline/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Familiarise yourself with the command line environment on a Linux     operating system.</p> </li> <li> <p>Run some basic linux system and file operation commands</p> </li> <li> <p>Navigation of biological data files structure and manipulation</p> </li> </ul>"},{"location":"modules/btp-module-ngs-cli/commandline/#resources","title":"Resources","text":""},{"location":"modules/btp-module-ngs-cli/commandline/#tools","title":"Tools","text":"<ul> <li> <p>Basic Linux system commands on an Ubuntu OS.</p> </li> <li> <p>Basic file operation commands</p> </li> </ul>"},{"location":"modules/btp-module-ngs-cli/commandline/#links","title":"Links","text":"<ul> <li> <p>Software Carpentry</p> </li> <li> <p>Example 1000Genome Project data</p> </li> </ul>"},{"location":"modules/btp-module-ngs-cli/commandline/#author-information","title":"Author Information","text":"<p>Primary Author(s):     Matt Field matt.field@anu.edu.au </p>"},{"location":"modules/btp-module-ngs-cli/commandline/#shell-exercise","title":"Shell Exercise","text":"<p>Let\u2019s try out your new shell skills on some real data.</p> <p>The file <code>1000gp.vcf</code> is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project (http://www.1000genomes.org). The \u2019vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2019Variant Call Format\u2019. The file starts with a bunch of comment lines (they start with \u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2019individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with.</p> <p>Open the Terminal and go to the directory where the data are stored: <pre><code>cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n</code></pre></p>  <p>Question</p> <p>What is the file size (in kilo-bytes), and how many lines are in the file?.</p>    Hint <p>Hint: <code>man ls</code>, <code>man wc</code></p>     Answer <p>3.6M</p> <p>45034 lines</p>   <p>Because this file is so large, you\u2019re going to almost always want to pipe (\u2018|\u2019) the result of any command to less (a simple text viewer, type \u2018<code>q</code>\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen.</p> <p>Let\u2019s start by printing the first 5 lines to see what it looks like. <pre><code>head -5 1000gp.vcf\n</code></pre></p> <p>That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with \u2019#\u2019)!</p> <p>Print the first 20 lines to see more of the file. <pre><code>head -20 1000gp.vcf\n</code></pre></p> <p>Okay, so now we can see the basic structure of the file. A few comment lines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are:</p> <ol> <li> <p>the chromosome (which volume the difference is in)</p> </li> <li> <p>the position (which character in the volume the difference starts     at)</p> </li> <li> <p>the ID of the difference</p> </li> <li> <p>the sequence in the reference human(s)</p> </li> </ol> <p>The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct.</p> <p>To start analyzing the actual data, we have to remove the header.</p>  <p>Question</p> <p>How can we print the first 10 non-header lines (those that don\u2019t start with a \u2019#\u2019)?</p>    Hint <p>Hint: <code>man grep</code> (remember to use pipes \u2018|\u2019)</p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | head  </p>    <p>Question</p> <p>How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)?</p>    Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | wc -l (should print 45024)</p>   <p>Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on).</p>  <p>Question</p> <p>Print the first 10 chromosomes, one per line.</p>    Hint <p>Hint: <code>man cut</code> (remember to remove header lines first)</p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | cut -f 1 | head</p>   <p>As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male).</p> <p>Let\u2019s look at which chromosomes these variations are on.</p>  <p>Question</p> <p>Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines).</p>    Hint <p>Hint: remove all duplicates from your previous answer (<code>man sort</code>)</p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | cut -f 1 | sort -u</p>   <p>Rather than using <code>sort</code> to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, <code>uniq</code>. The <code>uniq</code> command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, <code>uniq</code> won\u2019t work properly.</p>  <p>Question</p> <p>Using <code>sort</code> and <code>uniq</code>, print the number of times each chromosome occurs in the file.</p>    Hint <p>Hint: <code>man uniq</code></p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | cut -f 1 | sort | uniq -c</p>    <p>Question</p> <p>Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed.</p>    Hint <p>Hint: Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order.</p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r</p>   <p>This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily.</p>  <p>Question</p> <p>Sort the previous output by chromosome number</p>    Hint <p>Hint: A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field.</p>     Answer <p>grep -v \u201c^#\u201d 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/","title":"Read Alignment","text":""},{"location":"modules/btp-module-ngs-mapping/alignment/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Perform a simple NGS data alignment task, with Bowtie2, against one     interested reference data</p> </li> <li> <p>Interpret and manipulate the mapping output using SAMtools</p> </li> <li> <p>Visualise the alignment via a standard genome browser, e.g. IGV     browser</p> </li> </ul>"},{"location":"modules/btp-module-ngs-mapping/alignment/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/btp-module-ngs-mapping/alignment/#tools-used","title":"Tools Used","text":"<p>Bowtie2: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml</p> <p>Samtools: http://broadinstitute.github.io/picard</p> <p>BEDTools: http://code.google.com/p/bedtools/</p> <p>UCSC tools: http://hgdownload.cse.ucsc.edu/admin/exe/</p> <p>IGV genome browser: http://www.broadinstitute.org/igv/</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#useful-links","title":"Useful Links","text":"<p>SAM Specification: http://samtools.sourceforge.net/SAM1.pdf</p> <p>Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#sources-of-data","title":"Sources of Data","text":"<p>http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#author-information","title":"Author Information","text":"<p>Primary Author(s):     Myrto Kostadima kostadim@ebi.ac.uk</p> <p>Contributor(s):     Xi (Sean) Li sean.li@csiro.au</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to perform an unspliced alignment for a small subset of raw reads. We will align raw sequencing data to the mouse genome using Bowtie2 and then we will manipulate the SAM output in order to visualize the alignment on the IGV browser.</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#prepare-the-environment","title":"Prepare the Environment","text":"<p>We will use one data set in this practical, which can be found in the <code>ChIP-seq</code> directory on your desktop.</p> <p>Open the Terminal.</p> <p>First, go to the right folder, where the data are stored.</p> <pre><code>cd /home/trainee/chipseq\n</code></pre> <p>The <code>.fastq</code> file that we will align is called <code>Oct4.fastq</code>. This file is based on Oct4 ChIP-seq data published by Chen et al. (2008). For the sake of time, we will align these reads to a single mouse chromosome.</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#alignment","title":"Alignment","text":"<p>You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will try Bowtie2, a widely used ultrafast, memory efficient short read aligner.</p> <p>Bowtie2 has a number of parameters in order to perform the alignment. To view them all type</p> <pre><code>bowtie2 --help\n</code></pre> <p>Bowtie2 uses indexed genome for the alignment in order to keep its memory footprint small. Because of time constraints we will build the index only for one chromosome of the mouse genome. For this we need the chromosome sequence in FASTA format. This is stored in a file named <code>mm10</code>, under the subdirectory <code>bowtie_index</code>.</p> <p>The indexed chromosome is generated using the command:</p>  <p>STOP</p> <p>DO NOT run this command. This has already been run for you.</p> <p>** bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10 **</p>  <p>This command will output 6 files that constitute the index. These files that have the prefix <code>mm10</code> are stored in the <code>bowtie_index</code> subdirectory. To view if they files have been successfully created type:</p> <pre><code>ls -l bowtie_index\n</code></pre> <p>Now that the genome is indexed we can move on to the actual alignment. The first argument for <code>bowtie2</code> is the basename of the index for the genome to be searched; in our case this is <code>mm10</code>. We also want to make sure that the output is in SAM format using the <code>-S</code> parameter. The last argument is the name of the FASTQ file.</p> <p>Align the Oct4 reads using Bowtie2:</p> <pre><code>bowtie2 -x bowtie_index/mm10 -q Oct4.fastq &gt; Oct4.sam\n</code></pre> <p>The above command outputs the alignment in SAM format and stores them in the file <code>Oct4.sam</code>.</p> <p>In general before you run Bowtie2, you have to know what quality encoding your FASTQ files are in. The available FASTQ encodings for bowtie are:</p> <p>\u2013phred33-quals :   Input qualities are Phred+33 (default).</p> <p>\u2013phred64-quals :   Input qualities are Phred+64 (same as <code>\u2013solexa1.3-quals</code>).</p> <p>\u2013solexa-quals :   Input qualities are from GA Pipeline ver. &lt; 1.3.</p> <p>\u2013solexa1.3-quals :   Input qualities are from GA Pipeline ver. &gt;= 1.3.</p> <p>\u2013integer-quals :   Qualities are given as space-separated integers (not ASCII).</p> <p>The FASTQ files we are working with are Sanger encoded (Phred+33), which is the default for Bowtie2.</p> <p>Bowtie2 will take 2-3 minutes to align the file. This is fast compared to other aligners which sacrifice some speed to obtain higher sensitivity.</p> <p>Look at the top 10 lines of the SAM file using head (record lines are wrapped). Then try the second command, note use arrow navigation and to exit type \u2019q\u2019.</p> <pre><code>head  Oct4.sam\nless -S Oct4.sam\n</code></pre>  <p>Question</p> <p>Can you distinguish between the header of the SAM format and the actual alignments?</p>   Answer  <p>Answer</p> <p>The header line starts with the letter \u2018@\u2019, i.e.:</p>  <p>@HD   VN:1.0       SO:unsorted              @SQ   SN:chr1      LN:195471971             @PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d</p>    <p>While, the actual alignments start with read id, i.e.:</p>  <p>SRR002012.45   0    etc     SRR002012.48   16   chr1   etc</p>   <p>Question</p> <p>What kind of information does the header provide?</p>   Answer  <p>Answer</p>  <ul> <li> <p>@HD: Header line; VN: Format version; SO: the sort order of alignments.</p> </li> <li> <p>@SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length.</p> </li> <li> <p>@PG: Read group information; ID: Read group identifier; VN: Program version; CL: the command line that produces the alignment.</p> </li> </ul>   <p>Question</p> <p>To which chromosome are the reads mapped?</p>   Answer  <p>Answer</p> <p>Chromosome 1.</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#manipulate-sam-output","title":"Manipulate SAM output","text":"<p>SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space.</p> <p>Convert SAM to BAM using <code>samtools view</code> and store the output in the file <code>Oct4.bam</code>. You have to instruct <code>samtools view</code> that the input is in SAM format (<code>-S</code>), the output should be in BAM format (<code>-b</code>) and that you want the output to be stored in the file specified by the <code>-o</code> option:</p> <pre><code>samtools view -bSo Oct4.bam Oct4.sam\n</code></pre> <p>Compute summary stats for the Flag values associated with the alignments using:</p> <pre><code>samtools flagstat Oct4.bam\n</code></pre>"},{"location":"modules/btp-module-ngs-mapping/alignment/#visualize-alignments-in-igv","title":"Visualize alignments in IGV","text":"<p>IGV is a stand-alone genome browser. Please check their website (http://www.broadinstitute.org/igv/) for all the formats that IGV can display. For our visualization purposes we will use the BAM and bigWig formats.</p> <p>When uploading a BAM file into the genome browser, the browser will look for the index of the BAM file in the same folder where the BAM files is. The index file should have the same name as the BAM file and the suffix <code>.bai</code>. Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates.</p> <p>Sort alignments according to chromosomal position and store the result in the file with the prefix <code>Oct4.sorted</code>:</p> <pre><code>samtools sort Oct4.bam -o Oct4.sorted.bam\n</code></pre> <p>Index the sorted file.</p> <pre><code>samtools index Oct4.sorted.bam\n</code></pre> <p>The indexing will create a file called <code>Oct4.sorted.bam.bai</code>. Note that you don\u2019t have to specify the name of the index file when running <code>samtools index</code>, it simply appends a <code>.bai</code> suffix to the input BAM file.</p> <p>Another way to visualize the alignments is to convert the BAM file into a bigWig file. The bigWig format is for display of dense, continuous data and the data will be displayed as a graph. The resulting bigWig files are in an indexed binary format.</p> <p>The BAM to bigWig conversion takes place in two steps. Firstly, we convert the BAM file into a bedgraph, called <code>Oct4.bedgraph</code>, using the tool <code>genomeCoverageBed</code> from BEDTools. Then we convert the bedgraph into a bigWig binary file called <code>Oct4.bw</code>, using <code>bedGraphToBigWig</code> from the UCSC tools:</p> <pre><code>genomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome &gt; Oct4.bedgraph\nbedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw\n</code></pre> <p>Both of the commands above take as input a file called <code>mouse.mm10.genome</code> that is stored under the subdirectory <code>bowtie_index</code>. These genome files are tab-delimited and describe the size of the chromosomes for the organism of interest. When using the UCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which species/genome build you are working with. The way you do this for BEDTools is to create a \u201cgenome\u201d file, which simply lists the names of the chromosomes (or scaffolds, etc.) and their size (in basepairs).</p> <p>BEDTools includes pre-defined genome files for human and mouse in the <code>genomes</code> subdirectory included in the BEDTools distribution.</p> <p>Now we will load the data into the IGV browser for visualization. In order to launch IGV double click on the <code>IGV 2.3</code> icon on your Desktop. Ignore any warnings and when it opens you have to load the genome of interest.</p> <p>On the top left of your screen choose from the drop down menu <code>Mouse (mm10)</code>. If it doesn\u2019t appear in list, click <code>More ..</code>, type <code>mm10</code> in the Filter section, choose the mouse genome and press OK. Then in order to load the desire files go to:</p> <pre><code>File &gt; Load from File\n</code></pre> <p>On the pop up window navigate to Desktop -&gt; chipseq folder and select the file <code>Oct4.sorted.bam</code>.</p> <p>Repeat these steps in order to load <code>Oct4.bw</code> as well.</p> <p>Select <code>chr1</code> from the drop down menu on the top left. Right click on the name of <code>Oct4.bw</code> and choose Maximum under the Windowing Function. Right click again and select Autoscale.</p> <p>In order to see the aligned reads of the BAM file, you need to zoom in to a specific region. For example, look for gene <code>Lemd1</code> in the search box.</p>  <p>Question</p> <p>What is the main difference between the visualization of BAM and bigWig files?</p>   Answer  <p>Answer</p> <p>The actual alignment of reads that stack to a particular region can be displayed using the information stored in a BAM format. The bigWig format is for display of dense, continuous data that will be displayed in the Genome Browser as a graph.</p>   <p>Using the <code>+</code> button on the top right, zoom in to see more of the details of the alignments.</p>  <p>Question</p> <p>What do you think the different colors mean?</p>   Answer  <p>Answer</p> <p>The different color represents four nucleotides, e.g. blue is Cytidine (C), red is Thymidine (T).</p>"},{"location":"modules/btp-module-ngs-mapping/alignment/#practice-makes-perfect","title":"Practice Makes Perfect!","text":"<p>In the chipseq folder you will find the file <code>gfp.fastq</code>. Follow the above described analysis, from the bowtie2 alignment step, for this dataset as well. You will need these files for the ChIP-Seq module.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/","title":"Data Quality","text":""},{"location":"modules/btp-module-ngs-qc/ngs-qc/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Assess the overall quality of NGS (FastQ format) sequence reads</p> </li> <li> <p>Visualise the quality, and other associated matrices, of reads to     decide on filters and cutoffs for cleaning up data ready for     downstream analysis</p> </li> <li> <p>Clean up adaptors and pre-process the sequence data for further     analysis</p> </li> </ul>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/btp-module-ngs-qc/ngs-qc/#tools-used","title":"Tools Used","text":"<p>FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</p> <p>Skewer: http://sourceforge.net/projects/skewer/</p> <p>FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#useful-links","title":"Useful Links","text":"<p>FASTQ Encoding</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#author-information","title":"Author Information","text":"<p>Primary Author(s):  Sonika Tyagi sonika.tyagi@agrf.org.au </p> <p>Contributor(s):  Nandan Deshpande n.deshpande@unsw.edu.au</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#introduction","title":"Introduction","text":"<p>Going on a blind date with your read set? For a better understanding of the consequences please check the data quality!</p> <p>For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads.</p> <p>One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set.</p> <p>Highly redundant coverage (&gt;15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#quality-value-encoding-schema","title":"Quality Value Encoding Schema","text":"<p>Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry [www.illumina.com]. In order to use a single character to encode Phred qualities, ASCII characters are used (http://shop.alterlinks.com/ascii-table/ascii-table-us.php). All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark (!). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero.</p> <p>Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters &lt; 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33.</p> <p>FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are (<code>@ABCDEFGHI</code>), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities.</p> <p>For a graphical representation of the different ASCII characters used in the two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#q-score-encoding-implemented-with-the-novaseq-platform","title":"Q-score encoding implemented with the Novaseq platform","text":"<p>In order to reduce the data footrpints Illumina has come up with a new method to reduce quality score resolution and optimise data storae. The new Q-score  encoding now follows an 8 level mapping of individual quality scores (0-40 or &gt;40) [See Table 1]. With the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that  bin mapped to a new value of 22. This can be thought of as simply replacing all the  occurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence. Illumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically &gt; 50% and the resulting sorted BAM  les are reduced by ~30%.</p>    Quality Score Bins Mapped quality scores     N (no call) N (no call)   2-9 6   10-19 15   20-24 22   25-29 27   30-34 33   35-39 37   &gt;=40 40   Table 1: Novaseq Q-score bins mapping"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#prepare-the-environment","title":"Prepare the Environment","text":"<p>To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop.</p> <p>Open the Terminal and go to the directory where the data are stored:</p> <pre><code>cd\nls\ncd qc\npwd\n</code></pre> <p>At any time, help can be displayed for FastQC using the following command:</p> <pre><code>fastqc -h\n</code></pre> <p>Look at SYNOPSIS (Usage) and options after typing fastqc -h</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#quality-visualisation","title":"Quality Visualisation","text":"<p>We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file.</p> <p>Execute the following command on the two files:</p> <pre><code>fastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz\n</code></pre> <p>View the FastQC report file of the bad data using a web browser such as firefox. The \u2019&amp;\u2019 sign puts the job in the background.</p> <pre><code>firefox qcdemo_R2_fastqc.html &amp;\n</code></pre> <p>The report file will have a Basic Statistics table and various graphs and tables for different quality statistics. E.g.:</p>    Property Value     Filename qcdemo_R2.fastq.gz   File type Conventional base calls   Encoding Sanger / Illumina 1.9   Total Sequences 1000000   Filtered Sequences 0   Sequence length 150   %GC 37    <p>Table 2: Summary statistics for bad_example_untrimmed</p>  <p>Figure 1:bad_example_untrimmed_QC_plot</p> <p>A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base A is wrong (P(A)) is expressed by a quality score, Q(A), according to the relationship:</p> <pre><code>Q(A) =-10 log10(P(A))\n</code></pre> <p>The relationship between the quality score and error probability is demonstrated with the following table:</p>    Quality score, Q(A) Error probability, P(A) Accuracy of base call     10 0.1 90%   20 0.01 99%   30 0.001 99.9%   40 0.0001 99.99%   50 0.00001 99.999%    <p>Table 3: Quality Error Probabilities</p>  <p>Question</p> <p>How many sequences were there in your file? What is the read length?</p>    Answer <p>1,000,000. read length=150bp</p>    <p>Question</p> <p>Does the quality score values vary throughout the read length? </p>    Hint <p>look at the \u2019per base sequence quality plot\u2019</p>     Answer <p>Yes. Quality scores are dropping towards the end of the reads.</p>    <p>Question</p> <p>What is the quality score range you see?</p>    Answer <p>2-40</p>    <p>Question</p> <p>At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?</p>    Answer <p>Around 30 bp position</p>    <p>Question</p> <p>How can we trim the reads to filter out the low quality data?</p>    Answer <p>By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#good-quality-data","title":"Good Quality Data","text":"<p>View the FastQC report files <code>fastqc_report.html</code> to see examples of a good quality data and compare the quality plot with that of the <code>bad_example_fastqc</code>.</p> <pre><code>firefox qcdemo_R1_fastqc.html &amp;\n</code></pre> <p>Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#read-trimming","title":"Read Trimming","text":"<p>Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#quality-based-trimming","title":"Quality Based Trimming","text":"<p>Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data.</p> <p>The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming.</p> <p>Run the following command to quality trim a set of paired end data.</p> <pre><code>cd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n</code></pre> <p>-t :   number of threads to use</p> <p>-l :   min length to keep after trimming</p> <p>-q :   Quality threshold used for trimming at 3\u2019 end</p> <p>-Q :   mean quality threshold for a read</p> <p>-m :   pair-end mode</p> <p>Run FastQC on the quality trimmed file and visualise the quality scores.</p> <p>Look at the last files generated, are the file names same as the input ?</p> <pre><code>ls -ltr\n</code></pre> <p>Run Fastqc on the quality trimmed files:</p> <pre><code>fastqc -f fastq qcdemo-trimmed-pair1.fastq\nfastqc -f fastq qcdemo-trimmed-pair2.fastq\n</code></pre> <p>Visualise the fastqc results:</p> <pre><code>firefox qcdemo-trimmed-pair1_fastqc.html &amp;\nfirefox qcdemo-trimmed-pair2_fastqc.html &amp;\n</code></pre> <p>Let\u2019s look at the quality from the second reads. The output should look like:</p>    Property Value     Filename qcdemo-trimmed-pair2.fastq   File type Conventional base calls   Encoding Sanger / Illumina 1.9   Total Sequences 742262   Filtered Sequences 0   Sequence length 50-150   %GC 37    <p>Table 4:Summary Statistics of QC_demo_R1_trimmed</p>  <p>Figure 2:bad_example_quality_trimmed_plot</p> <p>Did the number of total reads in R1 and R2 change after trimming?</p> <p>Quality trimming discarded &gt;25000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.</p> <p>What reads lengths were obtained after quality based trimming?</p> <p>50-150</p> <p>Reads &lt;50 bp, following quality trimming, were discarded.</p>  <p>Question</p> <p>Did you observe adapter sequences in the data?</p>  Answer <p>No. (Hint: look at the overrepresented sequences)    </p>    <p>Question</p> <p>How can you use -a option with fastqc? (Hint: try fastqc -h).</p>  Answer <p>Adaptors can be supplied in a file for screening.</p>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#adapter-clipping","title":"Adapter Clipping","text":"<p>Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis.</p> <p>This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs.</p> <p>Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence.</p> <p>Cutadapt http://code.google.com/p/cutadapt/</p> <p>Trimmomatic http://www.usadellab.org/cms/?page=trimmomatic</p> <p>Here we are demonstrating <code>Skewer</code> to trim a given adapter sequence.</p> <pre><code>cd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n</code></pre> <p>-x :   adaptor sequence used</p> <p>-t :   number of threads to use</p> <p>-l :   min length to keep after trimming</p> <p>-L :   Max length to keep after trimming, in this experiment we were     expecting only small RNA fragments</p> <p>-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to     control the end you want to trim</p> <p>Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results.</p> <pre><code>fastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &amp;\n</code></pre>"},{"location":"modules/btp-module-ngs-qc/ngs-qc/#fixed-length-trimming","title":"Fixed Length Trimming","text":"<p>We will not cover Fixed Length Trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the <code>fastx_trimmer</code> from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type <code>fastx_trimmer -h</code> at anytime to display help.</p> <p>We will now do fixed-length trimming of the <code>bad_example.fastq</code> file using the following command. You should still be in the qc directory, if not cd back in.</p> <pre><code>cd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n</code></pre> <p>We used the following options in the command above:</p> <p>-Q 33 :   Indicates the input quality scores are Phred+33 encoded</p> <p>-f :   First base to be retained in the output</p> <p>-l :   Last base to be retained in the output</p> <p>-i :   Input FASTQ file name</p> <p>-o :   Output file name</p> <p>Run FastQC on the trimmed file and visualise the quality scores of the trimmed file.</p> <pre><code>fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &amp;\n</code></pre> <p>The output should look like:</p>    Property Value     Filename bad_example_trimmed01.fastq   File type Conventional base calls   Encoding Sanger / Illumina 1.9   Total Sequences 40000   Filtered Sequences 0   Sequence length 80   %GC 48    <p>Table 5:Summary Statistics of bad_example_trimmed summary </p>  <p>Figure 3: bad_example_trimmed_plot</p> <p>What values would you use for <code>-f</code> if you wanted to trim off 10 bases at the 5\u2019 end of the reads?</p> <p><code>-f 11</code></p>"},{"location":"modules/btp-module-rna-seq/rna-seq/","title":"RNA-Seq","text":""},{"location":"modules/btp-module-rna-seq/rna-seq/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Understand and perform a simple RNA-Seq analysis workflow.</p> </li> <li> <p>Perform spliced alignments to an indexed reference genome using     TopHat.</p> </li> <li> <p>Visualize spliced transcript alignments in a genome browser such as     IGV.</p> </li> <li> <p>Be able to identify differential gene expression between two     experimental conditions.</p> </li> <li> <p>Be familiar with R environment and be able to run R based RNA-seq     packages.</p> </li> </ul> <p>We also have bonus exercises where you can learn to:</p> <ul> <li> <p>Perform transcript assembly using Cufflinks.</p> </li> <li> <p>Run cuffdiff, a Cufflinks utility for differential expression     analysis.</p> </li> <li> <p>Visualize transcript alignments and annotation in a genome browser     such as IGV.</p> </li> </ul>"},{"location":"modules/btp-module-rna-seq/rna-seq/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/btp-module-rna-seq/rna-seq/#tools-used","title":"Tools Used","text":"<p>Tophat: https://ccb.jhu.edu/software/tophat/index.shtml</p> <p>Cufflinks: http://cole-trapnell-lab.github.io/cufflinks/</p> <p>Samtools: http://www.htslib.org/</p> <p>BEDTools: https://github.com/arq5x/bedtools2</p> <p>UCSC tools: http://hgdownload.cse.ucsc.edu/admin/exe/</p> <p>IGV: http://www.broadinstitute.org/igv/</p> <p>FeatureCount: http://subread.sourceforge.net/</p> <p>edgeR package: https://bioconductor.org/packages/release/bioc/html/edgeR.html</p> <p>CummeRbund manual: http://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#sources-of-data","title":"Sources of Data","text":"<p>http://www.ebi.ac.uk/ena/data/view/ERR022484 http://www.ebi.ac.uk/ena/data/view/ERR022485 http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#author-information","title":"Author Information","text":"<p>Primary Author(s):     Susan M Corley s.corley@unsw.edu.au     Sonika Tyagi sonika.tyagi@agrf.org.au </p> <p>Contributor(s):     Nathan S. Watson-Haigh nathan.haigh@acpfg.com.au      Myrto Kostadima, EMBL-EBI kostadmi@ebi.ac.uk      Remco Loos, EMBL-EBI remco@ebi.ac.uk</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to perform some basic tasks in the downstream analysis of RNA-seq data.</p> <p>First we will use RNA-seq data from zebrafish. You will align one set of reads to the zebrafish using Tophat2. You will then view the aligned reads using the IGV viewer. We will also demonstrate how gene counts can be derived from this data. You will go on to assembly a transcriptome from the read data using cufflinks. We will show you how this type of data may be analysed for differential expression.</p> <p>The second part of the tutorial will focus on RNA-seq data from a human experiment (cancer cell line versus normal cells). You will use the Bioconductor packages edgeR and voom (limma) to determine differential gene expression. The results from this analysis will then be used in the final session which introduces you to some of the tools used to gain biological insight into the results of a differential expression analysis</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#prepare-the-environment","title":"Prepare the Environment","text":"<p>We will use a dataset derived from sequencing of mRNA from <code>Danio rerio</code> embryos in two different developmental stages. Sequencing was performed on the Illumina platform and generated 76bp paired-end sequence data using polyA selected RNA. Due to the time constraints of the practical we will only use a subset of the reads.</p> <p>The data files are contained in the subdirectory called <code>data</code> and are the following:</p> <p><code>2cells_1.fastq</code> and <code>2cells_2.fastq</code>     These files are based on RNA-seq data of a 2-cell zebrafish embryo</p> <p><code>6h_1.fastq</code> and <code>6h_2.fastq</code>     These files are based on RNA-seq data of zebrafish embryos 6h post     fertilization</p> <p>Open the Terminal and go to the <code>rnaseq</code> working directory:</p> <pre><code>cd /home/trainee/rnaseq/\n</code></pre>  <p>STOP</p> <p>All commands entered into the terminal for this tutorial should be from within the <code>/home/trainee/rnaseq</code> directory.</p>  <p>Check that the <code>data</code> directory contains the above-mentioned files by typing:</p> <pre><code>ls data\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#alignment","title":"Alignment","text":"<p>There are numerous tools for performing short read alignment and the choice of aligner should be carefully made according to the analysis goals/requirements. Here we will use Tophat2, a widely used ultrafast aligner that performs spliced alignments.</p> <p>Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for the alignment to speed up the alignment and keep its memory footprint small. The the index for the <code>Danio rerio</code> genome has been created for you.</p>  <p>STOP</p> <p>The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you.</p> <p><code>bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9</code></p>  <p>Tophat2 has a number of parameters in order to perform the alignment. To view them all type:</p> <pre><code>tophat2 --help\n</code></pre> <p>The general format of the tophat2 command is:</p> <pre><code>tophat2 [options]* &lt;index_base&gt; &lt;reads_1&gt; &lt;reads_2&gt;\n</code></pre> <p>Where the last two arguments are the <code>.fastq</code> files of the paired end reads, and the argument before is the basename of the indexed genome.</p> <p>The quality values in the FASTQ files used in this hands-on session are Phred+33 encoded. We explicitly tell tophat of this fact by using the command line argument <code>\u2013solexa-quals</code>.</p> <p>You can look at the first few reads in the file <code>data/2cells_1.fastq</code> with:</p> <pre><code>head -n 20 data/2cells_1.fastq\n</code></pre> <p>Some other parameters that we are going to use to run Tophat are listed below:</p> <p>-g :   Maximum number of multihits allowed. Short reads are likely to map     to more than one location in the genome even though these reads can     have originated from only one of these regions. In RNA-seq we allow     for a limited number of multihits, and in this case we ask Tophat to     report only reads that map at most onto 2 different loci.</p> <p>\u2013library-type :   Before performing any type of RNA-seq analysis you need to know a     few things about the library preparation. Was it done using a     strand-specific protocol or not? If yes, which strand? In our data     the protocol was NOT strand specific.</p> <p>-j :   Improve spliced alignment by providing Tophat with annotated splice     junctions. Pre-existing genome annotation is an advantage when     analysing RNA-seq data. This file contains the coordinates of     annotated splice junctions from Ensembl. These are stored under the     sub-directory <code>annotation</code> in a file called <code>ZV9.spliceSites</code>.</p> <p>-o :   This specifies in which subdirectory Tophat should save the output     files. Given that for every run the name of the output files is the     same, we specify different directories for each run.</p> <p>It takes some time (approx. 20 min) to perform tophat spliced alignments, on this small subset of reads. Therefore, we have pre-aligned the <code>2cells</code> data for you using the following command:</p> <p>You DO NOT need to run this command yourself - we have done this for you.</p> <pre><code>tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n</code></pre> <p>Align the <code>6h</code> data yourself using the following command:</p> <pre><code># Takes approx. 20mins\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq\n</code></pre> <p>The <code>6h</code> read alignment will take approx. 20 min to complete. Therefore, we\u2019ll take a look at some of the files, generated by tophat, for the pre-computed <code>2cells</code> data.</p> <p>Tophat generates several files in the specified output directory. The most important files are listed below.</p> <p>accepted_hits.bam :   This file contains the list of read alignments in BAM format.</p> <p>align_summary.txt :   Provides a detailed summary of the read-alignments.</p> <p>unmapped.bam :   This file contains the unmapped reads.</p> <p>The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#alignment-visualisation-in-igv","title":"Alignment Visualisation in IGV","text":"<p>The Integrative Genomics Viewer (IGV) is able to provide a visualisation of read alignments given a reference sequence and a BAM file. We\u2019ll visualise the information contained in the <code>accepted_hits.bam</code> and <code>junctions.bed</code> files for the pre-computed <code>2cells</code> data. The former, contains the tophat spliced alignments of the reads to the reference while the latter stores the coordinates of the splice junctions present in the data set.</p> <p>Open the <code>rnaseq</code> directory on your Desktop and double-click the <code>tophat</code> subdirectory and then the <code>ZV9_2cells</code> directory.</p> <ol> <li> <p>Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop     (ignore any warnings that you may get as it opens). NOTE: IGV may     take several minutes to load for the first time, please be patient.</p> </li> <li> <p>Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of     the IGV window. Else you can also load the genome fasta file.</p> </li> <li> <p>Load the <code>accepted_hits.sorted.bam</code> file by clicking the \u201cFile\u201d     menu, selecting \u201cLoad from File\u201d and navigating to the     <code>Desktop/rnaseq/tophat/ZV9_2cells</code> directory.</p> </li> <li> <p>Rename the track by right-clicking on its name and choosing \u201cRename     Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d.</p> </li> <li> <p>Load the <code>junctions.bed</code> from the same directory and rename the     track \u201c2cells Junctions BED\u201d.</p> </li> <li> <p>Load the Ensembl annotations file <code>Danio_rerio.Zv9.66.gtf</code> stored in     the <code>rnaseq/annotation</code> directory.</p> </li> <li> <p>Navigate to a region on chromosome 12 by typing     <code>chr12:20,270,921-20,300,943</code> into the search box at the top of the     IGV window.</p> </li> </ol> <p>Keep zooming to view the bam file alignments</p> <p>Some useful IGV manuals can be found below</p> <p>http://www.broadinstitute.org/software/igv/interpreting_insert_size http://www.broadinstitute.org/software/igv/alignmentdata </p>  <p>Question</p> <p>Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide?</p>    Answer <p>As the name suggests, the file provides a details summary of the alignment statistics.</p>   <p>One other important file is \u2019unmapped.bam\u2019. This file contains the unampped reads.</p>  <p>Question</p> <p>Can you identify the splice junctions from the BAM file?</p>    Answer <p>Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns).</p>    <p>Question</p> <p>Are the junctions annotated for <code>CBY1</code> consistent with the annotation?</p>    Answer <p>Read alignment supports an extended length in exon 5 to the gene model(cby1-001)</p>   <p>Once tophat finishes aligning the 6h data you will need to sort the alignments found in the BAM file and then index the sorted BAM file.</p> <pre><code>samtools sort tophat/ZV9_6h/accepted_hits.bam -o tophat/ZV9_6h/accepted_hits.sorted.bam\nsamtools index tophat/ZV9_6h/accepted_hits.sorted.bam\n</code></pre> <p>Load the sorted BAM file into IGV, as described previously, and rename the track appropriately.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#generating-gene-counts","title":"Generating Gene Counts","text":"<p>In RNA-seq experiments the digital gene expression is recorded as the gene counts or number of reads aligning to a known gene feature. If you have a well annotated genome, you can use the gene structure file in a standard gene annotation format (GTF or GFF)) along with the spliced alignment file to quantify the known genes. We will demonstrate a utility called <code>FeatureCounts</code> that comes with the <code>Subread</code> package.</p> <pre><code>mkdir gene_counts\nfeatureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#isoform-expression-and-transcriptome-assembly","title":"Isoform Expression and Transcriptome Assembly","text":"<p>For non-model organisms and genomes with draft assemblies and incomplete annotations, it is a common practice to take an assembly based approach to generate gene structures followed by the quantification step. There are a number of reference based transcript assemblers available that can be used for this purpose such as, cufflinks and stringy. These assemblers can give gene or isoform level assemblies that can be used to perform a gene/isoform level quantification. These assemblers require an alignment of reads with a reference genome or transcriptome as an input. The second optional input is a known gene structure in <code>GTF</code> or <code>GFF</code> format.</p> <p>There are a number of tools that perform reconstruction of the transcriptome and for this workshop we are going to use Cufflinks. Cufflinks can do transcriptome assembly either <code>ab initio</code> or using a reference annotation. It also quantifies the isoform expression in Fragments Per Kilobase of exon per Million fragments mapped (FPKM).</p> <p>Cufflinks has a number of parameters in order to perform transcriptome assembly and quantification. To view them all type:</p> <pre><code>cufflinks --help\n</code></pre> <p>We aim to reconstruct the transcriptome for both samples by using the Ensembl annotation both strictly and as a guide. In the first case Cufflinks will only report isoforms that are included in the annotation, while in the latter case it will report novel isoforms as well.</p> <p>The Ensembl annotation for <code>Danio rerio</code> is available in <code>annotation/Danio_rerio.Zv9.66.gtf</code>.</p> <p>The general format of the <code>cufflinks</code> command is:</p> <pre><code>cufflinks [options]* &lt;aligned_reads.(sam|bam)&gt;\n</code></pre> <p>Where the input is the aligned reads (either in SAM or BAM format).</p> <p>Some of the available parameters for Cufflinks that we are going to use to run Cufflinks are listed below:</p> <p>-o :   Output directory.</p> <p>-G :   Tells Cufflinks to use the supplied GTF annotations strictly in     order to estimate isoform annotation.</p> <p>-b :   Instructs Cufflinks to run a bias detection and correction algorithm     which can significantly improve accuracy of transcript abundance     estimates. To do this Cufflinks requires a multi-fasta file with the     genomic sequences against which we have aligned the reads.</p> <p>-u :   Tells Cufflinks to do an initial estimation procedure to more     accurately weight reads mapping to multiple locations in the genome     (multi-hits).</p> <p>\u2013library-type :   Before performing any type of RNA-seq analysis you need to know a     few things about the library preparation. Was it done using a     strand-specific protocol or not? If yes, which strand? In our data     the protocol was NOT strand specific.</p> <p>Perform transcriptome assembly, strictly using the supplied GTF annotations, for the <code>2cells</code> and <code>6h</code> data using cufflinks:</p> <pre><code># 2cells data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n</code></pre> <p>Cufflinks generates several files in the specified output directory. Here\u2019s a short description of these files:</p> <p>genes.fpkm_tracking :   Contains the estimated gene-level expression values.</p> <p>isoforms.fpkm_tracking :   Contains the estimated isoform-level expression values.</p> <p>skipped.gtf :   Contains loci skipped as a result of exceeding the maximum number of     fragments.</p> <p>transcripts.gtf :   This GTF file contains Cufflinks\u2019 assembled isoforms.</p> <p>The complete documentation can be found at:</p> <p>http://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite</p> <p>So far we have forced cufflinks, by using the <code>-G</code> option, to strictly use the GTF annotations provided and thus novel transcripts will not be reported. We can get cufflinks to perform a GTF-guided transcriptome assembly by using the <code>-g</code> option instead. Thus, novel transcripts will be reported.</p> <p>GTF-guided transcriptome assembly is more computationally intensive than strictly using the GTF annotations. Therefore, we have pre-computed these GTF-guided assemblies for you and have placed the results under subdirectories:</p> <p><code>cufflinks/ZV9_2cells_gtf_guided</code> and <code>cufflinks/ZV9_6h_gft_guided</code>.</p> <p>You DO NOT need to run these commands. We provide them so you know how we generated the the GTF-guided transcriptome assemblies:</p> <pre><code># 2cells guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n</code></pre> <ol> <li> <p>Go back to IGV and load the pre-computed, GTF-guided transcriptome     assembly for the <code>2cells</code> data     (<code>cufflinks/ZV9_2cells_gtf_guided/transcripts.gtf</code>).</p> </li> <li> <p>Rename the track as \u201c2cells GTF-Guided Transcripts\u201d.</p> </li> <li> <p>In the search box type <code>ENSDART00000082297</code> in order for the browser     to zoom in to the gene of interest.</p> </li> </ol>  <p>Question</p> <p>Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts  assembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)?</p>    Answer <p>Yes. It appears that the Ensembl annotations may have truncated the last exon.  However, our data also doesn\u2019t contain reads that span between the last two exons.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#differential-gene-expression-analysis-using-edger","title":"Differential Gene Expression Analysis using edgeR","text":""},{"location":"modules/btp-module-rna-seq/rna-seq/#experiment-design","title":"Experiment Design","text":"<p>The example we are working through today follows a case Study set out in the edgeR Users Guide (4.3 Androgen-treated prostate cancer cells (RNA-Seq, two groups)) which is based on an experiment conducted by Li et al. (2008, Proc Natl Acad Sci USA, 105, 20179-84).</p> <p>The researchers used a prostate cancer cell line (LNCaP cells). These cells are sensitive to stimulation by male hormones (androgens). Three replicate RNA samples were collected from LNCaP cells treated with an androgen hormone (DHT). Four replicates were collected from cells treated with an inactive compound. Each of the seven samples was run on a lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The experimental design was therefore 4 control samples vs 3 treated samples.</p> <p>This workflow requires raw gene count files and these can be generated using a utility called featureCounts as demonstrated above. We are using a pre-computed gene counts data (stored in <code>pnas_expression.txt</code>) for this exercise.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#prepare-the-environment_1","title":"Prepare the Environment","text":"<p>Prepare the environment and load R:</p> <pre><code>cd /home/trainee/rnaseq/edgeR\nR (press enter)\n</code></pre> <p>Once on the R prompt. Load libraries:</p> <pre><code>library(edgeR)\nlibrary(biomaRt)\nlibrary(gplots)\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(org.Hs.eg.db)\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#read-in-data","title":"Read in Data","text":"<p>Read in count table and experimental design:</p> <pre><code>data &lt;- read.delim(\"pnas_expression.txt\", row.names=1, header=T)\ntargets &lt;- read.delim(\"Targets.txt\", header=T)\ncolnames(data) &lt;-targets$Label\nhead(data, n=20)\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#add-gene-annotation","title":"Add Gene annotation","text":"<p>The data set only includes the Ensembl gene id and the counts. It is useful to have other annotations such as the gene symbol and entrez id. Next we will add in these annotations. We will use the BiomaRt package to do this.</p> <p>We start by using the useMart function of BiomaRt to access the human data base of ensemble gene ids.</p> <pre><code>human&lt;-useMart(host=\"www.ensembl.org\", \"ENSEMBL_MART_ENSEMBL\", dataset=\"hsapiens_gene_ensembl\") \nattributes=c(\"ensembl_gene_id\", \"entrezgene\",\"hgnc_symbol\")\n</code></pre> <p>We create a vector of our ensemble gene ids.</p> <pre><code>ensembl_names&lt;-rownames(data)\nhead(ensembl_names)\n</code></pre> <p>We then use the function getBM to get the gene symbol data we want.This takes about a minute.</p> <pre><code>genemap&lt;-getBM(attributes, filters=\"ensembl_gene_id\", values=ensembl_names, mart=human)\n</code></pre> <p>Have a look at the start of the genemap dataframe.</p> <pre><code>head(genemap)\n</code></pre> <p>We then match the data we have retrieved to our dataset.</p> <pre><code>idx &lt;-match(ensembl_names, genemap$ensembl_gene_id)\ndata$entrezgene &lt;-genemap$entrezgene [ idx ]\ndata$hgnc_symbol &lt;-genemap$hgnc_symbol [ idx ]\nAnn &lt;- cbind(rownames(data), data$hgnc_symbol, data$entrezgene)\ncolnames(Ann)&lt;-c(\"Ensembl\", \"Symbol\", \"Entrez\")\nAnn&lt;-as.data.frame(Ann)\n</code></pre> <p>Let\u2019s check and see that this additional information is there.</p> <pre><code>head(data)\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#data-checks","title":"Data checks","text":"<p>Create DGEList object:</p> <pre><code>treatment &lt;-factor(c(rep(\"Control\",4), rep(\"DHT\",3)), levels=c(\"Control\", \"DHT\"))\ny &lt;-DGEList(counts=data[,1:7], group=treatment, genes=Ann)\n</code></pre> <p>Check the dimensions of the object:</p> <pre><code>dim(y)\n</code></pre> <p>We see we have 37435 rows (i.e. genes) and 7 columns (samples).</p> <p>Now we will filter out genes with low counts by only keeping those rows where the count per million (cpm) is at least 1 in at least three samples:</p> <pre><code>keep &lt;-rowSums( cpm(y)&gt;1) &gt;=3\ny &lt;- y[keep, ]\n</code></pre> <p>Check how many rows (genes) are retained now.</p> <pre><code>dim(y)\n</code></pre> <p>This gives you 16494 rows indicating that 20941 (37435-16494) genes were filtered out.</p> <p>As we have removed the lowly expressed genes the total number of counts per sample has not changed greatly. Let us check the total number of reads per sample in the original data (data) and now after filtering.</p> <p>Before:</p> <pre><code>colSums(data[,1:7])\n</code></pre> <p>After filtering:</p> <pre><code>colSums(y$counts)\n</code></pre> <p>We will now perform normalization to take account of different library size:</p> <pre><code>y&lt;-calcNormFactors(y)\n</code></pre> <p>We will check the calculated normalization factors:</p> <pre><code>y$samples\n</code></pre> <p>Lets have a look at whether the samples cluster by condition. (You should produce a plot as shown in Figure 4):</p> <pre><code>plotMDS(y, col=as.numeric(y$samples$group))\n</code></pre> <p>[H]  [fig:MDS plot]</p>  <p>Question</p> <p>Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples?</p>    Answer <p>The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions.</p>   <p>We will now estimate the dispersion. We start by estimating the common dispersion. The common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes.</p> <p>By using verbose we get the Disp and BCV values printed on the screen</p> <pre><code>y &lt;- estimateCommonDisp(y, verbose=T)\n</code></pre> <p>What value to you see for BCV?</p> <p>We now estimate gene-specific dispersion.</p> <pre><code>y &lt;- estimateTagwiseDisp(y)\n</code></pre> <p>We will plot the tagwise dispersion and the common dispersion (You should obtain a plot as shown in the Figure 5):</p> <pre><code>plotBCV(y)\n</code></pre> <p>[H]  [fig:BCV plot]</p> <p>We see here that the common dispersion estimates the overall Biological Coefficient of Variation (BCV) of the dataset averaged over all genes. The common dispersion is <code>0.02</code> and the BCV is the square root of the common dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell line experiment.</p> <p>As you can see from the plot the BCV of some genes (generally those with low expression) can be much higher than the common dispersion. For example we see genes with a reasonable level of expression with tagwise dispersion of 0.4 indicating 40% variation between samples.</p>  <p>Question</p> <p>If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have?</p>    Answer <p>If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions. </p>   <p>It is recommended to use the tagwise dispersion,which takes account of gene-to-gene variability.</p> <p>Now that we have normalized our data and also calculated the variability of gene expression between samples we are in a position to perform differential expression testing.As this is a simple comparison between two conditions, androgen treatment and placebo treatment we can use the exact test for the negative binomial distribution (Robinson and Smyth, 2008).</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#testing-for-differential-expression","title":"Testing for Differential Expression","text":"<p>We now test for differentially expressed BCV genes:</p> <pre><code>et &lt;- exactTest(y)\n</code></pre> <p>Now we will use the topTags function to adjust for multiple testing. We will use the Benjimini Hochberg (\u201cBH\u201d) method and we will produce a table of results:</p> <pre><code>res &lt;- topTags(et, n=nrow(y$counts), adjust.method=\"BH\")$table\n</code></pre> <p>Let\u2019s have a look at the first rows of the table:</p> <pre><code>head(res)\n</code></pre> <p>To get a summary of the number of differentially expressed genes we can use the decideTestsDGE function.</p> <pre><code>summary(de &lt;- decideTestsDGE(et))\n</code></pre> <p>This tells us that 2086 genes are downregulated and 2345 genes are upregulated at 5% FDR.We will now make subsets of the most significant upregulated and downregulated genes using a log fold change threshhold of 1.5.</p> <pre><code>alpha=0.05\nlfc=1.5\nedgeR_res_sig&lt;-res[res$FDR&lt;alpha,]\nedgeR_res_sig_lfc &lt;-edgeR_res_sig[abs(edgeR_res_sig$logFC) &gt;= lfc,]\nhead(edgeR_res_sig, n=20)\nnrow(edgeR_res_sig)\nnrow(edgeR_res_sig_lfc)\n</code></pre> <p>We can write out these results to our current directory.</p> <pre><code>write.table(edgeR_res_sig , \"edgeR_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F)\nwrite.table(edgeR_res_sig_lfc , \"edgeR_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F)\n</code></pre>  <p>Question</p> <p>How many differentially expressed genes are are found by edgeR before and after requiring the log fold change of 1.5?</p>    Answer <p>There are 4431 DEGs at an FDR = 0.05 which reduces to 1148 if we require a log fold change of 1.5.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#differential-expression-using-the-voom-function-and-the-limma-package","title":"Differential expression using the Voom function and the limma package","text":"<p>We will now show an alternative approach to differential expression which uses the <code>limma</code> package.This is based on linear models. The first step is to create a design matrix. In this case we have a simple design where we have only one condition (treated vs non-treated). However, you may be dealing with more complex experimental designs, for example looking at treatment and other covariates, such as age, gender, batch.</p> <pre><code>design &lt;-model.matrix(~treatment)\ncheck design\nprint(design)\n</code></pre> <p>We now use voom to transform the data into a form which is appropriate for linear modelling.</p> <pre><code>v &lt;-voom(y, design)\n</code></pre> <p>Next we will fit linear model to each gene in the dataset using the function lmFit. Following this we use the function eBayes to test each gene to find whether foldchange between the conditions being tested is statistically significant.We filter our results by using the same values of alpha (0.05) and log fold change (1.5) used previously.</p> <pre><code>fit_v &lt;-lmFit(v, design)\n\nfit_v &lt;- eBayes(fit_v)\n\nvoom_res&lt;-topTable(fit_v, coef=2,adjust.method=\"BH\", sort.by=\"P\", n=nrow(y$counts))\n\nvoom_res_sig &lt;-voom_res[voom_res$adj.P.Val &lt;alpha,]\n\nvoom_res_sig_lfc &lt;-voom_res_sig[abs(voom_res_sig$logFC) &gt;= lfc,]\n</code></pre> <p>How many differentially expressed genes are identified?</p> <pre><code>nrow(voom_res_sig)\nnrow(voom_res_sig_lfc)\n</code></pre>  <p>Question</p> <p>How many differentially expressed genes are found using voom before and after requiring the log fold change of 1.5?</p>    Answer <p>There are 4103 DEGs at an FDR = 0.05 which reduces to 1109 if we require a log fold change of 1.5.</p>   <p>We will write out these results.</p> <pre><code>write.table(voom_res_sig, \"voom_res_sig.txt\", sep=\"\\t\", col.names=NA, quote=F)\nwrite.table(voom_res_sig_lfc, \"voom_res_sig_lfc.txt\", sep=\"\\t\", col.names=NA, quote=F)\nwrite.table(voom_res, \"voom_res.txt\", sep=\"\\t\", col.names=NA, quote=F)\n</code></pre>"},{"location":"modules/btp-module-rna-seq/rna-seq/#data-visualisation","title":"Data Visualisation","text":"<p>Now let\u2019s visualize some of this data. First we will make a volcano plot using the <code>volcanoplot</code> function available in <code>limma</code>. This creates a plot which displays fold changes versus a measure of statistical significance of the change.</p> <pre><code>volcanoplot(fit_v, coef=2, highlight=5)\n</code></pre> <p>[H]  [fig:Volcano plot]</p> <p>Next we will create a heatmap of the top differentially expressed genes. We use the heatmap.2 function available in the gplots package.</p> <pre><code>select_top  &lt;- p.adjust(fit_v$p.value[, 2]) &lt;1e-2\nExp_top &lt;- v$E [select_top, ]\nheatmap.2(Exp_top, scale=\"row\", density.info=\"none\", trace=\"none\", main=\"Top DEGs\", labRow=\"\", cexRow=0.4, cexCol=0.8)\n</code></pre> <p>[H]  [fig:Heatmap]</p> <p>You can now quit the R prompt</p> <pre><code>q()\n</code></pre> <p>You can save your workspace by typing <code>Y</code> on prompt.</p> <p>Please note that the output files you are creating are saved in your present working directory. If you are not sure where you are in the file system try typing <code>pwd</code> on your command prompt to find out.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#differential-expression-using-cuffdiff","title":"Differential Expression using cuffdiff","text":"<p>This is optional exercise and will be run if time permits.</p> <p>NOTE: If this exercise is to be attempted it is necessary to leave the directory we have been working in and go back to the zebra fish data. Commands should be executed from the rnaseq directory.</p> <p>One of the stand-alone tools that perform differential expression analysis is Cuffdiff. We use this tool to compare between two conditions; for example different conditions could be control and disease, or wild-type and mutant, or various developmental stages.</p> <p>In our case we want to identify genes that are differentially expressed between two developmental stages; a <code>2cells</code> embryo and <code>6h</code> post fertilization.</p> <p>The general format of the cuffdiff command is:</p> <pre><code>cuffdiff [options]* &lt;transcripts.gtf&gt; &lt;sample1_replicate1.sam[,...,sample1_replicateM]&gt; &lt;sample2_replicate1.sam[,...,sample2_replicateM.sam]&gt;\n</code></pre> <p>Where the input includes a <code>transcripts.gtf</code> file, which is an annotation file of the genome of interest or the cufflinks assembled transcripts, and the aligned reads (either in SAM or BAM format) for the conditions. Some of the Cufflinks options that we will use to run the program are:</p> <p>-o :   Output directory.</p> <p>-L :   Labels for the different conditions</p> <p>-T :   Tells Cuffdiff that the reads are from a time series experiment.</p> <p>-b :   Instructs Cufflinks to run a bias detection and correction algorithm     which can significantly improve accuracy of transcript abundance     estimates. To do this Cufflinks requires a multi-fasta file with the     genomic sequences against which we have aligned the reads.</p> <p>-u :   Tells Cufflinks to do an initial estimation procedure to more     accurately weight reads mapping to multiple locations in the genome     (multi-hits).</p> <p>\u2013library-type :   Before performing any type of RNA-seq analysis you need to know a     few things about the library preparation. Was it done using a     strand-specific protocol or not? If yes, which strand? In our data     the protocol was NOT strand specific.</p> <p>-C :   Biological replicates and multiple group contrast can be defined     here</p> <p>Run cuffdiff on the tophat generated BAM files for the 2cells vs. 6h data sets:</p> <pre><code>cuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam\n</code></pre> <p>We are interested in the differential expression at the gene level. The results are reported by Cuffdiff in the file <code>cuffdiff/gene_exp.diff</code>. Look at the first few lines of the file using the following command:</p> <pre><code>head -n 20 cuffdiff/gene_exp.diff\n</code></pre> <p>We would like to see which are the most significantly differentially expressed genes. Therefore we will sort the above file according to the q value (corrected p value for multiple testing). The result will be stored in a different file called <code>gene_exp_qval.sorted.diff</code>.</p> <pre><code>sort -t$'\\t' -g -k 13 cuffdiff/gene_exp.diff &gt; cuffdiff/gene_exp_qval.sorted.diff\n</code></pre> <p>Look again at the top 20 lines of the sorted file by typing:</p> <pre><code>head -n 20 cuffdiff/gene_exp_qval.sorted.diff\n</code></pre> <p>Copy an Ensembl transcript identifier from the first two columns for one of these genes (e.g. <code>ENSDARG00000045067</code>). Now go back to the IGV browser and paste it in the search box.</p> <p>What are the various outputs generated by cuffdiff? Hint: Please refer to the <code>Cuffdiff output</code> section of the cufflinks manual online.</p> <p>Do you see any difference in the read coverage between the <code>2cells</code> and <code>6h</code> conditions that might have given rise to this transcript being called as differentially expressed?</p> <p>The coverage on the Ensembl browser is based on raw reads and no normalisation has taken place contrary to the FPKM values.</p> <p>The read coverage of this transcript (<code>ENSDARG00000045067</code>) in the 6h data set is much higher than in the 2cells data set.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#visualising-the-cuffdiff-expression-analysis","title":"Visualising the CuffDiff expression analysis","text":"<p>We will use an R-Bioconductor package called <code>cummeRbund</code> to visualise, manipulate and explore Cufflinks RNA-seq output. We will load an R environment and look at few quick tips to generate simple graphical output of the cufflinks analysis we have just run.</p> <p><code>CummeRbund</code> takes the cuffdiff output and populates a SQLite database with various type of output generated by cuffdiff e.g, genes, transcripts, transcription start site, isoforms and CDS regions. The data from this database can be accessed and processed easily. This package comes with a number of in-built plotting functions that are commonly used for visualising the expression data. We strongly recommend reading through the bioconductor manual and user guide of CummeRbund to learn about functionality of the tool. The reference is provided in the resource section.</p> <p>Prepare the environment. Go to the <code>cuffdiff</code> output folder and copy the transcripts file there.</p> <pre><code>cd /home/trainee/rnaseq/cuffdiff\ncp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff\nls -l\n</code></pre> <p>Load the R environment</p> <pre><code>R (press enter)\n</code></pre> <p>Load the require R package.</p> <pre><code>library(cummeRbund)\n</code></pre> <p>Read in the cuffdiff output</p> <pre><code>cuff&lt;-readCufflinks(dir=\"/home/trainee/rnaseq/cuffdiff\u201d, gtfFile='Danio_rerio.Zv9.66.gtf',genome=\"Zv9\", rebuild=T)\n</code></pre> <p>Assess the distribution of FPKM scores across samples</p> <pre><code>pdf(file = \"SCV.pdf\", height = 6, width = 6)\ndens&lt;-csDensity(genes(cuff))\ndens\ndev.off()\n</code></pre> <p>Box plots of the FPKM values for each samples</p> <pre><code>pdf(file = \"BoxP.pdf\", height = 6, width = 6)\nb&lt;-csBoxplot(genes(cuff))\nb\ndev.off()\n</code></pre> <p>Accessing the data</p> <pre><code>sigGeneIds&lt;-getSig(cuff,alpha=0.05,level=\"genes\")\nhead(sigGeneIds)\nsigGenes&lt;-getGenes(cuff,sigGeneIds)\nsigGenes\nhead(fpkm(sigGenes))\nhead(fpkm(isoforms(sigGenes)))\n</code></pre> <p>Plotting a heatmap of the differentially expressed genes</p> <pre><code>pdf(file = \"heatmap.pdf\", height = 6, width = 6)\nh&lt;-csHeatmap(sigGenes,cluster=\"both\")\nh\ndev.off()\n</code></pre> <p>What options would you use to draw a density or boxplot for different replicates if available ? (Hint: look at the manual at Bioconductor website)</p> <pre><code>densRep&lt;-csDensity(genes(cuff),replicates=T)\nbrep&lt;-csBoxplot(genes(cuff),replicates=T)\n</code></pre> <p>How many differentially expressed genes did you observe?</p> <p>type \u2019summary(sigGenes)\u2019 on the R prompt to see.</p>"},{"location":"modules/btp-module-rna-seq/rna-seq/#references","title":"References","text":"<ol> <li> <p>Trapnell, C., Pachter, L. &amp; Salzberg, S. L. TopHat: discovering     splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009).</p> </li> <li> <p>Trapnell, C. et al. Transcript assembly and quantification by     RNA-Seq reveals unannotated transcripts and isoform switching during     cell differentiation. Nat. Biotechnol. 28, 511-515 (2010).</p> </li> <li> <p>Langmead, B., Trapnell, C., Pop, M. &amp; Salzberg, S. L. Ultrafast and     memory-efficient alignment of short DNA sequences to the human     genome. Genome Biol. 10, R25 (2009).</p> </li> <li> <p>Roberts, A., Pimentel, H., Trapnell, C. &amp; Pachter, L. Identification     of novel transcripts in annotated genomes using RNA-Seq.     Bioinformatics 27, 2325-2329 (2011).</p> </li> <li> <p>Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. &amp; Pachter, L.     Improving RNA-Seq expression estimates by correcting for fragment     bias. Genome Biol. 12, R22 (2011).</p> </li> <li> <p>Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package     for differential expression analysis of digital gene expression     data. Bioinformatics, 26 (2010).</p> </li> <li> <p>Robinson MD and Smyth GK Moderated statistical tests for assessing     differences in tag abundance. Bioinformatics, 23, pp. -6.</p> </li> <li> <p>Robinson MD and Smyth GK (2008). Small-sample estimation of negative     binomial dispersion, with applications to SAGE data.\u201d Biostatistics,     9.</p> </li> <li> <p>McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential     expression analysis of multifactor RNA-Seq experiments with respect     to biological variation. Nucleic Acids Research, 40(10), pp. -9.</p> </li> </ol>"},{"location":"modules/btp-module-velvet/velvet/","title":"de novo Genome Assembly","text":""},{"location":"modules/btp-module-velvet/velvet/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Compile velvet with appropriate compile-time parameters set for a     specific analysis</p> </li> <li> <p>Be able to choose appropriate assembly parameters</p> </li> <li> <p>Assemble a set of paired-end reads from a single insert-size library</p> </li> <li> <p>Be able to visualise an assembly in AMOS Hawkeye</p> </li> </ul>"},{"location":"modules/btp-module-velvet/velvet/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":"<p>Although we have provided you with an environment which contains all the tools and data you will be using in this module, you may like to know where we have sourced those tools and data from.</p>"},{"location":"modules/btp-module-velvet/velvet/#tools-used","title":"Tools Used","text":"<p>Velvet: http://www.ebi.ac.uk/~zerbino/velvet/</p> <p>AMOS Hawkeye: http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye</p> <p>gnx-tools: https://github.com/mh11/gnx-tools</p> <p>FastQC: http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/</p> <p>R: http://www.r-project.org/</p>"},{"location":"modules/btp-module-velvet/velvet/#sources-of-data","title":"Sources of Data","text":"<ul> <li> <p>ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz</p> </li> <li> <p>http://www.ebi.ac.uk/ena/data/view/SRX008042</p> </li> <li> <p>ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz</p> </li> <li> <p>ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz</p> </li> <li> <p>ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz</p> </li> <li> <p>ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz</p> </li> </ul>"},{"location":"modules/btp-module-velvet/velvet/#introduction","title":"Introduction","text":"<p>The aim of this module is to become familiar with performing de novo genome assembly using Velvet, a de Bruijn graph based assembler, on a variety of sequence data.</p>"},{"location":"modules/btp-module-velvet/velvet/#prepare-the-environment","title":"Prepare the Environment","text":"<p>The first exercise should get you a little more comfortable with the computer environment and the command line.</p> <p>First make sure that you are in the denovo working directory by typing:</p> <pre><code>cd /home/trainee/denovo\n</code></pre> <p>and making absolutely sure you\u2019re there by typing:</p> <pre><code>pwd\n</code></pre> <p>Now create sub-directories for this and the two other velvet practicals. All these directories will be made as sub-directories of a directory for the whole course called NGS. For this you can use the following commands:</p> <pre><code>mkdir -p NGS/velvet/{part1,part2}\n</code></pre> <p>The <code>-p</code> tells <code>mkdir</code> (make directory) to make any parent directories if they don\u2019t already exist. You could have created the above directories one-at-a-time by doing this instead:</p> <pre><code>mkdir NGS\nmkdir NGS/velvet\nmkdir NGS/velvet/part1\nmkdir NGS/velvet/part2\n</code></pre> <p>After creating the directories, examine the structure and move into the directory ready for the first velvet exercise by typing:</p> <pre><code>ls -R NGS\ncd NGS/velvet/part1\npwd\n</code></pre>"},{"location":"modules/btp-module-velvet/velvet/#downloading-and-compiling-velvet","title":"Downloading and Compiling Velvet","text":"<p>For the duration of this workshop, all the software you require has been set up for you already. This might not be the case when you return to \u201creal life\u201d. Many of the programs you will need, including velvet, are quite easy to set up, it might be instructive to try a couple.</p> <p>Although you will be using the preinstalled version of velvet, it is useful to know how to compile velvet as some of the parameters you might like to control can only be set at compile time. You can find the latest version of velvet at:</p> <p>http://www.ebi.ac.uk/~zerbino/velvet/</p> <p>You could go to this URL and download the latest velvet version, or equivalently, you could type the following, which will download, unpack, inspect, compile and execute your locally compiled version of velvet:</p> <pre><code>cd /home/trainee/denovo/NGS/velvet/part1\npwd\ntar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz\nls -R\ncd velvet_1.2.10\nmake\n./velveth\n</code></pre> <p>The standout displayed to screen when \u2019make\u2019 runs may contain an error message but it is ignored</p> <p>Take a look at the executables you have created. They will be displayed as green by the command:</p> <pre><code>ls --color=always\n</code></pre> <p>The switch <code>\u2013color</code>, instructs that files be coloured according to their type. This is often the default but we are just being explicit. By specifying the value <code>always</code>, we ensure that colouring is always applied, even from a script.</p> <p>Have a look of the output the command produces and you will see that <code>MAXKMERLENGTH=31</code> and <code>CATEGORIES=2</code> parameters were passed into the compiler.</p> <p>This indicates that the default compilation was set for de Bruijn graph k-mers of maximum size 31 and to allow a maximum of just 2 read categories. You can override these, and other, default configuration choices using command line parameters. Assume, you want to run velvet with a k-mer length of 41 using 3 categories, velvet needs to be recompiled to enable this functionality by typing:</p> <pre><code>make clean\nmake MAXKMERLENGTH=41 CATEGORIES=3\n./velveth\n</code></pre> <p>Discuss with the persons next to you the following questions:</p>  <p>Question</p> <p>What are the consequences of the parameters you have given make for velvet?</p>    Answer <p>MAXKMERLENGTH: increase the max k-mer length from 31 to 41    CATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data.</p>    <p>Question</p> <p>Why does Velvet use k-mer 31 and 2 categories as default?</p>    Answer <p>Possibly a number of reason:     1) odd number to avoid palindromes       2) The first reads were very short (20-40 bp) and there were hardly any paired-end data around so there was no need to allow for longer k-mer lengths / more categories.       3) For programmers: 31 bp get stored in 64 bits (using 2bit encoding)</p>    <p>Question</p> <p>Should you get better results by using a longer k-mer length?</p>    Answer <p>If you can achieve a good k-mer coverage - yes.</p>   <p>What effect would the following compile-time parameters have on velvet:</p>  <p>Question</p>  <p>OPENMP=Y</p>   Answer <p>Turn on multithreading</p>    <p>Question</p> <p>LONGSEQUENCES=Y</p>    Answer <p>Assembling reads / contigs longer than 32kb long</p>    <p>Question</p> <p>BIGASSEMBLY=Y</p>    Answer <p>Using more than 2.2 billion reads</p>    <p>Question</p> <p>SINGLE_COV_CAT=Y</p>    Answer <p>Merge all coverage statistics into a single variable - save memory</p>   <p>For a further description of velvet compile and runtime parameters please see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual</p>"},{"location":"modules/btp-module-velvet/velvet/#assembling-paired-end-reads-using-velvet","title":"Assembling Paired-end Reads using Velvet","text":"<p>The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies.</p> <p>If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it!</p> <p>The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of ~350 bp.</p> <p>The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748.</p> <p>The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option.</p> <p>First move to the directory you made for this exercise and make a suitable named directory for the exercise:</p> <pre><code>cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRS004748 \ncd SRS004748\n</code></pre> <p>There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands:</p> <pre><code>ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./\n</code></pre> <p>It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type:</p> <pre><code>top\n</code></pre> <p><code>top</code> is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly.</p> <p>Now, back to the first terminal, you are ready to run <code>velveth</code> and <code>velvetg</code>. The reads are <code>-shortPaired</code> and for the first run you should not use any parameters for <code>velvetg</code>.</p> <p>From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command <code>time</code>. This will cause UNIX to report how long the program took to complete its task.</p> <p>Set the two stages of velvet running, whilst you watch the memory usage as reported by <code>top</code>. Time the <code>velvetg</code> stage:</p> <pre><code>velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz\ntime velvetg run_25\n</code></pre>  <p>Question</p> <p>What does <code>-fmtAuto</code> and <code>-create_binary</code> do? (see help menu)</p>    Answer <p><code>-fmtAuto</code> tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not.    <code>-create_binary</code> outputs sequences as a binary file. That means that <code>velvetg</code> can read the sequences from the binary file more quickly that from the original sequence files.</p>    <p>Question</p> <p>Comment on the use of memory and CPU for <code>velveth</code> and <code>velvetg</code>?</p>    Answer <p><code>velveth</code> uses only one CPU while <code>velvetg</code> uses all possible CPUs for some parts of the calculation.</p>    <p>Question</p> <p>How long did <code>velvetg</code> take?</p>    Answer <p>My own measurements are:    <code>real 1m8.877s; user 4m15.324s; sys 0m4.716s</code></p>   <p>Next, after saving your <code>contigs.fa</code> file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun <code>velvetg</code>. time and monitor the use of resources as previously. Start with <code>-cov_cutoff 16</code> thus:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.0\ntime velvetg run_25 -cov_cutoff 16\n</code></pre> <p>Up until now, <code>velvetg</code> has ignored the paired-end information. Now try running <code>velvetg</code> with both <code>-cov_cutoff 16</code> and <code>-exp_cov 26</code>, but first save your <code>contigs.fa</code> file. By using <code>-cov_cutoff</code> and <code>-exp_cov</code>, <code>velvetg</code> tries to estimate the insert length, which you will see in the <code>velvetg</code> output. The command is, of course:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.1\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26\n</code></pre>  <p>Question</p> <p>Comment on the time required, use of memory and CPU for <code>velvetg</code>?</p>    Answer <p>Runtime is lower when velvet can reuse previously calculated data. By using <code>-exp_cov</code>, the memory usage increases.</p>    <p>Question</p> <p>Which insert length does Velvet estimate?</p>    Answer <p>Paired-end library 1 has length: 228, sample standard deviation: 26</p>   <p>Next try running <code>velvetg</code> in \u2018paired-end mode\u2018. This entails running <code>velvetg</code> specifying the insert length with the parameter <code>-ins_length</code> set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of <code>contigs.fa</code>. The commands are:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.2\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350\nmv run_25/contigs.fa run_25/contigs.fa.3\n</code></pre>  <p>Question</p> <p>How fast was this run?</p>    Answer <p>My own measurements are:    <code>real 0m29.792s; user 1m4.372s; sys 0m3.880s</code></p>   <p>Take a look into the Log file.</p>  <p>Question</p> <p>What is the N50 value for the <code>velvetg</code> runs using the switches</p>    Answer <p>Base run: 19,510 bp   <code>-cov_cutoff 16</code>: 24,739 bp    <code>-cov_cutoff 16 -exp_cov 26</code>: 61,793 bp    <code>-cov_cutoff 16 -exp_cov 26 -ins_length 350</code>: n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp     </p>   <p>Try giving the <code>-cov_cutoff</code> and/or <code>-exp_cov</code> parameters the value <code>auto</code>. See the <code>velvetg</code> help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the <code>auto</code> option.</p>  <p>Question</p> <p>What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)?</p>    Answer <p>Median coverage depth = 26.021837   Removing contigs with coverage &lt; 13.010918 \u2026</p>    <p>Question</p> <p>How does the N50 value change?</p>    Answer <p>n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp</p>   <p>Run <code>gnx</code> on all the <code>contig.fa</code> files you have generated in the course of this exercise. The command will be:</p> <pre><code>gnx -min 100 -nx 25,50,75 run_25/contigs.fa*\n</code></pre>  <p>Question</p> <p>For which runs are there Ns in the <code>contigs.fa</code> file and why?</p>    Answer <p>contigs.fa.2, contigs.fa.3, contigs.fa</p>   <p>Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns.</p> <p>Comment on the number of contigs and total length generated for each run.</p>    Filename No. contigs Total length No. Ns     Contigs.fa.0 631 2,830,659 0   Contigs.fa.1 580 2,832,670 0   Contigs.fa.2 166 2,849,919 4,847   Contigs.fa.3 166 2,856,795 11,713   Contigs.fa 163 2,857,439 11,526"},{"location":"modules/btp-module-velvet/velvet/#amos-hawkeye","title":"AMOS Hawkeye","text":"<p>We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye.</p> <p>Run <code>velvetg</code> with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye:</p> <pre><code>time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes \ntime bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg         \nhawkeye run_25/velvet_asm.bnk\n</code></pre> <p>Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.</p> <p>Nearly all mates are compressed with no stretched mates and very few happy mates.</p>  <p>Question</p> <p>What is the mean and standard deviation of the insert size reported under the Libraries tab?</p>    Answer <p>Mean: 350 bp SD: 35 bp</p>    <p>Question</p> <p>Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places?</p>    Answer <p>We specified <code>-ins_length 350</code> to the <code>velvetg</code> command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate.</p>   <p>You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed.</p> <pre><code>asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2\nhawkeye run_25/velvet_asm.bnk\n</code></pre> <p>Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.</p> <p>There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates.</p>  <p>Question</p> <p>What is the mean and standard deviation of the insert size reported under the Libraries tab?</p>    Answer <p>Mean: 226 bp SD: 25 bp</p>    <p>Question</p> <p>Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match?</p>    Answer <p>Yes</p>    <p>Question</p> <p>Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate?</p>    Answer <p>This would indicate a possible misassembly and worthy of further investigation.    Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.</p>"},{"location":"modules/btp-module-velvet/velvet/#velvet-and-data-quality","title":"Velvet and Data Quality","text":"<p>So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies.</p> <p>Velvet does not use quality information present in FASTQ files.</p> <p>For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage.</p> <p>To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp.</p> <p>Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are:</p> <pre><code>cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRX008042 \ncd SRX008042\n</code></pre> <p>Create symlinks to the read data files that we downloaded for you from the SRA:</p> <pre><code>ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./\n</code></pre> <p>We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode.</p> <p>Start FastQC and set the process running in the background, by using a trailing <code>&amp;</code>, so we get control of our terminal back for entering more commands:</p> <pre><code>fastqc &amp;\n</code></pre> <p>Open the two compressed FASTQ files (File -&gt; Open) by selecting them both and clicking OK). Look at tabs for both files:</p>   <p>Question</p> <p>Are the quality scores the same for both files?</p>    Answer <p>Overall yes</p>    <p>Question</p> <p>Which value varies?</p>    Answer <p>Per sequence quality scores</p>    <p>Question</p> <p>Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file?</p>    Answer <p>The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced.</p>    <p>Question</p> <p>At which positions would you cut the reads if we did \u201cfixed length trimming\u201d?</p>    Answer <p>Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27</p>    <p>Question</p> <p>Why does the quality deteriorate towards the end of the read?</p>    Answer <p>Errors more likely for later cycles</p>    <p>Question</p> <p>Does it make sense to trim the 5\u2019 start of reads?</p>    Answer <p>Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning.</p>   <p>Have a look at the other options that FastQC offers.</p>  <p>Question</p> <p>Which other statistics could you use to support your trimming strategy?</p>    Answer <p>\u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d</p>    <p>Once you have decided what your trim points will be, close FastQC. We will use <code>fastx_trimmer</code> from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help:</p> <pre><code>fastx_trimmer -h\n</code></pre> <p><code>fastx_trimmer</code> is not able to read compressed FASTQ files, so we first need to decompress the files ready for input.</p> <p>The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows:</p> <pre><code>gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq\ngunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq\nfastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq \nfastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq\n</code></pre> <p>Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write.</p>  <p>Tips</p> <p>We could do what is called pipelining to send a stream of data from one command to another, using the pipe (<code>|</code>) character, without the need for intermediary files. The following command would achieve this:       </p> <p>gunzip \u2013to-stdout &lt; SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq    gunzip \u2013to-stdout &lt; SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq    </p>  <p>Now run <code>velveth</code> with a k-mer value of 21 for both the untrimmed and trimmed read files in <code>-shortPaired</code> mode. Separate the output of the two executions of <code>velveth</code> into suitably named directories, followed by <code>velvetg</code>:</p> <pre><code># untrimmed reads\nvelveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq\ntime velvetg run_21\n\n# trimmed reads\nvelveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq\ntime velvetg run_21trim\n</code></pre>  <p>Question</p> <p>How long did the two <code>velvetg</code> runs take?</p>    Answer <p>run_25: <code>real 3m16.132s; user 8m18.261s; sys 0m7.317s</code>  run_25trim: <code>real 1m18.611s; user 3m53.140s; sys 0m4.962s</code></p>    <p>Question</p> <p>What N50 scores did you achieve?</p>    Answer <p>Untrimmed: 11    Trimmed: 15</p>    <p>Question</p> <p>What were the overall effects of trimming?</p>    Answer <p>Time saving, increased N50, reduced coverage</p>   <p>The evidence is that trimming improved the assembly. The thing to do surely, is to run <code>velvetg</code> with the <code>-cov_cutoff</code> and <code>-exp_cov</code>. In order to use <code>-cov_cutoff</code> and <code>-exp_cov</code> sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms:</p> <pre><code>R --no-save\nlibrary(plotrix) \ndata &lt;- read.table(\"run_21/stats.txt\", header=TRUE) \ndata2 &lt;- read.table(\"run_21trim/stats.txt\", header=TRUE) \npar(mfrow=c(1,2))\nweighted.hist(data$short1_cov, data$lgth, breaks=0:50)\nweighted.hist(data2$short1_cov, data2$lgth, breaks=0:50)\n</code></pre> <p>Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed (right).</p>  <p>For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5.</p> <p>If you disagree, feel free to try different settings, but first quit R before running <code>velvetg</code>:</p> <pre><code>q()\n\ntime velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92\ntime velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92\n</code></pre>  <p>Question</p> <p>How good does it look now?</p>    Answer <p>Still not great    Runtime: Reduced runtime    Memory: Lower memory usage</p>    <p>Question</p> <p>K-mer choice (Can you use k-mer 31 for a read of length 30 bp?)</p>    Answer <p>K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results.</p>    <p>Question</p> <p>Does less data mean \u201cworse\u201d results?</p>    Answer <p>Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage.</p>   <p>Compare the results, produced during the last exercises, with each other, </p>    Metric SRR023408 SRR023408.trimmed     Overall Quality (1-5) 5 4   bp Coverage 95x (37bp; 7761796) 82x (32bp; 7761796)   k-mer Coverage 43x (21); 33x (25) 30x (21); 20.5x (25)   N50 (k-mer used) 2,803 (21) 2,914 (21)     <p>Question</p> <p>What would you consider as the \u201cbest\u201d assembly?</p>    Answer <p>SRR023408.trimmed</p>    <p>Question</p> <p>If you found a candidate, why do you consider it as \u201cbest\u201d assembly?</p>    Answer  <p>Overall data quality and coverage</p>   <p>Question</p> <p>How else might you assess the the quality of an assembly? </p>    Hint <p>Hawkeye</p>     Answer <p>By trying to identify paired-end constraint violations using AMOS Hawkeye.</p>"},{"location":"modules/btp-module-velvet/images/paired_end/","title":"Assembling Paired-end Reads","text":"<p>The use of paired-end data in de novo genome assembly results in better quality assemblies, particularly for larger, more complex genomes. In addition, paired-end constraint violation (expected distance and orientation of paired reads) can be used to identify misassemblies.</p> <p>If you are doing de novo assembly, pay the extra and get paired-ends: they\u2019re worth it!</p> <p>The data you will examine in this exercise is again from Staphylococcus aureus which has a genome of around 3MBases. The reads are Illumina paired end with an insert size of $~$350 bp.</p> <p>The required data can be downloaded from the SRA. Specifically, the run data (SRR022852) from the SRA Sample SRS004748.</p> <p>The following exercise focuses on preparing the paired-end FASTQ files ready for Velvet, using Velvet in paired-end mode and comparing results with Velvet\u2019s \u2019auto\u2019 option.</p> <p>First move to the directory you made for this exercise and make a suitable named directory for the exercise:</p> <pre><code>cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRS004748 \ncd SRS004748\n</code></pre> <p>There is no need to download the read files, as they are already stored locally. You will simply create a symlink to this pre-downloaded data using the following commands:</p> <pre><code>ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./\n</code></pre> <p>It is interesting to monitor the computer\u2019s resource utilisation, particularly memory. A simple way to do this is to open a second terminal and in it type:</p> <pre><code>top\n</code></pre> <p><code>top</code> is a program that continually monitors all the processes running on your computer, showing the resources used by each. Leave this running and refer to it periodically throughout your Velvet analyses. Particularly if they are taking a long time or whenever your curiosity gets the better of you. You should find that as this practical progresses, memory usage will increase significantly.</p> <p>Now, back to the first terminal, you are ready to run <code>velveth</code> and <code>velvetg</code>. The reads are <code>-shortPaired</code> and for the first run you should not use any parameters for <code>velvetg</code>.</p> <p>From this point on, where it will be informative to time your runs. This is very easy to do, just prefix the command to run the program with the command <code>time</code>. This will cause UNIX to report how long the program took to complete its task.</p> <p>Set the two stages of velvet running, whilst you watch the memory usage as reported by <code>top</code>. Time the <code>velvetg</code> stage:</p> <pre><code>velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz\ntime velvetg run_25\n</code></pre> <p>What does <code>-fmtAuto</code> and <code>-create_binary</code> do? (see help menu)</p> <p><code>-fmtAuto</code> tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not.</p> <p><code>-create_binary</code> outputs sequences as a binary file. That means that <code>velvetg</code> can read the sequences from the binary file more quickly that from the original sequence files.</p> <p>Comment on the use of memory and CPU for <code>velveth</code> and <code>velvetg</code>?</p> <p><code>velveth</code> uses only one CPU while <code>velvetg</code> uses all possible CPUs for some parts of the calculation.</p> <p>How long did <code>velvetg</code> take?</p> <p>My own measurements are:\\ <code>real 1m8.877s; user 4m15.324s; sys 0m4.716s</code></p> <p>Next, after saving your <code>contigs.fa</code> file from being overwritten, set the cut-off parameters that you investigated in the previous exercise and rerun <code>velvetg</code>. time and monitor the use of resources as previously. Start with <code>-cov_cutoff 16</code> thus:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.0\ntime velvetg run_25 -cov_cutoff 16\n</code></pre> <p>Up until now, <code>velvetg</code> has ignored the paired-end information. Now try running <code>velvetg</code> with both <code>-cov_cutoff 16</code> and <code>-exp_cov 26</code>, but first save your <code>contigs.fa</code> file. By using <code>-cov_cutoff</code> and <code>-exp_cov</code>, <code>velvetg</code> tries to estimate the insert length, which you will see in the <code>velvetg</code> output. The command is, of course:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.1\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26\n</code></pre> <p>Comment on the time required, use of memory and CPU for <code>velvetg</code>?</p> <p>Runtime is lower when velvet can reuse previously calculated data. By using <code>-exp_cov</code>, the memory usage increases.</p> <p>Which insert length does Velvet estimate?</p> <p>Paired-end library 1 has length: 228, sample standard deviation: 26</p> <p>Next try running <code>velvetg</code> in \u2018paired-end mode\u2018. This entails running <code>velvetg</code> specifying the insert length with the parameter <code>-ins_length</code> set to 350. Even though velvet estimates the insert length it is always advisable to check / provide the insert length manually as velvet can get the statistics wrong due to noise. Just in case, save your last version of <code>contigs.fa</code>. The commands are:</p> <pre><code>mv run_25/contigs.fa run_25/contigs.fa.2\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350\nmv run_25/contigs.fa run_25/contigs.fa.3\n</code></pre> <p>How fast was this run?</p> <p>My own measurements are:\\ <code>real 0m29.792s; user 1m4.372s; sys 0m3.880s</code></p> <p>Take a look into the Log file.</p> <p>What is the N50 value for the <code>velvetg</code> runs using the switches:\\</p> <p>Base run: 19,510 bp</p> <p><code>-cov_cutoff 16</code></p> <p>24,739 bp</p> <p><code>-cov_cutoff 16 -exp_cov 26</code></p> <p>61,793 bp</p> <p><code>-cov_cutoff 16 -exp_cov 26 -ins_length 350</code></p> <p>n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp</p> <p>Try giving the <code>-cov_cutoff</code> and/or <code>-exp_cov</code> parameters the value <code>auto</code>. See the <code>velvetg</code> help to show you how. The information Velvet prints during running includes information about the values used (coverage cut-off or insert length) when using the <code>auto</code> option.</p> <p>What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)?</p> <p>Median coverage depth = 26.021837\\ Removing contigs with coverage $&lt;$ 13.010918 \u2026</p> <p>How does the N50 value change?</p> <p>n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp</p> <p>Run <code>gnx</code> on all the <code>contig.fa</code> files you have generated in the course of this exercise. The command will be:</p> <pre><code>gnx -min 100 -nx 25,50,75 run_25/contigs.fa*\n</code></pre> <p>For which runs are there Ns in the <code>contigs.fa</code> file and why?</p> <p>contigs.fa.2, contigs.fa.3, contigs.fa\\ Velvet tries to use the provided (or infers) the insert length and fills ambiguous regions with Ns.</p> <p>Comment on the number of contigs and total length generated for each run.</p> <p>Filename       No. contigs   Total length   No. Ns</p>  <p>Contigs.fa.0   631           2,830,659      0   Contigs.fa.1   580           2,832,670      0   Contigs.fa.2   166           2,849,919      4,847   Contigs.fa.3   166           2,856,795      11,713   Contigs.fa     163           2,857,439      11,526</p> <p>:  </p>"},{"location":"modules/btp-module-velvet/images/paired_end/#amos-hawkeye","title":"AMOS Hawkeye","text":"<p>We will now output the assembly in the AMOS massage format and visualise the assembly using AMOS Hawkeye.</p> <p>Run <code>velvetg</code> with appropriate arguments and output the AMOS message file, then convert it to an AMOS bank and open it in Hawkeye:</p> <pre><code>time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes \ntime bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg         \nhawkeye run_25/velvet_asm.bnk\n</code></pre> <p>Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.</p> <p>Nearly all mates are compressed with no stretched mates and very few happy mates.</p> <p>What is the mean and standard deviation of the insert size reported under the Libraries tab?</p> <p>Mean: 350 bp SD: 35 bp</p> <p>Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places?</p> <p>We specified <code>-ins_length 350</code> to the <code>velvetg</code> command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate.</p> <p>You can get AMOS to re-estimate the mean and SD of insert sizes using intra-contig pairs. First, close Hawkeye and then run the following commands before reopening the AMOS bank to see what has changed.</p> <pre><code>asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2\nhawkeye run_25/velvet_asm.bnk\n</code></pre> <p>Looking at the scaffold view of a contig, comment on the proportion of \u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.</p> <p>There are only a few compressed and stretched mates compared to happy mates. There are similar numbers of stretched and compressed mates.</p> <p>What is the mean and standard deviation of the insert size reported under the Libraries tab?</p> <p>TODO Mean: 226 bp SD: 25 bp</p> <p>Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match?</p> <p>Yes</p> <p>Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate?</p> <p>This would indicate a possible misassembly and worthy of further investigation.</p> <p>Look at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.</p>"},{"location":"modules/btp-module-velvet/images/paired_end/#velvet-and-data-quality","title":"Velvet and Data Quality","text":"<p>So far we have used the raw read data without performing any quality control or read trimming prior to doing our velvet assemblies.</p> <p>Velvet does not use quality information present in FASTQ files.</p> <p>For this reason, it is vitally important to perform read QC and quality trimming. In doing so, we remove errors/noise from the dataset which in turn means velvet will run faster, will use less memory and will produce a better assembly. Assuming we haven\u2019t compromised too much on coverage.</p> <p>To investigate the effect of data quality, we will use the run data (SRR023408) from the SRA experiment SRX008042. The reads are Illumina paired end with an insert size of 92 bp.</p> <p>Go back to the main directory for this exercise and create and enter a new directory dedicated to this phase of the exercise. The commands are:</p> <pre><code>cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRX008042 \ncd SRX008042\n</code></pre> <p>Create symlinks to the read data files that we downloaded for you from the SRA:</p> <pre><code>ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./\n</code></pre> <p>We will use FastQC, a tool you should be familiar with, to visualise the quality of our data. We will use FastQC in the Graphical User Interface (GUI) mode.</p> <p>Start FastQC and set the process running in the background, by using a trailing <code>&amp;</code>, so we get control of our terminal back for entering more commands:</p> <pre><code>fastqc &amp;\n</code></pre> <p>Open the two compressed FASTQ files (File $-&gt;$ Open) by selecting them both and clicking OK). Look at tabs for both files:</p> <p>{width=\u201d80.00000%\u201d}</p> <p>Are the quality scores the same for both files?</p> <p>Overall yes</p> <p>Which value varies?</p> <p>Per sequence quality scores</p> <p>Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file?</p> <p>The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced.</p> <p>At which positions would you cut the reads if we did \u201cfixed length trimming\u201d?</p> <p>Looking at the \u201cPer base quality\u201d and \u201cPer base sequence content\u201d, I would choose around 27</p> <p>Why does the quality deteriorate towards the end of the read?</p> <p>Errors more likely for later cycles</p> <p>Does it make sense to trim the 5\u2019 start of reads?</p> <p>Looking at the \u201cPer base sequence content\u201d, yes - there is a clear signal at the beginning.</p> <p>Have a look at the other options that FastQC offers.</p> <p>Which other statistics could you use to support your trimming strategy?</p> <p>\u201cPer base sequence content\u201d, \u201cPer base GC content\u201d, \u201cKmer content\u201d, \u201cPer base sequence quality\u201d</p> <p>{width=\u201d80.00000%\u201d}</p> <p>Once you have decided what your trim points will be, close FastQC. We will use <code>fastx_trimmer</code> from the FASTX-Toolkit to perform fixed-length trimming. For usage information see the help:</p> <pre><code>fastx_trimmer -h\n</code></pre> <p><code>fastx_trimmer</code> is not able to read compressed FASTQ files, so we first need to decompress the files ready for input.</p> <p>The suggestion (hopefully not far from your own thoughts?) is that you trim your reads as follows:</p> <pre><code>gunzip &lt; SRR023408_1.fastq.gz &gt; SRR023408_1.fastq\ngunzip &lt; SRR023408_2.fastq.gz &gt; SRR023408_2.fastq\nfastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq \nfastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq\n</code></pre> <p>Many NGS read files are large. This means that simply reading and writing files can become the bottleneck, also known as I/O bound. Therefore, it is often good practice to avoid unnecessary disk read/write.</p> <p>We could do what is called pipelining to send a stream of data from one command to another, using the pipe (<code>|</code>) character, without the need for intermediary files. The following command would achieve this:</p> <pre><code>gunzip --to-stdout &lt; SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq \ngunzip --to-stdout &lt; SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq\n</code></pre> <p>Now run <code>velveth</code> with a k-mer value of 21 for both the untrimmed and trimmed read files in <code>-shortPaired</code> mode. Separate the output of the two executions of <code>velveth</code> into suitably named directories, followed by <code>velvetg</code>:</p> <pre><code># untrimmed reads\nvelveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq\ntime velvetg run_21\n\n# trimmed reads\nvelveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq\ntime velvetg run_21trim\n</code></pre> <p>How long did the two <code>velvetg</code> runs take?</p> <p>run_25: <code>real 3m16.132s; user 8m18.261s; sys 0m7.317s</code>\\ run_25trim: <code>real 1m18.611s; user 3m53.140s; sys 0m4.962s</code></p> <p>What N50 scores did you achieve?</p> <p>Untrimmed: 11\\ Trimmed: 15</p> <p>What were the overall effects of trimming?</p> <p>Time saving, increased N50, reduced coverage</p> <p>The evidence is that trimming improved the assembly. The thing to do surely, is to run <code>velvetg</code> with the <code>-cov_cutoff</code> and <code>-exp_cov</code>. In order to use <code>-cov_cutoff</code> and <code>-exp_cov</code> sensibly, you need to investigate with R, as you did in the previous exercise, what parameter values to use. Start up R and produce the weighted histograms:</p> <pre><code>R --no-save\nlibrary(plotrix) \ndata &lt;- read.table(\"run_21/stats.txt\", header=TRUE) \ndata2 &lt;- read.table(\"run_21trim/stats.txt\", header=TRUE) \npar(mfrow=c(1,2))\nweighted.hist(data$short1_cov, data$lgth, breaks=0:50)\nweighted.hist(data2$short1_cov, data2$lgth, breaks=0:50)\n</code></pre> <p>{width=\u201d80.00000%\u201d}</p> <p>For the untrimmed read histogram (left) there is an expected coverage of around 13 with a coverage cut-off of around 7. For the trimmed read histogram (right) there is an expected coverage of around 9 with a coverage cut-off of around 5.</p> <p>If you disagree, feel free to try different settings, but first quit R before running <code>velvetg</code>:</p> <pre><code>q()\n</code></pre> <pre><code>time velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92\ntime velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92\n</code></pre> <p>How good does it look now?\\</p> <p>Still not great</p> <p>Comment on:\\ Runtime</p> <p>Reduced runtime</p> <p>Memory</p> <p>Lower memory usage</p> <p>k-mer choice (Can you use k-mer 31 for a read of length 30 bp?)</p> <p>K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results.</p> <p>Does less data mean \u201cworse\u201d results?</p> <p>Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage.</p> <p>How would a smaller/larger k-mer size behave?</p> <p>Compare the results, produced during the last exercises, with each other:</p> <p>[0.9]{}[l|l|l|l]{} Metric &amp; SRR022852 &amp; SRR023408 &amp; SRR023408.trimmed\\ Overall Quality (1-5) &amp; &amp; &amp;\\ bp Coverage &amp; &amp; &amp;\\ k-mer Coverage &amp; &amp; &amp;\\ N50 (k-mer used) &amp; &amp; &amp;\\</p> <p>[0.9]{}[l|l|l|l]{} Metric &amp; SRR022852 &amp; SRR023408 &amp; SRR023408.trimmed\\ Overall Quality (1-5) &amp; 2 &amp; 5 &amp; 4\\ bp Coverage &amp; 136 x (36 bp;11,374,488) &amp; 95x (37bp; 7761796) &amp; 82x (32bp; 7761796)\\ k-mer Coverage &amp; 45x &amp; 43x (21); 33x (25) &amp; 30x (21); 20.5x (25)\\ N50 (k-mer used) &amp; 68,843 (25) &amp; 2,803 (21) &amp; 2,914 (21)\\</p> <p>What would you consider as the \u201cbest\u201d assembly?</p> <p>SRR022852</p> <p>If you found a candidate, why do you consider it as \u201cbest\u201d assembly?</p> <p>Overall data quality and coverage</p> <p>How else might you assess the the quality of an assembly? Hint: Hawkeye.</p> <p>By trying to identify paired-end constraint violations using AMOS Hawkeye.</p>"},{"location":"modules/cancer-module-alignment/alignment/","title":"Read Alignment","text":""},{"location":"modules/cancer-module-alignment/alignment/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Perform the simple NGS data alignment task against reference data.</p> </li> <li> <p>Learn about the SAM/BAM formats for further manipulation.</p> </li> <li> <p>Be able to sort and index BAM format for visualisation purposes.</p> </li> </ul>"},{"location":"modules/cancer-module-alignment/alignment/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-alignment/alignment/#tools-used","title":"Tools Used","text":"<p>BWA Burrows-Wheeler Algorithm: http://bio-bwa.sourceforge.net</p> <p>Samtools: http://picard.sourceforge.net/</p>"},{"location":"modules/cancer-module-alignment/alignment/#useful-links","title":"Useful Links","text":"<p>SAM Specification: http://samtools.sourceforge.net/SAM1.pdf</p> <p>Explain SAM Flags: https://broadinstitute.github.io/picard/explain-flags.html</p>"},{"location":"modules/cancer-module-alignment/alignment/#sources-of-data","title":"Sources of Data","text":"<p>http://sra.dnanexus.com/studies/ERP001071</p>"},{"location":"modules/cancer-module-alignment/alignment/#author-information","title":"Author Information","text":"<p>Primary Author(s): Sonika Tyagi sonika.tyagi@agrf.org.au Gayle Philip gkphilip@unimelb.edu.au</p> <p>Contributor(s):</p>"},{"location":"modules/cancer-module-alignment/alignment/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to perform an NGS alignment on the sequencing data coming from a tumour and normal group of samples. We will align raw sequencing data to the human genome using the BWA aligner and then we will discuss the sequence alignment and mapping format (SAM). SAM to BAM conversion, indexing and sorting will also be demonstrated. These are important and essential steps for downstream processing of the aligned BAM files.</p> <p>This data is the whole genome sequencing of a lung adenocarcinoma patient AK55. It was downloaded from <code>ERP001071</code>. Only the HiSeq2000 data for <code>Blood</code> and <code>liverMets</code> were analysed.</p> <p>Accession numbers associated with read data are assigned by the European Bioinformatics Institute (EBI) and start with \u2019ER\u2019. e.g. ERP is the study ID and ERR is the run ID. The original FASTQ files downloaded had the ERR number in front of each read name in the FASTQ file (e.g. @ERRxx HWI-ST478_xxxx). The read name had to be edited to remove the ERR number at the start of the name. This had caused problems for downstream programs such as Picard for marking optical duplicates.</p> <p>We have used 4 Blood samples (8 paired-end (PE) <code>*.fastq.gz</code> files) and 5 Liver samples (10 PE <code>*.fastq.gz</code> files) data from this study to perform the whole genome alignment using the BWA aligner. The whole process took &gt;150K CPU seconds per sample and the precomputed alignment will be used in different sections of this workshop.</p>"},{"location":"modules/cancer-module-alignment/alignment/#prepare-the-environment","title":"Prepare the Environment","text":"<p>By now you know about the raw sequence FASTQ format generated by the Illumina sequencers. Next we will see how FASTQ files are aligned to the reference genome and what the resulting standard alignment format is. In the interest of time, we have selected only 1 million paired reads from a <code>Blood</code> sample to demonstrate a <code>BWA</code> command. The remaining alignments have already been performed for you and will be required in the subsequent modules of the workshop.</p> <p>The input data for this section can be found in the <code>alignment</code> directory on your desktop. Please follow the commands below to go to the right folder and view the top 10 lines of the input FASTQ file:</p> <p>Open the Terminal.</p> <p>First, go to the right folder, where the data are stored.</p> <pre><code>cd /home/trainee/alignment\nls\nzless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz\n</code></pre> <p>Press <code>q</code> to stop the <code>zless</code> command.</p>"},{"location":"modules/cancer-module-alignment/alignment/#overview-of-the-process","title":"Overview of the Process","text":"<p>Figure 1: A flow diagram showing the steps that will be performed in this practical. </p>"},{"location":"modules/cancer-module-alignment/alignment/#alignment","title":"Alignment","text":"<p>You already know that there are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will use <code>BWA</code>, a widely used aligner based on the Burrows-Wheeler Algorithm. The alignment involves two steps:  </p> <p>1) Indexing the genome. 2) Running the alignment command.  </p> <p>BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the other two are for longer sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads. For more details see the BWA manual.</p> <p>BWA has a number of parameters in order to perform the alignment. To view them all, type</p> <pre><code>bwa &lt;press enter&gt;\n</code></pre> <p> BWA uses an indexed genome for the alignment in order to keep its memory footprint small. Indexing a genome is similar in concept to indexing a book. If you want to know on which page a certain word appears or a chapter begins, it is much more efficient/faster to look it up in a pre-built index than going through every page of the book until you find it. Indices allow the aligner to narrow down the potential origin of a query sequence within the genome, saving both time and memory.  </p> <p>Due to time constraints, we will NOT be running the indexing command. It is run only once for a version of a genome, and the complete command to index the human genome version hg19 is given below.</p>  <p>STOP</p> <p>You DO NOT need to run this command. This has already been run for you.</p> <p><code>bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta</code></p>  <p>We have used the following arguments for the indexing of the genome.</p>  <p>-p:  Prefix of the output database [same as db filename].   -a:  Algorithm for constructing BWT index. This method works with the     whole human genome. Ref genome filename: the last argument is the name of the reference genome file in the fasta format.</p>  <p>This command will output 6 files that constitute the index. These files have the prefix <code>human_g1k_v37.fasta</code> and are stored in the <code>bwaIndex</code> subdirectory. To view the precomputed index files, type:</p> <pre><code>ls -l bwaIndex\n</code></pre> <p> Now that the genome is indexed we can move on to the actual alignment.  </p> <p>Make a directory to store the output from your aligner.</p> <pre><code>mkdir outputs\n</code></pre> <p>The first argument for <code>bwa</code> is the basename of the index for the genome to be searched. In our case this is <code>human_g1k_v37.fasta</code>.</p> <p>Align the reads from the <code>Blood</code> sample using the following command:</p> <pre><code>bwa mem -M -t 4 -R '@RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA' bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz &gt; outputs/SM_Blood_ID_ERR059356.subset.sam\n</code></pre> <p>The above command outputs the alignment in SAM format and stores them in the file <code>SM_Blood_ID_ERR059356.subset.sam</code> in the subdirectory <code>outputs</code>.</p> <p>We have used the following arguments for the alignment of the reads.</p>  <p>mem: fast mode of high quality input such the Illumina -M: flags extra hits as secondary. This is needed for compatibility with   other tools downstream. -t: Number of threads. -R: Complete read group header line.</p>  <p> The SAM (Sequence Alignment/Map) format is currently the de facto standard for storing large nucleotide sequence alignments. It is a TAB-delimited text format consisting of a header section, which is optional, and an alignment section. If present, the header must be prior to the alignments. Header lines start with <code>@</code>, while alignment lines do not. Each alignment line has 11 mandatory fields with essential alignment information such as mapping position.</p> <p>Navigate into your <code>outputs</code> directory and look at the top 10 lines of the SAM file by typing:</p> <pre><code>cd outputs\nhead -n 10 SM_Blood_ID_ERR059356.subset.sam\n</code></pre>   <p>Question</p> <p>Can you distinguish between the header of the SAM format and the actual alignments?</p>   Answer <p>The header line starts with the letter \u2018@\u2019 i.e. <pre><code>@SQ SN:GL000192.1   LN:547496\n@RG SM:Blood    ID:ERR059356    LB:lb   PL:ILLUMINA\n@PG ID:bwa  PN:bwa  VN:0.7.15-r1140 CL:bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059356.subset\\tLB:lb\\tPL:ILLUMINA bwaIndex/human_g1k_v37.fasta input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz input/SM_Blood_ID_ERR059356.subset_R2.fastq.gz\n</code></pre></p> <p>The actual alignments start with read ID i.e. <pre><code>HWI-ST478_0133:3:1101:1374:2056#0   147 11\nHWI-ST478_0133:3:1101:1352:2070#0   163 14\n</code></pre></p>     <p>Question</p> <p>What kind of information does the header provide?</p>   Answer <ul> <li> <p>@HD: Header line; VN: Format version; SO: the sort order of     alignments.</p> </li> <li> <p>@SQ: Reference sequence information; SN: reference sequence name;     LN: reference sequence length.</p> </li> <li> <p>@PG: Program; ID: Program record identifier; VN: Program     version; CL: the command line that produces the alignment.</p> </li> </ul>     <p>Question</p> <p>To which chromosome are the reads mapped?</p>   Answer <p>All chromosomes are represented (look at the 3rd field). grep -v \u201c^@\u201d SM_Blood_ID_ERR059356.subset.sam  | cut -f3 | sort | uniq</p>"},{"location":"modules/cancer-module-alignment/alignment/#manipulating-sam-output","title":"Manipulating SAM output","text":"<p>SAM files are rather big and when dealing with a high volume of NGS data, storage space can become an issue. As we have already seen, we can convert SAM to BAM files (their binary equivalent that are not human readable) that occupy much less space.</p> <p>Convert SAM to BAM using <code>samtools view</code> and store the output in the file <code>SM_Blood_ID_ERR059356.subset.bam</code>. You have to instruct <code>samtools view</code> that the input is in SAM format (<code>-S</code>), the output should be in BAM format (<code>-b</code>) and that you want the output to be stored in the file specified by the <code>-o</code> option:</p> <pre><code>samtools view -bSo SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sam\n</code></pre> <p>BAM files are not human-readable but can be viewed with the <code>samtools view</code> command.</p>   <p>Advanced exercise<p>Compute summary stats for the Flag values associated with the alignments  using:  <pre><code>samtools flagstat SM_Blood_ID_ERR059356.subset.bam\n</code></pre></p> </p>"},{"location":"modules/cancer-module-alignment/alignment/#post-alignment-visualisation-option","title":"Post Alignment Visualisation option","text":"<p>IGV is a stand-alone genome browser that can be used to visualise the BAM outputs. Please check their website for all the formats that IGV can display.</p> <p>We will be using IGV later in the workshop for viewing a BAM file in the genome browser. It requires the index of the BAM file to be in the same folder as where the BAM file is. The index file should have the same name as the BAM file and the suffix <code>.bai</code>. Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates.</p> <p>Sort alignments according to chromosomal position and store the result in the file with the prefix <code>SM_Blood_ID_ERR059356.subset.sorted</code>:</p> <pre><code>samtools sort SM_Blood_ID_ERR059356.subset.bam SM_Blood_ID_ERR059356.subset.sorted\n</code></pre> <p> Index the sorted file.</p> <pre><code>samtools index SM_Blood_ID_ERR059356.subset.sorted.bam\n</code></pre> <p>The indexing will create a file called <code>SM_Blood_ID_ERR059356.subset.sorted.bam.bai</code>. Note that you don\u2019t have to specify the name of the index file when running <code>samtools index</code>, it simply appends a <code>.bai</code> suffix to the input BAM file. </p>  <p>Question</p> <p>How can you quickly find out whether a BAM file is already coordinate sorted or not?</p>   Answer <p>Use <code>samtools view -h</code> command to look at the SAM header. It will have an SO field (e.g. SO:coordinate)</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/","title":"Copy Number Variation","text":""},{"location":"modules/cancer-module-cnv/cnv-tut/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Understand and perform a simple copy number variation analysis on     NGS data</p> </li> <li> <p>Become familiar with Sequenza</p> </li> <li> <p>Understand the CNV inference process as an interplay between depth     of sequencing, cellularity and B-allele frequency</p> </li> <li> <p>Visualize CNV events by manual inspection</p> </li> </ul>"},{"location":"modules/cancer-module-cnv/cnv-tut/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-cnv/cnv-tut/#tools-used","title":"Tools Used","text":"<p>Sequenza: http://www.cbs.dtu.dk/biotools/sequenza/</p> <p>IGV: http://www.broadinstitute.org/igv/</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#sources-of-data","title":"Sources of Data","text":"<p>Raw data download: http://sra.dnanexus.com/studies/ERP001071</p> <p>Data publication: http://www.ncbi.nlm.nih.gov/pubmed/22194472</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#author-information","title":"Author Information","text":"<p>Primary Author(s): Velimir Gayevskiy, Garvan Institute v.gayevskiy@garvan.org.au Sonika Tyagi, AGRF sonika.tyagi@agrf.org.au </p> <p>Contributor(s): </p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to perform a copy number variation analysis (CNV) on a normal/tumour pair of alignment files (BAMs) produced by the mapping of Illumina short read sequencing data.</p> <p>To ensure reasonable analysis times, we will perform the analysis on a heavily subsetted pair of BAM files. These files contain just the first 60Mb of chromosome 5 but contain several examples of inferred copy number events to enable interpretation and visualisation of the copy number variation that is present in entire cancer genomes. <code>Sequenza</code> is the tool we will use to perform this analysis. It consists of two <code>Python</code> pre-processing steps followed by a third step in <code>R</code> to infer the depth ratio, cellularity, ploidy and to plot the results for interpretation.</p> <p>In the second part of the tutorial we will also be using <code>IGV</code> to visualise and manually inspect the copy number variation we inferred in the first part for validation purposes. This section will also include a discussion on the importance of good quality data by highlighting the inadequacies of the workshop dataset and the implications this has on analysis results.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#prepare-the-environment","title":"Prepare the Environment","text":"<p>We will use a dataset derived from whole genome sequencing of a 33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no familial cancer history.</p> <p>The data files are contained in the subdirectory called <code>data</code> and are the following:</p>  <p><code>normal.chr5.60Mb.bam</code> and <code>normal.chr5.60Mb.bam.bai</code></p> <p><code>tumour.chr5.60Mb.bam</code> and <code>tumour.chr5.60Mb.bam.bai</code></p>  <p> These files are based on subsetting the whole genomes derived from <code>blood</code> and <code>liver metastases</code> to the first 60Mb of chromosome 5. This will allow our analyses to run in a sufficient time during the workshop, but it\u2019s worth being aware that we are analysing just 1.9% of the genome which will highlight the length of time and resources required to perform cancer genomics on full genomes!</p> <p>Open the Terminal and go to the <code>cnv</code> working directory:</p> <pre><code>cd /home/trainee/cnv/\n</code></pre>  <p>All commands entered into the terminal for this tutorial should be from within the <code>cnv</code> directory.</p>  <p>Check that the <code>data</code> directory contains the above-mentioned files by typing:</p> <pre><code>ls -l data\n</code></pre>"},{"location":"modules/cancer-module-cnv/cnv-tut/#sequenza-cnv-analysis","title":"Sequenza CNV Analysis","text":"<p>Sequenza is run in three steps. The first pre-processing step is run on the final normal and tumour mapped data (BAM files) in order to walk the genome in a pileup format (automatically generated by <code>samtools</code>). This first step finds high quality sites in the genomes and extracts their depth and genotype in the normal genome and calculates the variant alleles and allele frequencies in the tumour genome. The second step is to perform a binning on these sites to save space and analysis time in the third step. Finally, the third step is run in <code>R</code> to normalise the depth ratio between the normal/tumour genomes, infer cellularity and ploidy and graphically output results for interpretation.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#step-1-pre-processing-walking-the-genome","title":"Step 1: Pre-processing \u2013 Walking the Genome","text":"<pre><code>pypy software/sequenza/sequenza-utils.py bam2seqz \\\n-n data/normal.chr5.60Mb.bam \\\n-t data/tumour.chr5.60Mb.bam \\\n--fasta assets/human_g1k_v37.fasta \\\n-gc assets/human_g1k_v37.gc50Base.txt.gz \\\n-C 5:1-60000000 | gzip &gt; stage1.seqz.gz\n</code></pre>  <p>Hint</p> <p>Press tab after typing a few characters of a directory of filename to auto-complete the rest. This makes entering long file names very quick.</p>  <p>Explanation of parameters:</p>  <p>-n: the normal BAM -t: the tumour BAM --fasta: the reference genome used for mapping (b37 here) -gc: GC content as windows through the genome (pre-generated and downloadable from the Sequenza website) -C: specifies the genomic location to process  </p>    <p>There will not be any indication that it is running once you launch the command, to make sure it is running open a new Terminal tab with <code>Shift + Control + T</code> (or from the menu with File then Open Tab) and type the command <code>top</code>. You should see the top line being the command \u2019pypy\u2019 with a % CPU usage of 98/99%. Press <code>q</code> to quit out of this process view and go back to the tab running <code>Sequenza</code>. If everything is running correctly, it will take approximately 40 minutes to run. Go have a coffee!</p>  <p> Once the command is done you will be returned to the terminal prompt. Make sure the output file is the correct size by typing <code>ls -lh</code> from the Terminal window that you ran <code>Sequenza</code> from, there should be a file called <code>stage1.seqz.gz</code> of the size ~395M.</p> <p>You can look at the first few lines of the output in the file <code>stage1.seqz.gz</code> with:</p> <pre><code>zcat stage1.seqz.gz | head -n 20\n</code></pre> <p>This output has one line for each position in the BAMs and includes information on the position, depths, allele frequencies, zygosity, GC in the location.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#step-2-perform-binning","title":"Step 2: Perform Binning","text":"<p>The binning step takes the rows of genomic positions and compresses them down to 1 row for every 200 rows previously. This massively reduces the file size and processing time in the third step.</p> <pre><code>pypy software/sequenza/sequenza-utils.py seqz-binning \\\n-w 200 \\\n-s stage1.seqz.gz | gzip &gt; stage2.seqz.gz\n</code></pre> <p>Explanation of parameters:</p>  <p>-w: the window size (typically 50 for exomes, 200 for genomes) -s: the large seqz file generated in the first step</p>  <p>This step should take approximately 4 minutes to complete.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#step-3-running-sequenza-algorithms-and-plotting-results","title":"Step 3: Running Sequenza Algorithms and Plotting Results","text":"<p>We will now perform the CNV analysis and output the results using the <code>R</code> part of Sequenza.</p> <p>Open the <code>R</code> terminal:</p> <pre><code>R\n</code></pre> <p>You should now see the <code>R</code> prompt identified with \u201c&gt;\u201d.</p> <p>Run the Sequenza <code>R</code> commands:</p> <pre><code>library(\"sequenza\")\nsetwd(\"/home/trainee/cnv\")\ndata.file &lt;- \"stage2.seqz.gz\"\nseqzdata &lt;- sequenza.extract(data.file)\nCP.example &lt;- sequenza.fit(seqzdata)\nsequenza.results(sequenza.extract = seqzdata, cp.table = CP.example, sample.id = \"CanGenWorkshop\", out.dir=\"sequenza_results\")\n</code></pre> <p>Quit <code>R</code>:  </p> <pre><code>q()\n</code></pre> <p>Then enter <code>n</code> at the \u201cSave workspace image\u201d prompt.</p> <p>If every command ran successfully, you will now have a <code>sequenza_results</code> folder containing 13 files (type <code>ls -l sequenza_results/</code>).</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#sequenza-analysis-results-and-visualisation","title":"Sequenza Analysis Results and Visualisation","text":"<p>One of the first and most important estimates that Sequenza provides is the tumour cellularity (the estimated percentage of tumour cells in the tumour genome). This estimate is based on the B allele frequency and depth ratio through the genome and is an important metric to know for interpretation of Sequenza results and for other analyses. Lets look at the cellularity estimate for our analysis by opening <code>CanGenWorkshop_model_fit.pdf</code> with the command:</p> <pre><code>evince sequenza_results/CanGenWorkshop_model_fit.pdf\n</code></pre> <p> The cellularity estimate is at the top along with the average ploidy estimate and the standard deviation of the B allele frequency. We can see that the cellularity has been estimated at 24% which is fairly low and we will see why this is bad in the next section on CNV visualisation. The ploidy value of 2.1 indicates this piece of the genome is not hugely amplified or deleted and the BAF standard deviation indicates there are no significant long losses of heterozygosity.</p> <p>Close the PDF window to resume the Terminal prompt.</p> <p>Let\u2019s now look at the CNV inferences through our genomic block. Open the genome copy number visualisation file with:</p> <pre><code>evince sequenza_results/CanGenWorkshop_genome_view.pdf\n</code></pre> <p> This file contains three \u201cpages\u201d of copy number events through the entire genomic block.</p> <ol> <li>The first page shows copy numbers of the A (red) and B (blue) alleles,</li> <li>The second page shows overall copy number changes, and</li> <li>The third page shows the B allele frequency and depth ratio through genomic block.  </li> </ol> <p>Looking at the overall copy number changes, we see that our block is at a copy number of 2 with a small duplication to copy number 4 about \u2153 of the way through the block and another just after halfway through the block. There is also a reduction in copy number to 1 copy about \u2158 of the way through the block. The gap that you see just before this reduction in copy number is the chromosomal centromere - an area that is notoriously difficult to sequence so always ends up in a gap with short read data.</p> <p>You can see how this is a very easy to read output and lets you immediately see the frequency and severity of copy number events through your genome. Let\u2019s compare the small genomic block we ran with the same output from the entire genome which has been pre-computed for you. This is located in the <code>pre_generated/results_whole_genome</code> folder and contains the same 13 output files as for the small genomic block. As before, let\u2019s look at the cellularity estimate with:</p> <pre><code>evince pre_generated/results_whole_genome/CanGenWorkshop_model_fit.pdf\n</code></pre> <p> It now looks like it\u2019s even worse at just 16%! A change is to be expected as we were only analysing 1.9% of the genome. Let\u2019s now look at the whole genome copy number profile with:</p> <pre><code>evince pre_generated/results_whole_genome/CanGenWorkshop_genome_view.pdf\n</code></pre> <p> You can see that there are a number of copy number events across the genome and our genomic block (the first 60Mb of chromosome 5) is inferred as mostly copy number 4 followed by a reduction to copy number 2, rather than 2 to 1 as we saw in the output we generated. The reason for this is that <code>Sequenza</code> uses the genome-wide depth ratio and BAF in order to estimate copy number, if you ask it to analyse a small block mostly at copy number 4 with a small reduction to copy number 2, the most likely scenario in lieu of more data is that this is a copy number 2 block with a reduction to 1. It\u2019s important to carefully examine the cellularity, ploidy and BAF estimates of your sample along with the plots of model fit (<code>CanGenWorkshop_model_fit.pdf</code>) and cellularity/ploidy contours (<code>CanGenWorkshop_CP_contours.pdf</code>) in order to decide if you believe Sequenza\u2019s inference of the copy numbers. Have a look at these for yourself if you want to get a better idea of how <code>Sequenza</code> makes its inferences and conclusions.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#cnv-visualisationconfirmation-in-igv","title":"CNV Visualisation/Confirmation in IGV","text":"<p>Let\u2019s see if we can visualise one of the CNV events where copy number increased significantly. We\u2019ll focus on the copy number 4 event seen at about \u2153 of the way through the <code>CanGenWorkshop_genome_view.pdf</code> output we generated. First, we need to find the coordinates that have been predicted for this event. Have a look at the <code>CanGenWorkshop_segments.txt</code> file in the results folder to view all predicted CNV events with:</p> <pre><code>less sequenza_results/CanGenWorkshop_segments.txt\n</code></pre> <p> There is only one at a copy number of 4 (CNt column) and it starts at 21,051,700 to 21,522,065 which is 470kb and corresponds to the small block we see in the genome view PDF.</p> <p>Quit out of viewing the segments file by pressing <code>q</code>.</p> <p>We will now open <code>IGV</code> and see if we can observe the predicted increase in copy number within these genomic coordinates.</p> <pre><code>/home/trainee/snv/Applications/igv/igv.sh\n</code></pre> <p>IGV will take 30 seconds or so to open so just be patient.</p> <p> For a duplication of this size, we will not be able to easily observe it just by looking at the raw read alignments. In order to see it we will generate two tiled data files (TDFs) within IGV which contain the average read depth for a given window size through the genome. This means that we can aggregate the average read depth over relatively large chunks of the genome and compare these values between the normal and tumour genomes.</p> <p>To begin, we will go to <code>Tools</code> then <code>Run igvtools...</code> in the IGV menubar. Specify the normal bam file (under <code>cnv</code> then <code>data</code>) as the input file and change the window size to 100,000 (one hundred thousand). Then press the <code>Run</code> button and IGV will make the TDF file. This takes about 5 minutes. Repeat this for the tumour genome.</p> <p>After you have both TDF files, go to <code>File</code> and <code>Load from file...</code> in the menubar and select the BAM and TDF files to open. Once you have opened them, they will appear as tracks along with the BAM tracks we loaded initially. Navigate to the genomic coordinates of our event (5:21,051,700-21,522,065) by typing it in the coordinate box at the top. Mouse over the two blue tracks to get the average depth values for the 100,000 bp windows. What you should see is that the liverMets sample has 3-6X more coverage than the Blood sample for the four windows that cover this region.</p> <p>This may seem a bit underwhelming, after all, wasn\u2019t the increase of the region to a copy number of 4, i.e. we expect a doubling of reads in the tumour? To explain why we are only seeing such a small coverage increase, we need to turn to our good friend mathematics!</p> <p>Imagine we have two 30X genomes for the normal and tumour samples and the tumour is at 100% purity. If there is a copy number increase to 4 in the tumour from 2 in the normal, the duplicated segment should indeed have twice as many reads as the same segment in the normal genome. Now, lets imagine the tumour genome was only at a purity of 50% (i.e. it contains 50% normal cells and 50% tumour cells). Now, half of the duplicated \u201ctumour genome\u201d segment will be at a copy number of 2 and half will be at 4. What does this mean when we sequence them as a mixture? The resulting average copy number of the block will be . Now what if we only have 16% tumour cells in our \u201ctumour genome\u201d? This will be (0.84*2)+(0.16*4) = 2.32. You can see how sequencing a low cellularity tumour at a low depth makes it much harder to infer copy number variations!</p> <p>Returning to our genomes at hand, when we previously looked at the cellularity estimate of this tumour we saw it was 20% from the small block we ran or 16% from the whole genome. Thus, the read depth increase of just 3-6X (about 10-20% more reads) in this segment is not surprising. A low cellularity tumour greatly reduces our power to infer copy number events as relatively small changes in depth can occur by chance in the genome and these can be mis-identified as copy number changes. As well as this, it reduces our power for other analyses since we must also remember that a tumour can itself contain multiple clones which have to share just 16% of reads.</p> <p>It is possible to sequence through a low-cellularity sample when, for example, there is no way to take another sample (as is the case of most biopsies). \u201cSequencing through\u201d means to simply sequence the tumour at a much higher coverage, usually 90-120X. This will mean that there will be a net increase in reads supplying evidence for copy number events and variants and in aggregate these will still retain power to infer these events when using tools that look at the whole genome like Sequenza does.</p>"},{"location":"modules/cancer-module-cnv/cnv-tut/#references","title":"References","text":"<ol> <li>F. Favero, T. Joshi, A. M. Marquard, N. J. Birkbak, M. Krzystanek,     Q. Li, Z. Szallasi, and A. C. Eklund. \u201cSequenza: allele-specific     copy number and mutation profiles from tumor sequencing data\u201d.     Annals of Oncology, 2015, vol. 26, issue 1, 64-70.</li> </ol>"},{"location":"modules/cancer-module-snv/snv/","title":"Single Nucleotide Variant Calling and Annotation","text":""},{"location":"modules/cancer-module-snv/snv/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Prepare raw BAM alignments for variant detection</p> </li> <li> <p>Perform QC measures on BAM files</p> </li> <li> <p>Understand and perform simple variant detection on paired NGS data</p> </li> <li> <p>Add annotation information to raw variant calls</p> </li> <li> <p>Visualise variant calls using IGV</p> </li> </ul>"},{"location":"modules/cancer-module-snv/snv/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-snv/snv/#tools-used","title":"Tools Used","text":"<p>SAMTools: https://samtools.github.io/</p> <p>IGV: http://www.broadinstitute.org/igv/</p> <p>Genome Analysis Toolkit: http://www.broadinstitute.org/gatk/</p> <p>Picard: https://broadinstitute.github.io/picard/</p> <p>MuTect: http://www.broadinstitute.org/cancer/cga/mutect/</p> <p>Strelka: https://sites.google.com/site/strelkasomaticvariantcaller/</p> <p>VarScan2: https://dkoboldt.github.io/varscan/</p> <p>Variant Effect Predictor: http://www.ensembl.org/info/docs/tools/vep</p> <p>GEMINI: http://gemini.readthedocs.org</p>"},{"location":"modules/cancer-module-snv/snv/#sources-of-data","title":"Sources of Data","text":"<p>http://sra.dnanexus.com/studies/ERP001071 http://www.ncbi.nlm.nih.gov/pubmed/22194472</p>"},{"location":"modules/cancer-module-snv/snv/#author-information","title":"Author Information","text":"<p>Primary Author(s): Matt Field matt.field@anu.edu.au Dan Andrews dan.andrews@anu.edu.au Velimir Gayevskiy v.gayevskiy@garvan.org.au Mathieu Bourgey mathieu.bourgey@mcgill.ca </p> <p>Contributor(s): Gayle Philip Sonika.Tyagi@agrf.org.au Sonika Tyagi gkphilip@unimelb.edu.au </p>"},{"location":"modules/cancer-module-snv/snv/#introduction","title":"Introduction","text":"<p>The goal of this hands-on session is to present the main steps that are commonly used to process and to analyze cancer sequencing data. We will focus only on whole genome data and provide command lines that allow detecting Single Nucleotide Variants (SNV). This workshop will show you how to launch individual steps of a complete DNA-Seq SNV pipeline using cancer data.</p> <p>In the second part of the tutorial we will also be using IGV to visualise and manually inspect candidate variant calls.</p>"},{"location":"modules/cancer-module-snv/snv/#prepare-the-environment","title":"Prepare the Environment","text":"<p>We will use a dataset derived from whole genome sequencing of a 33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no familial cancer history.</p> <p>The data consists of whole genome sequencing of liver metastatic lung cancer (frozen), primary lung cancer (FFPE) and blood tissue of a lung adenocarcinoma patient (AK55).</p> <p>Open the Terminal and go to the <code>snv</code> working directory:</p> <pre><code>cd /home/trainee/snv/\n</code></pre>  <p>All commands entered into the terminal for this tutorial should be from within the <code>snv</code> directory.</p>  <p>The BAM alignment files are contained in the subdirectory called <code>alignment</code> and are located in the following subdirectories:</p>  <p><code>normal/normal.sorted.bam</code> and <code>normal/normal.sorted.bam.bai</code></p> <p><code>tumour/tumor.sorted.bam</code> and <code>tumour/tumour.sorted.bam.bai</code></p>  <p>Check that the <code>alignment</code> directory contains the above-mentioned files by typing:</p> <pre><code>ls -l alignment/*\n</code></pre> <p> These files are based on subsetting the whole genomes derived from blood and liver metastases to the first 10Mb of chromosome 4. This will allow our analyses to run in a sufficient time during the workshop, but it\u2019s worth being aware that this is less &lt; 0.5% of the genome which highlights the length of time and resources required to perform cancer genomics on full genomes!</p> <p>The initial structure of your folders should look like this (type <code>ls -l</code>):</p> <pre><code>-- alignment/             # bam files\n  -- normal/                # The blood sample directory containing bam files\n  -- tumour/                # The tumour sample directory containing bam files\n-- ref/                   # Contains reference genome files      \n</code></pre> <p> Now we need to set some environment variables to save typing lengthy file paths over and over. Copy and paste the following commands into your terminal.</p> <pre><code>export APP_ROOT=/home/trainee/snv/Applications\nexport IGVTOOLS_PATH=$APP_ROOT/igvtools/\nexport PICARD_JAR=$APP_ROOT/picard/picard.jar\nexport GATK_JAR=$APP_ROOT/gatk/GenomeAnalysisTK.jar\nexport STRELKA_HOME=$APP_ROOT/strelka/\nexport MUTECT_JAR=$APP_ROOT/mutect/muTect-1.1.5.jar\nexport VARSCAN_JAR=$APP_ROOT/varscan/VarScan.v2.4.1.jar\nexport REF=/home/trainee/snv/ref\nexport SNV_BASE=/home/trainee/snv\nexport JAVA7=/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\nexport IGV=$APP_ROOT/igv/igv.sh\nexport VEP=$APP_ROOT/ensembl-tools/scripts/variant_effect_predictor/variant_effect_predictor.pl\nexport VEP_CACHE=/mnt/workshop/data/bgdata/datasets/vepcache/70-20150729/homo_sapiens/\n</code></pre> <p> Make sure you are in the correct directory by typing:</p> <pre><code>cd $SNV_BASE\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#bam-files","title":"BAM Files","text":"<p>Let\u2019s spend some time exploring BAM files.</p>"},{"location":"modules/cancer-module-snv/snv/#exploring-bam-files","title":"Exploring BAM files","text":"<pre><code>samtools view alignment/normal/normal.sorted.bam | head -n4\n</code></pre> <p>Here you have examples of alignment results. A full description of the flags can be found in the SAM specification.</p> <p>Another useful bit of information in the SAM is the CIGAR string. It\u2019s the 6th column in the file.</p> <p>This column explains how the alignment was achieved.</p>  <p>M == base aligns but doesn\u2019t have to be a match. A SNP will have an M even if it disagrees with the reference. I == Insertion D == Deletion S == soft-clips. These are handy to find un removed adapters, viral insertions, etc.</p>  <p>An in-depth explanation of the CIGAR can be found here. The exact details of the CIGAR string can be found in the SAM specification as well. We won\u2019t go into too much detail at this point since we want to concentrate on cancer specific issues now.</p> <p>Now, you can try using Picard\u2019s explain flag site to understand what is going on with your reads.</p>  <p>Question</p> <p>There are 3 unique flags, what do they mean? The flag is the second column.</p>   Answer <p>129: read paired second in pair  </p> <p>113: read paired read reverse strand mate reverse strand first in pair  </p> <p>161: read paired mate reverse strand second in pair  </p>    <p> There are lots of possible different flags, let\u2019s look at a few more</p> <pre><code>samtools view alignment/normal/normal.sorted.bam | head -n 100\n</code></pre>  <p>Question</p> <p>Let\u2019s take the last read, which looks properly paired and find its mate pair.</p>   Hint <p>a)    Instead of using <code>head</code>, what unix command could we pipe the output to? b)    Once we\u2019ve found both reads, the command can be stopped by typing <code>CTRL-C</code> </p>     Answer <pre><code>samtools view alignment/normal/normal.sorted.bam | grep HWI-ST478_0133:4:2205:14675:32513\n</code></pre>     <p>Question</p> <p>Using the cigar string, what can we tell about the alignment of the mate pair?</p>   Answer <p>The mate pair has a less convincing alignment with two insertions and soft clipping reported.</p>     <p>Question</p> <p>How might the alignment information from the original read be used by the aligner?</p>   Answer <p>Even though the alignment of the mate pair is questionable the presence   of it\u2019s properly paired mate helps the aligner in deciding where to put   the less-certain read.</p>    <p>You can use <code>Samtools</code> to filter reads as well.</p>  <p>Question</p> <p>How many reads mapped and unmapped were there?</p>   Hint <p>Look at the samtools view help menu by typing <code>samtools view</code> without any arguments</p>     Answer <p><pre><code>samtools view -c -f4 alignment/normal/normal.sorted.bam\n</code></pre> 77229 <pre><code>samtools view -c -F4 alignment/normal/normal.sorted.bam\n</code></pre> 22972373</p>"},{"location":"modules/cancer-module-snv/snv/#step-1-pre-processing-indel-realignment","title":"Step 1: Pre-processing: Indel Realignment","text":"<p>The first step for this is to realign around indels and SNP dense regions. The Genome Analysis toolkit (<code>GATK</code>) has a tool for this called IndelRealigner. It basically runs in 2 steps:  </p> <ol> <li>Find the targets</li> <li>Realign them</li> </ol> <pre><code>$JAVA7 -Xmx2G  -jar ${GATK_JAR} \\\n  -T RealignerTargetCreator \\\n  -R ${REF}/human_g1k_v37.fasta \\\n  -o alignment/normal/realign.intervals \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L ${REF}/human_g1k_v37.intervals\n</code></pre> <pre><code>$JAVA7 -Xmx2G -jar ${GATK_JAR} \\\n  -T IndelRealigner \\\n  -R ${REF}/human_g1k_v37.fasta \\\n  -targetIntervals alignment/normal/realign.intervals \\\n  --nWayOut .realigned.bam \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L ${REF}/human_g1k_v37.intervals\n</code></pre> <p>Explanation of parameters:</p>  <p>-I: BAM file(s) -T: GATK algorithm to run -R: the reference genome used for mapping (b37 from GATK here) -jar: Path to GATK jar file -L: Genomic intervals to operate on</p>  <p> Move the realigned BAMs and index files to the corresponding normal and tumour directories.</p> <pre><code>mv normal.sorted.realigned.ba* alignment/normal/\nmv tumour.sorted.realigned.ba* alignment/tumour/\n</code></pre>  <p>Question</p> <p>Why did we use both normal and tumor together?</p>   Answer <p>Because if a region needs realignment, maybe one of the samples in the pair has less reads or was excluded from the target creation. This makes sure the normal and tumor are all in-sync for the somatic calling step.</p>     <p>Question</p> <p>How many regions did it think needed cleaning?</p>   Answer <pre><code>wc -l alignment/normal/realign.intervals\n</code></pre> <p>27300</p>    <p> Indel Realigner also makes sure the called deletions are left aligned when there is a microsatellite or homopolymer. e.g.</p> <p>This   ATCGAAAA-TCG   into   ATCG-AAAATCG  </p> <p>or  </p> <p>ATCGATATATATA\u2013TCG   into   ATCG\u2013ATATATATATCG  </p>   <p>Question</p> <p>Why is it important?</p>   Answer <p>This makes it easier for downstream analysis tools.</p> <p>For NGS analysis, the convention is to left align indels.</p> <p>This is only really needed when calling variants with legacy locus-based tools such as samtools or GATK UnifiedGenotyper. Otherwise you will have worse performance and accuracy.</p> <p>With more sophisticated tools (like GATK HaplotypeCaller) that involve reconstructing haplotypes (e.g. through reassembly), the problem of multiple valid representations is handled internally and does not need to be corrected explicitly.</p>"},{"location":"modules/cancer-module-snv/snv/#step-2-pre-processing-fixmates","title":"Step 2: Pre-processing: Fixmates","text":"<p>Some read entries don\u2019t have their mate information written properly. We use <code>Picard</code> to do this:</p> <p>Normal sample:   <pre><code>$JAVA7 -Xmx2G -jar ${PICARD_JAR} FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/normal/normal.sorted.realigned.bam \\\n  OUTPUT=alignment/normal/normal.matefixed.bam\n</code></pre></p> <p>Tumour sample:  <pre><code>$JAVA7 -Xmx2G -jar ${PICARD_JAR} FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/tumour/tumour.sorted.realigned.bam \\\n  OUTPUT=alignment/tumour/tumour.matefixed.bam\n</code></pre></p>"},{"location":"modules/cancer-module-snv/snv/#step-3-pre-processing-mark-duplicates","title":"Step 3: Pre-processing: Mark Duplicates","text":"<p>Question</p> <p>What are duplicate reads?</p>   Answer <p>Different read pairs representing the same initial DNA fragment.</p>     <p>Question</p> <p>What are they caused by?</p>   Answer <ul> <li>PCR reactions (PCR duplicates).  </li> <li>Some clusters that are thought of being separate in the flowcell but are the same (optical duplicates)</li> </ul>     <p>Question</p> <p>What are the ways to detect them?</p>   Answer <ol> <li> <p>Picard and samtools uses the alignment positions:  </p> <ul> <li>Both 5\u2019 ends of both reads need to have the same positions.</li> <li>Each reads have to be on the same strand as well.</li> </ul> </li> <li> <p>Another method is to use a kmer approach:</p> <ul> <li>Take a part of both ends of the fragment.</li> <li>Build a hash table.</li> <li>Count the similar hits.</li> </ul> </li> <li> <p>Brute force, compare all the sequences.</p> </li> </ol>    <p> Here we will use <code>Picard</code>\u2018s approach:</p> <p>Normal Sample:   <pre><code>$JAVA7 -Xmx2G -jar ${PICARD_JAR} MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/normal/normal.matefixed.bam \\\n  OUTPUT=alignment/normal/normal.sorted.dup.bam \\\n  METRICS_FILE=alignment/normal/normal.sorted.dup.metrics\n</code></pre></p> <p>Tumour Sample: <pre><code>$JAVA7 -Xmx2G -jar ${PICARD_JAR} MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/tumour/tumour.matefixed.bam \\\n  OUTPUT=alignment/tumour/tumour.sorted.dup.bam \\\n  METRICS_FILE=alignment/tumour/tumour.sorted.dup.metrics\n</code></pre></p> <p> We can look in the metrics output to see what happened.</p> <pre><code>less alignment/normal/normal.sorted.dup.metrics\n</code></pre>  <p>Question</p> <p>What percent of reads are duplicates?</p>   Answer <p>0.046996%</p>     <p>Question</p> <p>Often, we have multiple libraries and when this occurs separate measures are calculated for each library. Why is it important to not combine everything?</p>   Answer <ul> <li>Each library represents a set of different DNA fragments.</li> <li>Each library involves different PCR reactions</li> </ul> <p>PCR duplicates can not occur between fragments of two different libraries. However, similar fragments could be found between libraries when the coverage is high.</p>"},{"location":"modules/cancer-module-snv/snv/#step-4-pre-processing-base-quality-recalibration","title":"Step 4: Pre-processing: Base Quality Recalibration","text":"<p>Question</p> <p>Why do we need to recalibrate base quality scores?</p>   Answer <p>The vendors tend to inflate the values of the bases in the reads. The recalibration tries to lower the scores of some biased motifs for some technologies.</p>    <p> Base Quality Recalibration runs in 2 steps:</p> <ol> <li>Build covariates based on context and known SNP sites.</li> <li>Correct the reads based on these metrics.</li> </ol> <p>GATK BaseRecalibrator:</p> <pre><code>for i in normal tumour\ndo\n  $JAVA7 -Xmx2G -jar ${GATK_JAR} \\\n    -T BaseRecalibrator \\\n    -nct 2 \\\n    -R ${REF}/human_g1k_v37.fasta \\\n    -knownSites ${REF}/dbSnp-138_chr4.vcf \\\n    -L 4:1-10000000 \\\n    -o alignment/${i}/${i}.sorted.dup.recalibration_report.grp \\\n    -I alignment/${i}/${i}.sorted.dup.bam\n\n    $JAVA7 -Xmx2G -jar ${GATK_JAR} \\\n      -T PrintReads \\\n      -nct 2 \\\n      -R ${REF}/human_g1k_v37.fasta \\\n      -BQSR alignment/${i}/${i}.sorted.dup.recalibration_report.grp \\\n      -o alignment/${i}/${i}.sorted.dup.recal.bam \\\n      -I alignment/${i}/${i}.sorted.dup.bam\ndone\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#bam-qc","title":"BAM QC","text":"<p>Once your whole BAM is generated, it\u2019s always a good thing to check the data again to see if everything makes sense.</p>"},{"location":"modules/cancer-module-snv/snv/#step-1-bam-qc-compute-coverage","title":"Step 1: BAM QC: Compute Coverage","text":"<p>If you have data from a capture kit, you should see how well your targets worked. <code>GATK</code> has a depth of coverage tool to do this:</p> <pre><code>for i in normal tumour\ndo\n  $JAVA7  -Xmx2G -jar ${GATK_JAR} \\\n    -T DepthOfCoverage \\\n    --omitDepthOutputAtEachBase \\\n    --summaryCoverageThreshold 10 \\\n    --summaryCoverageThreshold 25 \\\n    --summaryCoverageThreshold 50 \\\n    --summaryCoverageThreshold 100 \\\n    --start 1 --stop 500 --nBins 499 -dt NONE \\\n    -R ${REF}/human_g1k_v37.fasta \\\n    -o alignment/${i}/${i}.sorted.dup.recal.coverage \\\n    -I alignment/${i}/${i}.sorted.dup.recal.bam \\\n    -L 4:1-10000000\ndone\n</code></pre> <p>Explanation of parameters:</p>  <p>--omitBaseOutputAtEachBase: Do not output depth of coverage at each base --summaryCoverageThreshold: Coverage threshold (in percent) for summarizing statistics -dt: Downsampling -L: Genomic intervals to operate on  </p>  <p> In this project, coverage is expected to be 25x. Look at the coverage:</p> <pre><code>less -S alignment/normal/normal.sorted.dup.recal.coverage.sample_interval_summary\n</code></pre> <p>Type <code>q</code> to return to the prompt.</p> <pre><code>less -S alignment/tumour/tumour.sorted.dup.recal.coverage.sample_interval_summary\n</code></pre>  <p>Question</p> <p>Does the coverage fit with the expectation?</p>   Answer <ul> <li> <p>Yes the mean coverage of the region is 25x.</p> </li> <li> <p><code>summaryCoverageThreshold</code> is a useful function to see if your coverage is uniform.</p> </li> <li> <p>Another way is to compare the mean to the median. If both are quite different that means something is wrong in your coverage.</p> </li> </ul>"},{"location":"modules/cancer-module-snv/snv/#step-2-bam-qc-insert-size","title":"Step 2: BAM QC: Insert Size","text":"<p>Insert size corresponds to the size of DNA fragments sequenced. It is different from the gap size (= distance between reads)!</p> <p> These metrics are computed using <code>Picard</code>:</p> <pre><code>for i in normal tumour\ndo\n  $JAVA7 -Xmx2G -jar ${PICARD_JAR} CollectInsertSizeMetrics \\\n    VALIDATION_STRINGENCY=SILENT \\\n    REFERENCE_SEQUENCE=${REF}/human_g1k_v37.fasta \\\n    INPUT=alignment/${i}/${i}.sorted.dup.recal.bam \\\n    OUTPUT=alignment/${i}/${i}.sorted.dup.recal.metric.insertSize.tsv \\\n    HISTOGRAM_FILE=alignment/${i}/${i}.sorted.dup.recal.metric.insertSize.histo.pdf \\\n    METRIC_ACCUMULATION_LEVEL=LIBRARY\ndone\n</code></pre> <p> Look at the output:</p> <pre><code>less -S alignment/normal/normal.sorted.dup.recal.metric.insertSize.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.insertSize.tsv\n</code></pre>  <p>Question</p> <p>How do the two libraries compare?    </p>   Answer <p>The tumour sample has a larger median insert size than the normal sample (405 vs. 329).</p>"},{"location":"modules/cancer-module-snv/snv/#step-3-bam-qc-alignment-metrics","title":"Step 3: BAM QC: Alignment metrics","text":"<p>Alignment metrics tells you if your sample and your reference fit together.</p> <p>For the alignment metrics, <code>samtools flagstat</code> is very fast but <code>bwa-mem</code> breaks some reads into pieces, the numbers can be a bit confusing.</p> <p>Instead, we will use <code>Picard</code> to compute the metrics:</p> <pre><code>for i in normal tumour\ndo\n  $JAVA7 -Xmx2G -jar ${PICARD_JAR} CollectAlignmentSummaryMetrics \\\n    VALIDATION_STRINGENCY=SILENT \\\n    REFERENCE_SEQUENCE=${REF}/human_g1k_v37.fasta \\\n    INPUT=alignment/${i}/${i}.sorted.dup.recal.bam \\\n    OUTPUT=alignment/${i}/${i}.sorted.dup.recal.metric.alignment.tsv \\\n    METRIC_ACCUMULATION_LEVEL=LIBRARY\ndone\n</code></pre> <p> Explore the results</p> <pre><code>less -S alignment/normal/normal.sorted.dup.recal.metric.alignment.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.alignment.tsv\n</code></pre>  <p>Question</p> <p>Do you think the sample and the reference genome fit together?</p>   Answer <p>Yes, 99% of the reads have been aligned. Usually, we consider:</p> <ul> <li>A good alignment if &gt; 85%</li> <li>Reference assembly issues if [60-85]%</li> <li>Probably a mismatch between sample and ref if &lt; 60 %</li> </ul>"},{"location":"modules/cancer-module-snv/snv/#variant-calling","title":"Variant Calling","text":"<p>Most of SNV caller use either a Bayesian, a threshold or a t-test approach to do the calling</p> <p>Here we will try 3 variant callers:</p> <ul> <li><code>Varscan 2</code></li> <li><code>MuTecT</code></li> <li><code>Strelka</code></li> </ul> <p>Other candidates:</p> <ul> <li><code>Virmid</code></li> <li><code>Somatic sniper</code></li> </ul> <p>Many, MANY others can be found here: https://www.biostars.org/p/19104/</p> <p>In our case, let\u2019s create a new work directory to start with (from base directory):</p> <pre><code>cd $SNV_BASE\nmkdir variant_calling\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#varscan-2","title":"Varscan 2","text":"<p><code>Varscan 2</code> calls somatic variants (SNPs and indels) using a heuristic method and a statistical test based on the number of aligned reads supporting each allele. It expects both a normal and a tumour file in <code>SAMtools pileup</code> format from sequence alignments in binary alignment/map (BAM) format. To build a pileup file, you will need:</p> <ul> <li>A SAM/BAM file (<code>*.sorted.dup.recal.bam</code>) that has been sorted using the <code>sort</code> command of <code>samtools</code>.</li> <li>The reference sequence (<code>human_g1k_v37.fasta</code>) to which reads were aligned, in FASTA format.</li> <li>The <code>samtools</code> software package.</li> </ul> <pre><code>for i in normal tumour\ndo\nsamtools mpileup -L 1000 -B -q 1 \\\n  -f ${REF}/human_g1k_v37.fasta \\\n  -r 4:1-10000000 \\\n  alignment/${i}/${i}.sorted.dup.recal.bam \\\n  &gt; variant_calling/${i}.mpileup\ndone\n</code></pre> <p>Notes on <code>samtools</code> arguments:</p>  <p>-L: max per-sample depth for INDEL calling [1000] -B: disable BAQ (per-Base Alignment Quality) -q: skip alignments with mapQ smaller than 1 -g: generate genotype likelihoods in BCF format</p>  <p>Notes on <code>bcftools</code> arguments:</p>  <p>-v: output potential variant sites only -c: SNP calling (force \u2013e : likelihood based analyses) -g: call genotypes at variant sites  </p>  <p> <pre><code>$JAVA7 -Xmx2G -jar ${VARSCAN_JAR} \\\nsomatic variant_calling/normal.mpileup \\\nvariant_calling/tumour.mpileup \\\nvariant_calling/varscan \\\n--output-vcf 1 \\\n--strand-filter 1 \\\n--somatic-p-value 0.001\n</code></pre></p>"},{"location":"modules/cancer-module-snv/snv/#mutect","title":"MuTect","text":"<p>Now let\u2019s try a different variant caller, <code>MuTect</code>.</p> <pre><code>$JAVA7 -Xmx2G -jar ${MUTECT_JAR} \\\n  -T MuTect \\\n  -R ${REF}/human_g1k_v37.fasta \\\n  -dt NONE -baq OFF --validation_strictness LENIENT \\\n  --dbsnp ${REF}/dbSnp-138_chr4.vcf \\\n  --input_file:normal alignment/normal/normal.sorted.dup.recal.bam \\\n  --input_file:tumor alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --out variant_calling/mutect.call_stats.txt \\\n  --coverage_file variant_calling/mutect.wig.txt \\\n  -pow variant_calling/mutect.power \\\n  -vcf variant_calling/mutect.vcf \\\n  -L 4:1-10000000\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#strelka","title":"Strelka","text":"<p>And finally let\u2019s try Illumina\u2019s <code>Strelka</code>.</p> <pre><code>  cp ${STRELKA_HOME}/etc/strelka_config_bwa_default.ini .\n\n  sed 's/isSkipDepthFilters =.*/isSkipDepthFilters = 1/g' -i strelka_config_bwa_default.ini\n\n  ${STRELKA_HOME}/bin/configureStrelkaWorkflow.pl \\\n    --normal=alignment/normal/normal.sorted.dup.recal.bam \\\n    --tumor=alignment/tumour/tumour.sorted.dup.recal.bam \\\n    --ref=${REF}/human_g1k_v37.fasta \\\n    --config=${SNV_BASE}/strelka_config_bwa_default.ini \\\n    --output-dir=variant_calling/strelka/\n\n  cd variant_calling/strelka/\n  make -j2\n  cd ../..\n\n  cp variant_calling/strelka/results/passed.somatic.snvs.vcf variant_calling/strelka.vcf\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#comparing-variant-callers","title":"Comparing variant callers","text":"<p>Now we have variants from all three methods. Let\u2019s compress and index the VCFs for future visualisation.</p> <pre><code>for i in variant_calling/*.vcf; do bgzip -c $i &gt; $i.gz ; tabix -p vcf $i.gz; done\n</code></pre> <p>Let\u2019s look at a compressed VCF. Details on the VCF spec can be found here.</p> <pre><code>zless -S variant_calling/varscan.snp.vcf.gz\n</code></pre> <p> Fields vary from caller to caller. Some values are are almost always there:</p> <ul> <li>Ref vs. alt alleles</li> <li>Variant quality (QUAL column)</li> <li>The per-sample genotype (GT) values.</li> </ul> <p>Note on VCF fields:</p>  <p>DP: Raw read depth GT: Genotype PL: List of Phred-scaled genotype likelihoods. (min is better) DP: \u201c\u201d# high-quality bases\u201d SP: Phred-scaled strand bias P-value GQ: Genotype Quality  </p>    <p>Question</p> <p>Looking at the three vcf files, how can we detect only somatic variants?</p>   Answer <p>Some commands to find somatic variant in the vcf file:</p> <p>varscan:</p> <pre><code>grep SOMATIC variant_calling/varscan.snp.vcf\n</code></pre> <p>MuTecT:</p> <pre><code>grep -v REJECT variant_calling/mutect.vcf | grep -v \"^#\"\n</code></pre> <p>Strelka:</p> <pre><code>grep -v \"^#\" variant_calling/strelka.vcf\n</code></pre>"},{"location":"modules/cancer-module-snv/snv/#variant-visualisation","title":"Variant Visualisation","text":"<p>The Integrative Genomics Viewer (<code>IGV</code>) is an efficient visualization tool for interactive exploration of large genome datasets.</p> <p>Before jumping into <code>IGV</code>, we\u2019ll generate a track IGV that can be used to plot coverage:</p> <pre><code>for i in normal tumour\ndo\n  $JAVA7 -jar ${IGVTOOLS_PATH}/igvtools.jar count \\\n    -f min,max,mean \\\n    alignment/${i}/${i}.sorted.dup.recal.bam \\\n    alignment/${i}/${i}.sorted.dup.recal.bam.tdf \\\n    b37\ndone\n</code></pre> <p>Open <code>IGV</code></p> <pre><code>$IGV\n</code></pre> <p>Then:</p> <ol> <li>Choose the reference genome corresponding to those use for alignment (b37).</li> <li>Load BAM files (<code>tumour.sorted.dup.recal.bam</code> and <code>normal.sorted.dup.recal.bam</code>).</li> <li>Load VCF files (from <code>variant_calling</code> directory).</li> </ol>  <p>Explore and play with the data:</p> <ul> <li>Find germline variants</li> <li>Find somatic variants</li> <li>Look around\u2026</li> </ul>"},{"location":"modules/cancer-module-snv/snv/#variant-annotation","title":"Variant Annotation","text":"<p>Following variant calling, we end up with a VCF file of genomic coordinates with the genotype(s) and quality information for each variant. By itself, this information is not much use to us unless there is a specific genomic location we are interested in. Generally, we next want to annotate these variants to determine whether they impact any genes and if so what is their level of impact (e.g. are they causing a premature stop codon gain or are they likely less harmful missense mutations).</p> <p>The sections above have dealt with calling somatic variants from the first 10Mb of chromosome 4. This is important in finding variants that are unique to the tumour sample(s) and may have driven both tumour growth and/or metastasis. An important secondary question is whether the germline genome of the patient contains any variants that may have contributed to the development of the initial tumour through predisposing the patient to cancer. These variants may not be captured by somatic variant analysis as their allele frequency may not change in the tumour genome compared with the normal.</p> <p>For this section, we will use all variants from the first 60Mb of chromosome 5 that have been pre-generated using the GATK <code>HaplotypeCaller</code> variant caller on both the normal and tumour genomes. The output of this was GVCF files which were fed into GATK <code>GenotypeGVCFs</code> to produce a merged VCF file. We will use this pre-generated file as we are primarily interested in the annotation of variants rather than their generation. The annotation method we will use is called <code>Variant Effect Predictor</code> or <code>VEP</code> for short and is available from Ensembl here.</p> <p> Our pre-generated VCF file is located in the <code>variants</code> folder. Let\u2019s have a quick look at the variants:</p> <pre><code>zless variants/HC.chr5.60Mb.vcf.gz\n</code></pre> <p>Notice how there are two genotype blocks at the end of each line for the normal (<code>Blood</code>) and tumour (<code>liverMets</code>) samples.</p> <p> Let\u2019s now run <code>VEP</code> on this VCF file to annotate each variant with its impact(s) on the genome.</p> <pre><code>perl $VEP --dir_cache $VEP_CACHE -i variants/HC.chr5.60Mb.vcf.gz --vcf -o variants/HC.chr5.60Mb.vep.vcf --stats_file variants/HC.chr5.60Mb.vep.html --format vcf --offline -fork 4 --fasta ref/human_g1k_v37.fasta --fields Consequence,Codons,Amino_acids,Gene,SYMBOL,Feature,EXON,PolyPhen,SIFT,Protein_position,BIOTYPE\n</code></pre> <p> <code>VEP</code> will take approximately 10 minutes to run and once it is finished you will have a new VCF file with all of the information in the input file but with added annotations in the INFO block. <code>VEP</code> also produces an HTML report summarising the distribution and impact of variants identified.</p> <p>Once <code>VEP</code> is done running, let\u2019s first look at the HTML report it produced with the following command:</p> <pre><code>firefox variants/HC.chr5.60Mb.vep.html\n</code></pre> <p>This report shows information on the <code>VEP</code> run, the number of variants, the classes of variants detected, the variant consequences and the distributions of variants through the genome. Close Firefox to resume the terminal prompt.</p> <p> Now let\u2019s look at the variant annotations that <code>VEP</code> has added to the VCF file by focussing on a single variant. Let\u2019s fetch the same variant from the original VCF file and the annotated VCF file to see what has been changed.</p> <pre><code>zcat variants/HC.chr5.60Mb.vcf.gz | grep '5\\s174106\\s'\ngrep '5\\s174106\\s' variants/HC.chr5.60Mb.vep.vcf\n</code></pre> <p> These commands give us the original variant:</p> <pre><code>5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446   GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n</code></pre> <p>and the same variant annotated is:</p> <pre><code>5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446;CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&amp;non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron  GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n</code></pre> <p> You can see that <code>VEP</code> has added:</p> <pre><code>CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant&amp;non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n</code></pre> <p> This is further composed of two annotations for this variant: <pre><code>missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding\n</code></pre></p> <p>and</p> <pre><code>non_coding_transcript_exon_variant&amp;non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n</code></pre> <p> The first of these is saying that this variant is a missense variant in the gene PLEKHG4B for the transcript ENST00000283426 and the second that it is also a non_coding_transcript_exon_variant in the transcript ENST00000504041.</p>"},{"location":"modules/cancer-module-snv/snv/#variant-filtration","title":"Variant Filtration","text":"<p>We now have a VCF file where each variant has been annotated with one or more impacts for one or more genes. In a typical whole cancer genome, you will have about 4-5 million variants, and therefore rows, in a VCF file which takes up gigabytes of space. In our small example, we have just 100,000 variants which is already too large to make any kind of meaningful sense out of by just opening up the VCF file in a text editor. We need a solution that allows us to perform intelligent queries on our variants to search this mass of noise for the signal we are interested in.</p> <p>Luckily, such a free tool exists and is called <code>GEMINI</code>. <code>GEMINI</code> takes as an input your annotated VCF file and creates a database file which it can then query using Structured Query Language (SQL) commands. Not only does <code>GEMINI</code> make your variants easily searchable, it also brings in many external annotations to add more information about your variants (such as their frequencies in international databases).</p> <p>To get started with <code>GEMINI</code>, let\u2019s make a database out of our annotated VCF file.</p> <pre><code>gemini load -v variants/HC.chr5.60Mb.vep.vcf --cores 4 --skip-gerp-bp --skip-cadd -t VEP variants/HC.chr5.60Mb.vep.vcf.db\n</code></pre> <p>This will take approximately 10 minutes. You will see a few errors due to multiallelic sites, normally these sites are decomposed and normalized before creating the <code>GEMINI</code> database but this is outside the scope of this workshop.</p> <p>Once the database has been created let\u2019s run a basic query to see what kind of information we get out of <code>GEMINI</code>.</p> <pre><code>gemini query -q \"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants LIMIT 10;\" --header variants/HC.chr5.60Mb.vep.vcf.db\n</code></pre> <p>This will output a bunch of ordered information for your query to the command line, this is usually saved to a TSV file and opened in a spreadsheet as we will do for the next query. In the mean time, let\u2019s dissect this query to understand the syntax we need to use to filter our variants. First, we have a SELECT statement which simply specifies that we want to select data from the database. The following comma-separated values are the columns that we want to output from the database, in this case we are selecting all columns with the star character and then all sub-columns for each sample with the other values. Then, we have a \u201cFROM variants\u201d statement which is specifying the table within the database that we want to fetch information from. Finally, the \u201cLIMIT 10\u201d statement specifies that no more than 10 rows should be returned. In summary then, we are asking for all columns for 10 rows from the table <code>variants</code>. If you haven\u2019t used SQL before don\u2019t worry, the <code>GEMINI</code> website is very helpful and provides many examples for how to query your database.</p> <p>Let\u2019s now perform a more interesting query to find variants that have a medium or high impact on a gene and are rare or not present in existing international allele frequency databases. We will save the output of this query to a file and open it up in a spreadsheet.</p> <pre><code>gemini query -q \"SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants WHERE (impact_severity = 'HIGH' OR impact_severity = 'MED') AND (aaf_1kg_all &lt; 0.01 OR aaf_1kg_all is null) AND (aaf_esp_all &lt; 0.01 OR aaf_esp_all is null) AND (aaf_exac_all &lt; 0.01 OR aaf_exac_all is null);\" --header variants/HC.chr5.60Mb.vep.vcf.db &gt; variants/gemini-result.tsv\n</code></pre> <p>Notice that we have added a WHERE statement which restricts the rows that are returned based on values that we specify for specific columns. Here, we are asking to return variants where their impact on the gene (impact_severity column) is medium or high and the allele frequency in 1000Genomes, ESP and EXaC is less than 1% or the variant is not present in any of these databases.</p> <p>Now let\u2019s open the result in a spreadsheet to look at the annotations:</p> <pre><code>libreoffice --calc variants/gemini-result.tsv\n</code></pre> <p>Tick the \u201cTab\u201d under \u201cSeparated by\u201d on the dialog window that comes up.  </p> <p>You can see that the first 14 columns contain information on the variant including its location, ref, alt, dbSNP ID, quality and type. Slowly scroll to the right and look at the columns of data that are provided. Most importantly, column BD includes the gene this variant impacts, BN the impact itself and BP the impact severity. Scroll towards the end of the spreadsheet until you get to columns ED and EE, these contain the genotype for each of the samples. Columns EH and EI contain the total depth for each variant in each sample and the 4 following columns contain the reference and alternate depths for each sample. Finally, columns EN and EO contain the genotype qualities (from the GQ field in the VCF) for each sample. As you scroll back and forth through this spreadsheet, you will see that <code>GEMINI</code> brings in information from a variety of sources including: OMIM, ClinVar, GERP, PolyPhen 2, SIFT, ESP, 1000 Genomes, ExAC, ENCODE, CADD and more! We are only looking at a small number of variants from the start of a chromosome so not many of these annotations will be present but in a full genome database they are incredibly useful.</p> <p><code>GEMINI</code> allows you to filter your variants based on any column that you see in this results file. For example, you may want all variants in a specific gene, in which case you would simply add \u201cWHERE gene = \u2019BRCA1\u2019\u201d to your query. For complete documentation with many examples of queries, see the GEMINI documentation here.</p>"},{"location":"modules/cancer-module-snv/snv/#references","title":"References","text":"<ol> <li> <p>Paila U, Chapman BA, Kirchner R and Quinlan AR. \u201cGEMINI: Integrative     Exploration of Genetic Variation and Genome Annotations\u201d. PLoS     Comput Biol, 2013, 9(7): e1003153. doi:10.1371/journal.pcbi.1003153</p> </li> <li> <p>McLaren W, Pritchard B, Rios D, Chen Y, Flicek P and Cunningham F.     \u201cDeriving the consequences of genomic variants with the Ensembl API     and SNP Effect Predictor\u201d. Bioinformatics, 2010, 26(16):2069-70,     doi:10.1093/bioinformatics/btq330</p> </li> </ol>"},{"location":"modules/cancer-module-snv/snv/#acknowledgements","title":"Acknowledgements","text":"<p>This is based on an Introduction to DNA-Seq processing for cancer data by Mathieu Bourgey, Ph.D.  </p> <p>This tutorial is an adaptation of the one created by Louis letourneau. Mathieu Bourgey would like to thank and acknowledge Louis for this help and for sharing his material. The format of the tutorial has been inspired from Mar Gonzalez Porta. He also wants to acknowledge Joel Fillon, Louis Letrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume Bourque for the help in building these pipelines and working with all the various datasets.</p>"},{"location":"modules/cancer-module-somatic/01_signatures/","title":"Assessing somatic mutational signatures","text":""},{"location":"modules/cancer-module-somatic/01_signatures/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Visualise mutational signatures present in a cohort using somatic     single nucleotide mutation data in Variant Call Format (VCF) files.</p> </li> <li> <p>Compare analysis output with published results to identify common     mutational signatures.</p> </li> <li> <p>Have gained overview knowledge of how somatic signatures can help     with cohort cancer analysis.</p> </li> </ul>"},{"location":"modules/cancer-module-somatic/01_signatures/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-somatic/01_signatures/#tools-used","title":"Tools Used","text":"<p>R-3.2.2 statistical environment: https://www.r-project.org/</p> <p>SomaticSignatures R package: http://bioconductor.org/packages/release/bioc/html/SomaticSignatures.html</p> <p>BSgenome.Hsapiens.UCSC.hg19: http://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg19.html</p> <p>VariantAnnotation: https://bioconductor.org/packages/release/bioc/html/VariantAnnotation.html</p> <p>GenomicRanges: https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html</p> <p>Cairo: https://cran.rstudio.com/web/packages/Cairo/index.html</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#sources-of-data","title":"Sources of Data","text":"<p>TCGA melanoma SNV data: https://tcga-data.nci.nih.gov/tcga/</p> <p>ICGC ovarian SNV data:  https://dcc.icgc.org/</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#useful-links","title":"Useful Links","text":"<p>Variant Call Format (VCF) specification:  http://samtools.github.io/hts-specs/VCFv4.2.pdf</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#author-information","title":"Author Information","text":"<p>Primary Author(s): Ann-Marie Patch, QIMR Berghofer ann-marie.patch@qimrberghofer.edu.au Erdahl Teber, CMRI eteber@cmri.org.au </p> <p>Contributor(s): Martha Zakrzewski Martha.Zakrzewski@qimrberghofer.edu.au</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#introduction","title":"Introduction","text":"<p>The most common genetic model for cancer development is the accumulation of DNA mutations over time, eventually leading to the disruption or dysregulation of enough key genes that lead cells to uncontrolled growth. Cells in our bodies accumulate DNA mutations over time due to normal aging processes and through exposure to carcinogens.</p> <p>Recently researchers found a method to take all the single nucleotide mutations identified in tumour cells (somatic SNVs) and group them together by the type of the mutation and also what the neighbouring bases are. This is commonly referred to as somatic mutational signatures. Common mutational processes that are regularly identified in cancer sequencing are:</p> <ul> <li> <p>Age: the aging process. These are high in C/T transitions due to     deamination of methyl-cytidine.</p> </li> <li> <p>Smoking: marks exposure to inhaled carcinogens and has high numbers     of C/A transversions.</p> </li> <li> <p>UV: UV exposure. These are also high in C/T transitions at     di-pyrimidine sites.</p> </li> <li> <p>BRCA: Indicates that the homologous recombination repair pathway is     defective.</p> </li> <li> <p>APOBEC: Thought to be marking dysregulated APOBEC enzyme activity on     single stranded DNA produced during the repair processing of other     lesions such as double stand breaks.</p> </li> <li> <p>MMR: Mismatch repair pathway not working properly. These are high in     C/T mutations too.</p> </li> </ul> <p> In cohort cancer analysis it is common to try to generate subtypes to group your data based on a particular molecular phenotype. A reason for doing may include finding sets of patients that have a similar form of the disease and therefore all might benefit from a particular treatment. We can use the somatic mutational signatures analysis to group the data from a cohort of patients to inform which genomes are most similar based on the pattern of exposures or processes that have contributed to their genome changes. The patients don\u2019t have to have the same type of cancer so pan-cancer studies are using this analysis to find similarities across cancer types.</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#preparing-the-r-environment","title":"Preparing the R environment","text":"<p>The mathematical framework developed by Alexandrov et al. was implemented in MATLAB. We are going to use a version implemented in R by Gehring et al. called <code>SomaticSignatures package</code>, that is very quick and flexible but currently only accepts point mutations not insertions or deletions (indels). In tests on our data we have found that the Somatic Signatures package in R returns very similar results to the full implementation of Alexandrov\u2019s framework.</p> <p>The data files you will need are contained in the subdirectory called <code>somatic/somatic_signatures</code>:</p> <p>Open the Terminal and go to the <code>somatic_signatures</code> working directory:</p> <pre><code>cd ~/somatic/somatic_signatures\npwd\n</code></pre> <p>In this folder you should find 12 files that end with the extension <code>.vcf</code>. Use the list command to make sure you can see them.</p> <pre><code>ls\n</code></pre> <p>These files contain data extracted from the TCGA melanoma paper and Australian ICGC ovarian paper both mentioned in the introductory slides. They have been edited in order to allow this practical to run quickly and are not good examples of VCF files.</p> <p>Start R and set the working directory. Just start by typing R onto the command line.</p> <pre><code>R\n</code></pre> <p>Load all the package libraries needed for this analysis by running the commands.</p> <pre><code>library(SomaticSignatures)\nlibrary(BSgenome.Hsapiens.UCSC.hg19)\nlibrary(ggplot2)\nlibrary(Cairo)\n</code></pre> <p>Set the directory where any output files will be generated</p> <pre><code>setwd(\"~/somatic/somatic_signatures\")\n</code></pre>"},{"location":"modules/cancer-module-somatic/01_signatures/#loading-and-preparing-the-snv-mutation-data","title":"Loading and preparing the SNV mutation data","text":"<p>The mutations used in this analysis need to be high quality somatic mutations</p> <ul> <li> <p>Remember the goal is to find the key mutational processes that these     tumours have been exposed to, so you need to exclude germline     mutations (mutations that the person was born with that can be seen     in the sequencing of matched normal samples).</p> </li> <li> <p>Sequencing errors can also occur at particular DNA sequence contexts     and can also be picked up using this method. To avoid this use only     high quality mutation calls.</p> </li> </ul> <p>Read in the mutations from the 12 VCF files</p> <pre><code>files &lt;- list.files(\"~/somatic/somatic_signatures\", pattern=\"vcf$\", full.names=TRUE)\n</code></pre> <p>To make sure all the files are listed run the command.</p> <pre><code>files\n</code></pre> <p>You should see a list of 12 sample files.</p> <p>Next read in all the genomic positions of variants in the VCF files using the <code>vranges</code> class.</p> <pre><code>vranges &lt;- lapply(files, function(v) readVcfAsVRanges(v,\"hg19\"))\n</code></pre> <p>Join all the lists of variant positions into one big data set so that it can be processed together and look at what is contained in the concatenated <code>vranges</code> data</p> <pre><code>vranges.cat &lt;- do.call(c,vranges)\nvranges.cat\n</code></pre> <p>The first line of output of the <code>vranges.cat</code> shows us that in total we have put over 100,000 mutations recording the chromosome positions and mutation base changes along with what sample they were seen in.</p> <p>Note there are a lot of NA values in this data set because we have left out non-essential information in order to cut down on the processing time.</p> <p>Next we need to ensure all the positions in the <code>vranges</code> object have been recorded in UCSC notation form so that they will match up with the reference we are using.</p> <pre><code>vranges.cat &lt;- ucsc(vranges.cat)\n</code></pre> <p>It is always important to select the correct reference for your data.</p> <p>We can print out how many mutations we have read in for each of the cancer samples we are using by using the command.</p> <pre><code>print(table(sampleNames(vranges.cat)))\n</code></pre> <p>We have now added all the positional and base change information now we can use the reference and the position of the mutation to look up the bases on either side of the mutation i.e. the mutation context.</p> <p>Run the mutationContext function of SomaticSignatures.</p> <pre><code>mc &lt;- mutationContext(vranges.cat, BSgenome.Hsapiens.UCSC.hg19)\n</code></pre> <p>We can inspect what information we had added to the <code>vranges.cat</code> object by typing <code>mc</code> on the command line. Notice that the mutation and its context have been added to the last two columns.</p> <pre><code>mc\n</code></pre>"},{"location":"modules/cancer-module-somatic/01_signatures/#snv-mutation-context","title":"SNV mutation context","text":"<p>There are a total of 96 possible single base mutations and context combinations. We can calculate this by listing out the six possible types of single nucleotide mutations:</p> <pre>\n  -   C/A   the reverse compliment (G/T) is also in this group\n  -   C/G   includes (G/C)\n  -   C/T   includes (G/C)\n  -   T/A   includes (A/T)\n  -   T/C   includes (A/G)\n  -   T/G   includes (A/C)\n  </pre> <p>The neighbouring bases, on either side of a mutation, are referred to as the mutation context. There are 16 possible combinations of mutation contexts. Here [.] stands for one of the mutations listed above.</p> <pre>\n  -   A[.]A   A[.]C   A[.]G   A[.]T\n  -   C[.]A   C[.]C   C[.]G   C[.]T\n  -   G[.]A   G[.]C   G[.]G   G[.]T\n  -   T[.]A   T[.]C   T[.]G   T[.]T\n  </pre> <p>Now if we substitute the [.]\u2019s with each of the 6 different mutations you will find there are 96 possible types of combined mutations and contexts (6 x 16).</p> <p>Start by substituting [.] for the A/C mutation type</p> <pre>\n  -   A[C/A]A\n  -   A[C/A]C\n  -   A[C/A]G\n  -   A[C/A]T\n  -   C[C/A]A\n  -   C[C/A]C\n  -   C[C/A]G\n  </pre> <p>and so on\u2026</p> <p>We assign all the somatic mutations identified in a single tumour to one of these categories and total up the number in each.</p>  <p>Question</p> <p>What about a mutation that looks like G[A/C]A, where should this go?</p>   Hint <p>Remember to reverse compliment all the nucleotides.</p>     Answer <p>In the T[T/G]C context count.</p>    <p> Now we have all the information that is needed for each sample we can make a matrix that contains counts of mutations in each of the 96 possible combinations of mutations and contexts counting up the totals separately for each sample</p> <pre><code>mm &lt;- motifMatrix(mc, group = \"sampleNames\", normalize=TRUE)\ndim(mm)\n</code></pre> <p>The output of the <code>dim(mm)</code> command show us that there are 96 rows (these are the context values) and 12 columns which are the 12 samples.</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#running-the-nmf-analysis","title":"Running the NMF analysis","text":"<p>Using the matrix we have made we can now run the non-negative matrix factorisation (NMF) process that attempts to find the most stable, grouping solutions for all of the combinations of mutations and contexts. It does this by trying to find similar patterns, or profiles, amongst the samples to sort the data into firstly just 2 groups. This is repeated to get replicate values for each attempt and then separating the data by 3 groups, and then 4 and so on.</p> <p>These parameter choices have been made to keep running time short for this practical. If you have more samples from potentially diverse sources you may need to run with a larger range of signatures and with more replicates.</p> <p>To find out how many signatures we have in the data run the command.</p> <pre><code>gof_nmf &lt;- assessNumberSignatures(mm, 2:10, nReplicates = 5)\n</code></pre> <p>Visualise the results from the NMF processing by making a pdf of the plot</p> <pre><code>pdf(file=\"plotNumberOfSignatures.pdf\", width=9, height=8)\nplotNumberSignatures(gof_nmf)\ndev.off()\n</code></pre> <p>Open up the PDF and examine the curve. The plotNumberOfSignatures PDF that will have been made in the working directory that you set up at the beginning</p> <p> Figure 1: This plot is used to find the number of signatures that is likely to be the best grouping solution. The top plot shows the decreasing residual sum of squares for each increasing number of signatures and the bottom plot the increasing explained variance as the number of potential signatures increases. Ideally the best solution will be the lowest number of signatures with a low RSS and a high explained variance.</p> <p> Look at the y-axis scale on the bottom panel of Figure 1. The explained variance is already very high and so close to finding the correct solution for the number of signatures even with just 2. The error bars around each point are fairly small considering we have a very small sample set. Deciding how many signatures are present can be tricky but here let\u2019s go for 3. This is where the gradient of both curves have started to flatten out.</p> <p>Now run the NMF again but this time stipulating that you want to group the data into 3 different mutational signatures.</p> <pre><code>  sigs_nmf = identifySignatures(mm, 3, nmfDecomposition)\n</code></pre> <p>Visualise the shape of the profiles for these 3 signatures</p> <pre><code>  pdf(file=\"plot3Signatures.pdf\", width=10, height=8)\n  plotSignatures(sigs_nmf,normalize=TRUE, percent=FALSE) + ggtitle(\"Somatic Signatures: NMF - Barchart\") + scale_fill_brewer(palette = \"Set2\")\n  dev.off()\n</code></pre> <p>Open up <code>plot3Signatures.pdf</code> that will have been made in the working directory.</p> <p>You should have generated a plot with three signature profiles obtained from the NMF processing of the test dataset.</p> <p>The 96 possible mutation/context combinations are plotted along the x axis arranged in blocks of 6 lots of 16 (see information above). The height of the bars indicates the frequency of those particular mutation and context combinations in each signature.</p> <p>Although the section colours are different to the plot you have generated the mutations are still in the same order across the plot.</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#interpreting-the-signature-results","title":"Interpreting the signature results","text":"<p>In their paper Alexandrov et al. used this analysis to generate profiles from the data for more than 7000 tumour samples sequenced through both exome and whole genome approaches. They were able to group the data to reveal which genomes have been exposed to similar mutational processes contributing to the genome mutations.</p> <p> Figure 2. The 96 possible mutation/context combinations are plotted along the x axis arranged in blocks of 6 lots of 16. The y axis indicates the frequency of those particular mutation and context combinations in each signature. Source: Alexandrov et al. Nature 2013</p>   <p>Question</p> <p>Can you match up, by eye, the profile shapes against a selection of known mutational signatures supplied (Figure 2)?</p> <p>Try to match up the patterns made by the positions of the highest peaks for each signature.</p>   Answer <ul> <li> <p>Alexandrov signature 7 matches with our signature 1</p> </li> <li> <p>Alexandrov signature 13 matches with our signature 2</p> </li> <li> <p>Alexandrov signature 3 matches with our signature 3</p> </li> </ul>    <p> Now use the table from Alexandrov et al. to identify which mutational processes our three generated signatures have been associated with.</p> <p> Figure 3. The 21 signatures identified are indicated as rows with the number corresponding to Figure 2 on the left. The types of tumours used in the analysis are listed as columns. A green dot at the intersection of a signature and tumour indicates the signature was identified in that sample type. Where verified the mutational process is listed on the right. Source: Alexandrov et al. Nature 2013</p>   <p>Question</p> <p>What mutational mechanisms have been associated with the signatures that you have generated?</p>   Answer <ul> <li> <p>Our signature 1 (AS7) is associated with Ultraviolet radiation     damage to DNA. This has previously been identified in Head and Neck     and Melanoma cancer samples.</p> </li> <li> <p>Our signature 2 (AS 13) is associated with the activity of     anti-viral APOBEC enzymes. This has previously been seen in Breast     and Bladder cancer samples.</p> </li> <li> <p>Our signature 3 (AS3) is associated with BRCA1 and BRCA2 mutations,     i.e. the homologous recombination repair pathway not working     properly. This has been seen in Breast, Ovarian and Pancreas cancer     samples.</p> </li> </ul>    <p> Now we can plot out the results for the individual samples in our dataset to show what proportion of their mutations have been assigned to each of the signatures.</p> <pre><code>pdf(file=\"PlotSampleContribution3Signatures.pdf\", width=9, height=6)\nplotSamples(sigs_nmf, normalize=TRUE) + scale_y_continuous(breaks=seq(0, 1, 0.2), expand = c(0,0))+ theme(axis.text.x = element_text(size=6))\ndev.off()\n</code></pre> <p>If you don\u2019t have time to carry out the advanced questions you can exit R and return to the normal terminal command line.</p> <pre><code>quit()\nn\n</code></pre> <p> Open the resulting <code>PlotSampleContribution3Signatures.pdf</code>.  </p> <p>This shows the results for the mutation grouping for each sample. The samples are listed on the x-axis and the proportion of all mutations for that sample is shown on the y-axis. The colours of the bars indicate what proportion of the mutations for that sample were grouped into each of the signatures. The colour that makes up most of the bar for each sample is called its \u201cmajor signature\u201d.</p> <p>The data you have been using contains samples from High Grade Serous Ovarian Carcinomas and Cutaneous Melanoma. </p>  <p>Question</p> <p>Using the major signature found for each sample can you guess which are ovarian and which are melanoma samples?</p>   Answer <ul> <li> <p>Samples 9-12 have the majority signature of our signature 1. This is     the UV signature and so these are likely to be Melanoma samples.</p> </li> <li> <p>Samples 4-8 have the majority signature of our signature 3. This is     the BRCA signature and these are most likely to be ovarian samples.</p> </li> <li> <p>Samples 1-3 have the majority signature of our signature 2. This is     the APOBEC signature indicating activity of the anti-viral APOBEC     enzymes. These are less likely to be from cutaneous melanoma because     they have very few UV associated mutations although it could     possibly be from a different subtype. However it is much more likely     that these will be ovarian tumours as this APOBEC signature has been     seen in breast tumours which can be similar to ovarian cancers in     terms of the mutated genes.</p> </li> </ul>     <p>Question</p> <p>This is an open question for discussion at the end of the practical.</p> <p>How can this analysis be useful for cancer genomics studies?</p>   <p>Advanced exercise</p> <p>Now rerun the process this time using 4 signatures as the solution.</p>   Hint <p>You don\u2019t have to start back at the beginning but you can jump to the step where you run the NMF but this time for 4 instead of 3 signatures. Then continue through making the plots. You will need to change the name of each plot you remake with 4 signatures because Cairo won\u2019t let you overwrite and existing file.</p>   <p>Can you find a good match in the set of known signatures for all 4 patterns?</p> <p>Can you find a verified process for all of the profiles you are seeing?</p>"},{"location":"modules/cancer-module-somatic/01_signatures/#references","title":"References","text":"<p>Alexandrov et al. Nature 2013:  http://www.nature.com/nature/journal/v500/n7463/pdf/nature12477.pdf</p> <p>Gehring et al. Bioinformatics 2015:  http://bioinformatics.oxfordjournals.org/content/early/2015/07/31/bioinformatics.btv408.full</p>"},{"location":"modules/cancer-module-somatic/02_intogen/","title":"Cohort analysis for the identification of driver genes","text":""},{"location":"modules/cancer-module-somatic/02_intogen/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Run the <code>IntOGen</code> analysis software on cohort mutation data.</p> </li> <li> <p>Have gained experience of the structure of the analysis output files     in order to identify potential driver genes.</p> </li> <li> <p>Have gained overview knowledge of different methods for     identification of genes important in cancers.</p> </li> </ul>"},{"location":"modules/cancer-module-somatic/02_intogen/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-somatic/02_intogen/#tools-used","title":"Tools Used","text":"<p>IntOGen mutations platform: https://www.intogen.org/search</p>"},{"location":"modules/cancer-module-somatic/02_intogen/#sources-of-data","title":"Sources of Data","text":"<p>TCGA melanoma somatic SNV data from 338 tumour samples: https://tcga-data.nci.nih.gov/tcga/</p>"},{"location":"modules/cancer-module-somatic/02_intogen/#useful-links","title":"Useful Links","text":"<p>Mutation Annotation Format (MAF) specification: https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification</p> <p>IntOGen installation instructions: https://bitbucket.org/intogen/intogen-pipeline/overview</p>"},{"location":"modules/cancer-module-somatic/02_intogen/#author-information","title":"Author Information","text":"<p>Primary Author(s): Ann-Marie Patch, QIMR Berghofer ann-marie.patch@qimrberghofer.edu.au Erdahl Teber, CMRI eteber@cmri.org.au </p> <p>Contributor(s): Scott Wood scott.wood@qimrberghofer.edu.au </p>"},{"location":"modules/cancer-module-somatic/02_intogen/#introduction","title":"Introduction","text":"<p>Cancer driver genes are commonly described as genes that when mutated directly affect the potential of a cell to become cancerous. They are important to a tumour cell as they confer a growth or survival advantage over the normal surrounding cells. The mutations in these driver genes are then clonally selected for as the population of tumour cells increases. We think of the key genes driving tumour initiation (development), progression, metastases, resistance and survival. Driver gene mutations are often described as \u201cearly\u201d events because they were key in turning a normally functioning and regulated cell into a dysregulated one. The logical assumption is that these key mutations will be present in all tumour cells in a patient\u2019s sample; although sometime this is not true.</p> <p>There are two major research goals that underline the need to identify driver genes:</p> <ul> <li> <p>By identifying the early changes that take place researchers might     be able to find a treatment to stop the root cause of why cells     become malignant.</p> </li> <li> <p>By identifying groups of patients with the same genes mutated then     we can develop therapies that will work for all of them.</p> </li> </ul> <p>When we sequence tumour samples we tend to use samples that come from fully developed cancers that can carry hundreds to thousands of mutations in genes and many more outside of genes. The accumulation of these passenger mutations in cancer cells can happen because often the repair mechanisms or damage sensing processes are amongst the first pathways to become disrupted accelerating the mutational rate. Mutations that occur in genes after the cell has become cancerous may still affect the growth rate, invasiveness and even the response to chemotherapy but may not be present in all cells of a tumour. These genes may be drivers of chemo-resistance or metastasis and are equally good targets for therapies.</p> <p>IntOGen-mutations is a platform that aims to identify driver mutations using two methodologies from cancer cohort mutation data: the first identifies mutations that are most likely to have a functional impact combined with identifying genes that are frequently mutated; and the second, genes that harbour clustered mutations. These measures are all indicators of positive selection that occurs in cancer evolution and may help the identification of driver genes.</p>"},{"location":"modules/cancer-module-somatic/02_intogen/#analysing-cancer-cohort-data-with-intogen","title":"Analysing cancer cohort data with IntOGen","text":"<p>IntOGen-mutations is available as a web based service that can allow  users to run their analysis on the host\u2019s servers or it can be downloaded  and run on a local server.</p> <p>For the purposes of the course we will be using a local version of <code>IntOGen</code> so that we don\u2019t encounter any issues sharing resources.</p> <ul> <li> <p>To begin open a terminal and navigate to the directory <code>somatic/intogen</code>.</p> <pre><code>cd ~/somatic/intogen\n</code></pre> </li> </ul> <p>In this directory you will find a Mutation Annotation Format (MAF) file  containing a cut down version of the somatic variant calls identified from melanoma samples investigated as part of the TCGA cancer genomics projects. You can see what files are in the directory by typing <code>ls</code>, look inside the  file using <code>less TCGA_Melanoma_SMgene.maf</code> and close the file and return to the command line by typing <code>q</code>. </p> <ul> <li> <p>Run the <code>IntOGen</code> analysis by typing</p> <pre><code>intogen -i TCGA_Melanoma_SMgene.maf -o TCGA_Mela_out\n</code></pre> </li> </ul> <p>The TCGA melanoma maf used in this practical has been modified from the original to reduce processing time and only contains data for the top 680 mutated genes.</p> <p>The tool will take around 10 minutes to run and the progress will be indicated by the logging lines printed to the terminal. Once complete the output can be explored.</p> <p>Whilst the tool is running we can explore the options we have used to run <code>IntOGen</code>.</p> <ul> <li> <p>To get a list of <code>IntOGen</code> options open up a new terminal</p> <pre><code>intogen --help\n</code></pre> </li> </ul> <p>This command will list the running options that you can alter as command line inputs or in a configuration file. We are using the default options for this run so we didn\u2019t have to supply a configuration file and we only used <code>-i</code> to set the input and <code>-o</code> to control the mane of the output directory.</p> <ul> <li> <p>To look at the default options open up the configuration file by typing</p> <pre><code>less ~./intogen/task.conf\nless ~./intogen/system.conf\n</code></pre> </li> </ul> <p>It is important to set the correct genome assembly in the <code>task.conf</code> to match the one that you used as your reference when the variant were called. In our <code>task.conf</code> this should be <code>hg19</code>. </p>"},{"location":"modules/cancer-module-somatic/02_intogen/#exploring-the-output-of-intogen","title":"Exploring the output of IntOGen","text":"<p>When you run your data over the web on the remote site there is a browse facility that allows you to explore your data using the web version of the database. Running <code>IntOGen</code> locally provides the same tabular information but in a flat file format. </p> <p>There should be 14 files generated from a successful run of this version of <code>IntOGen</code>:</p> <pre><code>gene.tsv\ngene.oncodriveclust\npathway.recurrences\ngene.oncodrivefm\nsample_gene.impact\ngene.recurrences                                \nsample_variant+transcript.impact\nsummary.tsv\ntranscript.recurrences\nTCGA_Melanoma_slimSMgene.smconfig\noncodrivefm-pathways-MA_SCORE.tsv\noncodrivefm-pathways-PPH2_SCORE.tsv  \noncodrivefm-pathways-SIFT_SCORE.tsv  \npathway.oncodrivefm\n</code></pre> <p>View these files by using <code>ls</code> as below.</p> <pre><code>```bash\nls ~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/\n```\n</code></pre> <p>This practical will concentrate on the identification of driver genes so  we will look at the main output concerning genes. The <code>gene.tsv</code> is the main gene centric output summary table.</p> <ul> <li> <p>Open up the <code>gene.tsv</code> file in <code>LibreOffice</code> by double clicking on the icon on your desktop. </p> </li> <li> <p>Select the file tab and click on open. </p> </li> <li> <p>Navigate to the results directory  <code>~/somatic/intogen/TCGA_Mela_out/project/TCGA_Melanoma_slimSMgene/</code> </p> </li> <li> <p>Double click on <code>gene.tsv</code>. </p> </li> <li> <p>In the pop-up box under the <code>Separator options</code> ensure only the tab box is  checked and click <code>OK</code>.</p> </li> </ul> <p>This file contains the overall summary results for the <code>IntOGen</code> pipeline presented by gene and reports Q values (i.e. multiple testing corrected P values) for the mutation frequency and cluster modules.</p> <p>Significantly mutated genes from the cohort data are identified using both the <code>OncodriveFM</code> and <code>OncodriveClUST</code> modules of <code>IntOGen</code>. The <code>OncodriveFM</code>  module detects genes that have accumulated mutations with a high functional  impact. It uses annotations from the Ensembl variant effect predictor (VEP, V.70)  that includes SIFT and Polyphen2 and precomputed MutationAssessor functional  impacts. It calculates a P value per gene from the number of mutations detected across all possible coding bases of a gene with a positive weighting for mutations with a high functional impact. The <code>OncodriveCLUST</code> module detects genes that  have more variants than would be expected across the cohort that alter the  same region of the gene.</p> <p>The file is sorted to bring the most significantly altered genes to the top. The key columns that help you identify the significantly mutated genes are the 3rd and 4th  (C and D) that indicate which of the modules identified a significant result and  the Q-values for the modules that are in 21st and 23rd (U and W)</p> <p>The top twelve genes have significant Q-values for both modules and include BRAF,  NRAS and TP53. The next 35 are significant by only one of the modules.</p> <p>All of these have small Q-values which means they are all significantly mutated genes in this TCGA Melanoma cohort of 338 patients.</p> <ul> <li>Now look at their sample frequency count (column 9 <code>MUTS_CS_SAMPLES</code>) these are the number of samples that contain at least one mutation in the gene.</li> </ul>  <p>Question</p> <p>a)   Which significantly mutated gene has mutations in the most samples?</p> <p>b)   Which gene/genes have the lowest Q-value from OncodriveFM and OncodriveCLUST?</p> <p>c)   Why don\u2019t the genes with the lowest Q values also have the highest sample frequency value?</p>   Answer <p>a)  BRAF has 175 out of 327 cases with a mutation.</p> <p>b)  TP53 or PTEN have the lowest OncodriveFM Q-values and NRAS has the lowest Q-value for OncodriveCLUST.</p> <p>c)  The P value calculation takes into account the length of the     coding sequence of the gene, the mutation rate of the nucleotides     contained within it and for OncodriveFM the functional consequences     of those changes. Therefore a small gene with a small number of deleterious     mutations may have a lower P value and also Q value than a large     gene with a high mutation frequency.</p>    <p>The results for the assessment of clustered mutations in genes carried out by the <code>OncodriveCLUST</code> module of <code>IntOGen</code> are shown as  amino acid residue positions of the encoded protein.</p> <p>The three known oncogenes BRAF, NRAS and IDH1 have very low CLUST_QVALUEs indicating that the mutations in these genes are highly clustered. The  <code>CLUST_COORDS</code> column reports that there are 160 samples with mutations  between the amino acid positions 594-601 of BRAF; 84 samples with mutations at amino acid position 61 of NRAS; and 15 sample with mutations at amino  acid position 132 of IDH1.</p>  <p>Question</p> <p>Why are the oncogenes more likely to have clustered mutations and the tumour suppressor genes less likely?</p>   Answer <p>Gain of function mutations are required to activate oncogenes and so only key residues in the protein will result in activation. Tumour suppressors are frequently affected by loss of function mutations and deletions. A truncating mutation or frameshift indel can occur in any exon, except the last one, and have the same deleterious functional result.</p>    <p> The other files in the output support the information in this sheet.</p> <p>The <code>sample_variant+transcript.impact</code> file includes a summary of all mutations  found in each of the genes and protein coding transcripts of those genes for all  samples identified that have that mutation. It also reports the variant impact scores from SIFT, PolyPhen2, MutationAssesor, reporting also impact categories of which there are four; high, medium, low and none.</p> <ul> <li>Open up the <code>sample_variant+transcript.impact</code> file and explore the data.</li> </ul>  <p>Question</p> <p>Can you find out what the nucleotide change details for the most common BRAF mutation that results in V600E amino acid change in the cohort? Sort the data by GENE, then TRANSCRIPT and then PROTEIN_POS to make this easier. The gene ID for BRAF is <code>ENSG00000157764</code>.</p>   Answer <p>It is an A&gt;T at position chr7:140453136 identified in 127 samples.</p>"},{"location":"modules/cancer-module-somatic/02_intogen/#references","title":"References","text":"<p>Gunes et al. Nat. Methods 2010 :   http://www.nature.com/nmeth/journal/v7/n2/pdf/nmeth0210-92.pdf</p> <p>Gonzalez-Perez et al. Nat. Methods 2013 :   http://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2642.pdf</p>"},{"location":"modules/cancer-module-sv/sv_tut/","title":"Structural Variant Analysis","text":""},{"location":"modules/cancer-module-sv/sv_tut/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>By the end of the structural variant (SV) detection practical course participants will:</p> <ul> <li> <p>Have been provided with key fundamentals on how paired-end mappings     and split-read/soft-clipped read patterns are used in detecting     deletions, tandem duplicates, inversions and translocations.</p> </li> <li> <p>Know what important quality control checks need to be evaluated     prior to structural variant calling.</p> </li> <li> <p>Have run <code>DELLY</code> on a subset of whole genome next generation     sequencing data pertaining to a single human tumour with a matched     normal control.</p> </li> <li> <p>Be able to filter high confidence SV predictions.</p> </li> <li> <p>Have gained basic knowledge to interpret the VCF output provided by     DELLY.</p> </li> <li> <p>Have used their understanding of distinct SV paired-end mapping and     soft-clipped read patterns to visually verify <code>DELLY</code> predicted SVs     using <code>IGV</code>.</p> </li> </ul>"},{"location":"modules/cancer-module-sv/sv_tut/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-sv/sv_tut/#tools-used","title":"Tools Used","text":"<p>DELLY: https://github.com/tobiasrausch/delly</p> <p>Samtools: http://sourceforge.net/projects/samtools/files/samtools/1.2</p> <p>Tabix: http://sourceforge.net/projects/samtools/files/tabix/tabix-0.2.6.tar.bz2</p> <p>Vcftools: https://vcftools.github.io/index.html</p> <p>Picard: https://github.com/broadinstitute/picard</p> <p>Python2.7.10: https://www.python.org/downloads/release/python-2710/</p> <p>PyVCF Banyan numpy: https://pypi.python.org/pypi</p>"},{"location":"modules/cancer-module-sv/sv_tut/#useful-links","title":"Useful Links","text":"<p>SAM Specification: http://samtools.sourceforge.net/SAM1.pdf</p> <p>Explain SAM Flags: http://picard.sourceforge.net/explain-flags.html</p>"},{"location":"modules/cancer-module-sv/sv_tut/#author-information","title":"Author Information","text":"<p>Primary Author(s):  Erdahl Teber eteber@cmri.org.au Ann-Marie Patch Ann-Marie.Patch@qimrberghofer.edu.au</p> <p>Contributor(s):  Sonika Tyagi sonika.tyagi@agrf.org.au</p>"},{"location":"modules/cancer-module-sv/sv_tut/#alignment-quality-control","title":"Alignment Quality Control","text":"<p>For structural variant calling several alignment quality control metrics should be evaluated. All paired-end mapping methods heavily rely on the insert size distribution. GC-content biases are important as it can impact read-depths. <code>DELLY</code> generates read-depth ratios between tumour and control samples. The percentage of mapped reads, singletons, duplicates and properly paired reads are additional metrics you should evaluate prior to any structural variant calling. These statistics vary largely by protocol and hence, it is usually best to compare multiple different sequencing runs using the same against each other to highlight outliers.</p> <p>It is recommended that Picard module commands <code>CollectInsertSizeMetrics</code> and <code>CollectGcBiasMetrics</code>, and <code>samtools flagstat</code> command be used.</p>"},{"location":"modules/cancer-module-sv/sv_tut/#prepare-the-environment","title":"Prepare the Environment","text":"<p>As a quick introduction we will do a structural variant analysis using a single immortal cancer cell line and its control genome (mortal parental cells). Total execution time to run the <code>DELLY</code> structural discovery calling program for a matched normal tumour pair will vary depending on the read coverage and the size of the genome. As a guide, it can take approximately 10 to 50 hours (translocation predictions taking the longest), for a matched normal tumour pair (each 40-50x coverage) running on 2 cpus on a server with sufficient RAM.</p> <p>The bam files we will be working on are a subset of the original WGS bam files, limited to specific chromosomal regions to speed up the analysis and to meet the time constraints for this practical.</p> <p>Firstly, we will use shell variables to help improve the readability of commands and streamline scripting. Each distinct variable will store a directory path to either, the input WGS bam files, hg19 reference, programs or output.</p> <p>Open the Terminal.</p> <p>First, go to the right folder, where the data are stored.</p> <pre><code>cd /home/trainee/sv\nls\nmkdir &lt;YourFirstName&gt;\ncd &lt;YourFirstName&gt;\n\nexport DS=/home/trainee/sv/data\nexport RF=/home/trainee/sv/reference_data\nexport SF=/home/trainee/sv/variantFiltering/somaticVariants\nexport BR=/home/trainee/snv/Applications/igv\nexport CZ=/home/trainee/sv/converter\n</code></pre>"},{"location":"modules/cancer-module-sv/sv_tut/#somatic-structural-variant-discovery-using-delly","title":"Somatic Structural Variant Discovery using DELLY","text":"<p>In order to generate putative somatic SVs it is crucial to account for germline SVs. To facilitate, DELLY requires the joint input of a match normal control and the cancer aligned sequencing data (bam files).</p> <pre><code>delly -t DEL -x $RF/hg19.excl -o del.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t DUP -x $RF/hg19.excl -o dup.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t INV -x $RF/hg19.excl -o inv.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t TRA -x $RF/hg19.excl -o tra.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>DEL: conduct deletion discovery DUP: conduct tandem duplication discovery INV: conduct inversion discovery TRA: conduct translocation discovery -o: vcf output -g: reference genome in FASTA format -x: genomic regions to exclude (e.g. centro- and telomeric regions)  </p>"},{"location":"modules/cancer-module-sv/sv_tut/#delly-vcf-output","title":"DELLY VCF output","text":"<p>A VCF file has multiple header lines starting with the hash <code>#</code> sign. Below the header lines is one record for each structural variant. The record format is described in the table below:</p>    Column Field Description     1 CHROM Chromosome name   2 POS 1-based position. For an indel, this is the position preceding the indel   3 ID Variant identifier   4 REF Reference sequence at POS involved in the variant   5 ALT Comma delimited list of alternative sequence(s)   6 QUAL Phred-scaled probability of all samples being homozygous reference   7 FILTER Semicolon delimited list of filters that the variant fails to pass   8 INFO Semicolon delimited list of variant information   9 FORMAT Colon delimited list of the format of sample genotypes in subsequent fields   10+  Individual genotype information defined by FORMAT    <p>You can look at the header of the vcf file using <code>grep -A 1</code> includes the first structural variant record in the file:</p> <pre><code>grep \"^#\" -A 1 del.vcf\n</code></pre> <p> The INFO field holds structural variant site information whereas all genotype information (annotated as per the FORMAT fields) is provided in the sample column. Reference supporting reads are compared to alternative supporting reads and mapping qualities are used to compute genotype likelihoods (GL) for homozygous reference (0/0), heterozygous reference (0/1) and homozygous alternate (1/1) (GT). The final genotype (GT) is simply derived from the best GL and GQ is a phred-scaled genotype quality reflecting the confidence in this genotype. If GQ&lt;15 the genotype is flagged as LowQual. The genotyping takes into account all paired-ends with a mapping quality greater than 20 by default.</p> <p>The INFO field provides information on the quality of the SV prediction and breakpoints. If you browse through the vcf file you will notice that a subset of the DELLY structural variant predictions have been refined using split-reads. These precise variants are flagged in the vcf info field with the tag <code>PRECISE</code>. To count the number of precise and imprecise variants you can simply use <code>grep</code>.</p> <pre><code>grep -c -w \"PRECISE\" *.vcf\ngrep -c -w \"IMPRECISE\" *.vcf\n</code></pre> <p> DELLY clusters abnormal paired-ends and every single cluster gives rise to an <code>IMPRECISE</code> SV call. For every <code>IMPRECISE</code> SV call an attempt is made to identify supporting split-reads/soft-clipped reads DELLY then computes a consensus sequence (INFO:CONSENSUS) out of all split-read candidates and then aligns this consensus sequence to the reference requiring at least -m many aligned bases to the left and right (default is 13). INFO:PE is the number of supporting paired-ends. INFO:CT refers connection types (CT), which indicates the order and orientation of paired-end cluster mappings (e.g. 3to3 for 3\u2019 to 3\u2019 and 5to5 for 5\u2019 to 5\u2019). Values can be 3to5, 5to3, 3to3 or 5to5. Different names exist for these connection types in the literature, head-to-head inversions, tail-to-tail inversions, and so on. The consensus alignment quality (SRQ) is a score between 0 and 1, where 1 indicates 100% identity to the reference. Nearby SNPs, InDels and micro-insertions at the breakpoint can lower this score but only for mis-assemblies it should be very poor. DELLY currently drops consensus alignments with a score &lt;0.8 and then falls back to an <code>IMPRECISE</code> prediction.</p> <p>SVs are flagged as FILTER:LowQual if PE &lt;3 OR MAPQ &lt;20 (for translocations: PE &lt;5 OR MAPQ &lt;20), otherwise, the SV results in a FILTER:PASS. <code>PRECISE</code> variants will have split-read support (SR &gt;0).</p>"},{"location":"modules/cancer-module-sv/sv_tut/#somatic-structural-variant-filtering","title":"Somatic Structural Variant Filtering","text":"<p>Please note that this vcf file contains germline and somatic structural variants but also false positives caused by repeat induced mis-mappings or incomplete reference sequences. As a final step we have to use the structural variant site information and the cancer and normal genotype information to filter a set of confident somatic structural variants. DELLY ships with a somatic filtering python script. For a set of confident somatic calls one could exclude all structural variants &lt;400bp, require a minimum variant allele frequency of 10%, no support in the matched normal and an overall confident structural variant site prediction with the VCF filter field being equal to PASS.</p> <pre><code>python $SF/somaticFilter.py -t DEL  -T cancer_cell_line -N control -v del.vcf -o del.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t DUP -T cancer_cell_line -N control -v dup.vcf -o dup.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t INV  -T cancer_cell_line -N control -v inv.vcf -o inv.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t TRA -T cancer_cell_line -N control -v tra.vcf -o tra.filt.vcf -a 0.1 -m 400 -f\n</code></pre> <p> Using <code>VCFtools</code> we can merge all somatic structural variants together in a single vcf file.</p> <pre><code>vcf-concat del.filt.vcf dup.filt.vcf inv.filt.vcf tra.filt.vcf | vcf-sort &gt; somatic.sv.vcf\n</code></pre> <p> For large VCF files you should also zip and index them using <code>bgzip</code> and <code>tabix</code>. Please run the below commands to meet the requirements for visualising somatic structural variants using <code>IGV</code>.</p> <pre><code>bgzip somatic.sv.vcf\ntabix somatic.sv.vcf.gz\n</code></pre>"},{"location":"modules/cancer-module-sv/sv_tut/#visualisation-of-somatic-structural-variants","title":"Visualisation of Somatic Structural Variants","text":"<p>The final step will be to browse some of these somatic structural variants in IGV and to visually verify the reliability of the calls. VCF file structure was designed by the 1000Genomes Project consortium with considerable focus being placed on single-nucleotide variants and InDels. It is arguable whether the file format is easy to interpret for basic SVs and much less for complex SV formatting. To make it easier to see the breakpoints we will create a bed file.</p> <pre><code>$CZ/sv.vcf2bed.sh somatic.sv.vcf.gz &gt; somatic.sv.bed\nhead somatic.sv.bed\n</code></pre> <p> Load the IGV browser</p> <pre><code>$BR/igv.sh\n</code></pre> <p> Once IGV has started use <code>File</code> and <code>Load from File</code> to load the <code>cancer_cell_line.bam</code>. To dedicate more of the screen to read pile ups, right click on the left hand box that contains the <code>cancer_cell_line.bam Junctions</code> then select <code>Remove Track</code>. Go to <code>View</code> menu and select <code>Preferences</code>, then click on the Alignments tab and in the <code>Visibility range threshold (kb)</code> text box, enter 600. This will allow you to increase your visibility of pile ups as you zoom out. Now look for check box for <code>Filter secondary alignments</code> click on to ensure we do not see secondary alignments (alternate mapped position of a read). Also ensure that <code>Show soft-clipped bases</code> has been checked then click <code>OK</code>.</p> <p>Then import <code>somatic.sv.bed</code> from your working directory using <code>Regions</code> and <code>Import Regions</code>.</p>"},{"location":"modules/cancer-module-sv/sv_tut/#verify-deletion","title":"Verify Deletion","text":"<p>This is an advanced section. </p>  <p>The somatic structural variants can then be browsed easily using the <code>Region Navigator</code>. Select the deletion (chrX:76853017-77014863) from the <code>Region Navigator</code> and click <code>View</code>. This will centre the IGV alignment view on the selected structural variant. Close the regions of interest pop up window. The red bar below the ruler marks the region of the deletion.</p> <p>It\u2019s usually best to zoom out once by clicking on the <code>-</code> sign in the toolbar at the top, to give a wide view of the supporting abnormal paired-end read mappings.</p> <p>To highlight the abnormal paired-ends right click on the main track display panel and select <code>Color alignments by</code> and then switch to <code>insert size and pair orientation</code>.</p> <p>Read pairs that have a larger than expected insert size will be highlighted in red. Click <code>View as pairs</code>. Right click and <code>Sort alignments by</code> then select <code>start location</code>.</p>   <p>Question</p> <p>How many abnormal paired-end read pairs (red coloured F/R oriented read pairs) can you see that spans the deletion region? Does this number coincide with the INFO:PE?</p>   Answer <p>19, YES</p>     <p>Question</p> <p>Zoom into left breakpoint and tally the number of soft-clipped reads at both junctions. How many abnormal split-reads (soft-clipped reads) did you observe? Clue INFO:SR.</p>   Answer <p>11</p>     <p>Question</p> <p>Go to the RefSeq genes track at the bottom of <code>IGV</code> and right click to <code>Expanded</code>. Is the predicted deletion likely to have a deleterious impact on a gene? If so, what gene and exons are deleted?</p>   Answer <p>ATRX, exons 2 to 25.</p>     <p>Question</p> <p>Does this region appear to be completely removed from this cancer genome? How can you tell?</p>   Answer <p>Yes, there is no read coverage within this deletion region relative to the control genome.</p>"},{"location":"modules/cancer-module-sv/sv_tut/#verify-translocation","title":"Verify translocation","text":"<p>This is an advanced section.</p>  <ul> <li> <p>Select the duplication (chrX:45649874-45689322) from the <code>Region Navigator</code>.</p> </li> <li> <p>Highlight the abnormal paired-ends by clicking and selecting <code>Color alignments by</code> and then switch to <code>insert size and pair orientation</code>. Also, invoke <code>Sort alignments by</code> then select <code>start location</code>.</p> </li> <li> <p>Zoom out until you can see all the purple reads at the junction.</p> </li> </ul>   <p>Question</p> <p>What is the direction of the purple cluster of reads (indicates that mate reads are mapped to Chr15)? Is it pointing to the tail or head of Chr18?</p>   Answer <p>Forward, towards the tail, or 3\u2019 (+ive)</p>     <p>Question</p> <p>Right click on to one of the purple coloured reads and select <code>View mate region in split-screen</code>. This will split the screen and display Chr15 on the left and place a red highlighted outline on both reads, to indicate the pairs. Right click and <code>Sort alignments by</code> then select <code>start location</code>.</p> <p>What is the direction of the yellow cluster of reads (indicates that mate reads are mapped to Chr18)? Is it pointing to the tail or head of Chr15? If you wish to zoom in or out, first click inside of the chromosome ideogram panel, then ctrl- to zoom out and shift+ to zoom in.</p>   Answer <p>Reverse, towards the head, or 5\u2019 (+ive)</p>     <p>Question</p> <p>How is the Chr15 and Chr18 fused (which one of the four translocation connection types)? If you are uncertain then run a BLAT (https://genome.ucsc.edu/cgi-bin/hgBlat?command=start) search using the INFO:CONSENSUS sequence (gedit <code>somatic.sv.vcf</code>).</p>   Answer <p>RF, head to tail, or 5 to 3. Therefore, Chr18 left side is fused to Chr15 right side.</p>     <p>Question</p> <p>Did DELLY predict a reciprocal translocation? How can you tell?</p>   Answer <p>No, as we would expect to observe a Chr18 right side fused to Chr15 left side, near the same breakpoints.</p>     <p>Question</p> <p>What gene structures is this translocation predicted to impact?</p>   Answer <p>PARD6G on Chr15 and ADAMTSL3 on Chr18.</p>     <p>Question</p> <p>What is one possible reason why there is no observable read coverage after Chr18 breakpoint?</p>   Answer <p>Chromosome loss.</p>"},{"location":"modules/cancer-module-sv/sv_tut/#verify-tandem-duplication","title":"Verify tandem duplication","text":"<p>This is an advanced section.</p>  <ul> <li> <p>Select the tandem duplication (chrX:45649874-45689322) from the <code>Region Navigator</code>.</p> </li> <li> <p>Highlight the abnormal paired-ends by clicking and selecting <code>Color alignments by</code> and then switch to <code>insert size and pair orientation</code>. Also, invoke <code>Sort alignments by</code> then select <code>start location</code>.</p> </li> <li> <p>Zoom out until you can see all the red paired-end reads spanning the two junctions. After that zoom in on the cluster of abnormal reads on the left junction and then right junction.</p> </li> </ul>   <p>Question</p> <p>Which is the order and orientation of these paired-end reads (FR, RF, FF or RR)?</p>   Answer <p>RF</p>     <p>Question</p> <p>What is the estimated read-depth ratio of the cancer_cell_line versus normal control (INFO:RDRATIO) over the duplicate region?</p>   Answer <p>3.3 (\u00a03 x increased read depth)</p>"},{"location":"modules/cancer-module-sv/sv_tut/#verify-inversion","title":"Verify Inversion","text":"<p>This is an advanced section.</p>  <ul> <li> <p>Type into the search box near at tool bar, Chr20:54834492</p> </li> <li> <p>Right click on main display and select <code>Group alignments by</code> then switch on <code>paired-orientation</code>. Also, right click and <code>Sort alignments by</code> then select <code>start location</code>.</p> </li> <li> <p>Zoom out until you can see all the red coloured cluster of reads near the breakpoint.</p> </li> <li> <p>Right click on to one of the red coloured reads and select <code>View mate region in split-screen</code>. This will split the screen and display the read mate on the right side. This will take you to the mate-reads near the second breakpoint.</p> </li> </ul>   <p>Question</p> <p>Which direction are the paired-end reads spanning (left or right spanning)?</p>   Answer <p>Right</p>     <p>Question</p> <p>What is the estimated size of the inverted interval?</p>   Answer <p>55,408,660 \u2013 54,834,492 = 574,168 bp</p>"},{"location":"modules/cancer-module-sv/sv_tut/#acknowledgements","title":"Acknowledgements","text":"<p>We would like to thank and acknowledge Tobias Rausch (EMBL Heidelberg) for his help and for allowing us to borrow and adapt his replies to questions and original course material.</p>"},{"location":"modules/cancer-module-viz/visu/","title":"Variant Visualisation","text":"<p>This module was written by Mathieu Bourgey and the original on-line version is available here.</p>"},{"location":"modules/cancer-module-viz/visu/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li>Generate Circos like graphics using R</li> </ul>"},{"location":"modules/cancer-module-viz/visu/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/cancer-module-viz/visu/#tools-used","title":"Tools Used","text":"<p>R: https://cran.r-project.org/</p> <p>R package circlize: https://cran.r-project.org/web/packages/circlize/index.html</p>"},{"location":"modules/cancer-module-viz/visu/#author-information","title":"Author Information","text":"<p>Primary Author(s): Mathieu Bourgey mathieu.bourgey@mcgill.ca </p> <p>Contributor(s): </p>"},{"location":"modules/cancer-module-viz/visu/#introduction","title":"Introduction","text":"<p>This short workshop will show you how to visualize your data.</p> <p>We will be working on 3 types of somatic calls:</p> <ul> <li> <p>SNV calls from MuTect (vcf)</p> </li> <li> <p>SV calls from DELLY (vcf)</p> </li> <li> <p>CNV calls from SCoNEs (tsv)</p> </li> </ul>"},{"location":"modules/cancer-module-viz/visu/#prepare-the-environment","title":"Prepare the Environment","text":"<p>We will use a dataset derived from the analysis of whole genome sequencing paired normal/tumour samples.</p> <p>The call files are contained in the folder <code>visualization</code>:</p> <ul> <li><code>mutect.somatic.vcf</code> </li> <li><code>delly.somatic.vcf</code> </li> <li><code>scones.somatic.tsv</code> </li> </ul> <p>Many tools are available to do this and the most commonly known is Circos. Circos is a really not user-friendly. In this tutorial, we show you an easy alternative to build a circular representation of genomic data.</p> <p>First we need to go in the folder <code>visualization</code> to do the analysis:</p> <pre><code>cd /home/trainee/visualization/\n</code></pre> <p>Let\u2019s see what is in this folder:</p> <pre><code>ls\n</code></pre> <p><code>circos.R  delly.somatic.vcf  mutect.somatic.vcf  scones.somatic.tsv</code></p> <p>Take a look at the data files.</p> <p>This data has not been restricted to a short piece of a chromosome and SNVs have already been filtered:</p> <p><pre><code>less mutect.somatic.tsv\nless delly.somatic.vcf\nless data/scones.somatic.30k.tsv\n</code></pre> </p>  <p>Question</p> <p>What can you see from this data?</p>   Answer <ul> <li> <p>the filtered output of 3 different software: Mutect (SNVs), Delly (SVs), SCoNEs (CNVs).</p> </li> <li> <p>The 3 files show 2 different formats (vcf, tsv).</p> </li> <li> <p>Almost all type of variants are represented here: mutations, deletion, inversion, translocation, large amplification and deletion (CNVs).</p> </li> </ul>     <p>Question</p> <p>Why don\u2019t we use the vcf format for all types of calls?</p>   Answer <p>The 1000 Genomes project tries to use/include SV calls in the vcf format. Some tools like Delly use this format for SVs. It is a good idea to try to include everything together but this not the be the best way to handle SVs and CNVs.</p>     <p>Question</p> <p>Why?</p>   Answer <p>Due to the nature of these calls, you can not easily integrate the positional information of the two breakpoints (that could be located far away or in an other chromosome) using a single position format.</p>     <p>The analysis will be done using the R program:</p> <pre><code>R\n</code></pre> <p>We will use the circlize package from the cran R project. This package generates circular plots and has the advantage of being able to provide pre-built functions for genomic data. One of the main advantages of this tool is the use of bed format as input data.</p> <pre><code>library(circlize)\n</code></pre> <p> Let\u2019s import the variants:</p> <pre><code>snp=read.table(\"mutect.somatic.vcf\")\nsv=read.table(\"somatic.sv.vcf\")\ncnv=read.table(\"data/scones.somatic.tsv\",header=T)\n</code></pre> <p> We need to set up the generic graphical parameters:</p> <pre><code>par(mar = c(1, 1, 1, 1))\ncircos.par(\"start.degree\" = 90)\ncircos.par(\"track.height\" = 0.05)\ncircos.par(\"canvas.xlim\" = c(-1.3, 1.3), \"canvas.ylim\" = c(-1.3, 1.3))\n</code></pre> <p> Let\u2019s draw hg19 reference ideograms:</p> <pre><code>circos.initializeWithIdeogram(species = \"hg19\")\n</code></pre> <p> Unfortunately circlize does not support hg38 yet. So we will need to reformat our data to fit the hg19 standards. As we are working only on autosomes, we won\u2019t need to lift-over and we could simply add chr at the beginning of the chromosome names.</p> <p>We can now draw 1 track for somatic mutations:</p> <pre><code>snv_tmp=read.table(\"data/mutec.somatic.vcf\",comment.char=\"#\")\nsnv=cbind(paste(\"chr\",as.character(snp[,1]),sep=\"\"),snp[2],snp[,2]+1)\ncircos.genomicTrackPlotRegion(snv,stack=TRUE, panel.fun = function(region, value, ...) {\n    circos.genomicPoints(region, value, cex = 0.05, pch = 9,col='orange' , ...)\n})\n</code></pre> <p> Let\u2019s draw the 2 tracks for CNVs. One track for duplications in red and one blue track for deletions.</p> <pre><code>dup=cnv[cnv[,5]&gt;2,]\ndup[,1]=paste(\"chr\",as.character(dup[,1]),sep=\"\")\ndel=cnv[cnv[,5]&lt;2,]\ndel[,1]=paste(\"chr\",as.character(del[,1]),sep=\"\")\ncircos.genomicTrackPlotRegion(dup, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col = \"red\",bg.border = NA, cex=1 , ...)\n})\ncircos.genomicTrackPlotRegion(del, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col = \"blue\",bg.border = NA, cex=1 , ...)\n})\n</code></pre> <p>We can clearly see a massive deletion in chromosome 3.</p> <p> To finish we just need to draw 3 tracks + positional links to represent SVs.</p> <p>Unfortunately the vcf format has not been designed for SVs. SVs are defined by 2 breakpoints and the vcf format stores the second one in the info field. So we will need to extract this information to draw these calls.</p> <pre><code>chrEnd=NULL\nposEnd=NULL\nfor (i in 1:dim(sv)[1]) {\n    addInfo=strsplit(as.character(sv[i,8]),split=\";\")\n    chrInf=strsplit(addInfo[[1]][3],split=\"=\")\n    chrEnd=c(chrEnd,chrInf[[1]][2])\n    posInf=strsplit(addInfo[[1]][4],split=\"=\")\n    posEnd=c(posEnd,posInf[[1]][2])\n}\nsvTable=data.frame(paste(\"chr\",sv[,1],sep=\"\"),as.numeric(sv[,2]),as.numeric(posEnd),paste(\"chr\",chrEnd,sep=\"\"),as.character(sv[,5]))\n</code></pre> <p> Now that we have reformatted the SV calls, let\u2019s draw them.</p> <pre><code>typeE=c(\"&lt;DEL&gt;\",\"&lt;INS&gt;\",\"&lt;INV&gt;\")\ncolE=c(\"blue\",\"black\",\"green\")\nfor (i in 1:3) {\n        bed_list=svTable[svTable[,5]==typeE[i],]\n        circos.genomicTrackPlotRegion(bed_list,stack=TRUE, panel.fun = function(region, value, ...) {\n                circos.genomicPoints(region, value, cex = 0.5, pch = 16, col = colE[i], ...)\n        })\n}\n\nbed1=cbind(svTable[svTable[,5]==\"&lt;TRA&gt;\",1:2],svTable[svTable[,5]==\"&lt;TRA&gt;\",2]+5)\nbed2=cbind(svTable[svTable[,5]==\"&lt;TRA&gt;\",c(4,3)],svTable[svTable[,5]==\"&lt;TRA&gt;\",3]+5)\n\nfor (i in 1:dim(bed1)[1]) {\n    circos.link(bed1[i,1],bed1[i,2],bed2[i,1],bed2[i,2])\n}\n</code></pre> <p> A good graph needs a title and legend:</p> <pre><code>title(\"Somatic calls (SNV - SV - CNV)\")\nlegend(0.7,1.4,legend=c(\"SNV\", \"CNV-DUPLICATION\",\"CNV-DELETION\",\"SV-DELETION\",\"SV-INSERTION\",\"SV-INVERSION\"),col=c(\"orange\",\"red\",\"blue\",\"blue\",\"black\",\"green\",\"red\"),pch=c(16,15,15,16,16,16,16,16),cex=0.75,title=\"Tracks:\",bty='n')\nlegend(0.6,0.95,legend=\"SV-TRANSLOCATION\",col=\"black\",lty=1,cex=0.75,lwd=1.2,bty='n')\n</code></pre> <p> You should obtain a plot like this one:</p>    <p>Question</p> <p>Could you generate the graph and save it into a pdf file?</p>   Answer <pre><code>circos.clear()\npdf(\"circos.pdf\")\n...\ndev.off()\n</code></pre>    <p>Finally exit R   <pre><code>q(\"yes\")\n</code></pre></p>"},{"location":"modules/cancer-module-viz/visu/#acknowledgements","title":"Acknowledgements","text":"<p>Mathieu Bourgey would like to thank and acknowledge Louis Letourneau for this help and for sharing his material. The format of the tutorial has been inspired from Mar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis Letrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume Bourque for the help in building these pipelines and working with all the various datasets.</p>"},{"location":"modules/cancer-module-viz/visu/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. This means that you are able to copy, share and modify the work, as long as the result is distributed under the same license.</p>"},{"location":"modules/denovo-module-bac/bac_asm/","title":"Bacterial Assembly","text":""},{"location":"modules/denovo-module-bac/bac_asm/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Perform a simple genome assembly for a small organism using <code>Velvet</code></p> </li> <li> <p>Visualise the assembly graph using <code>Bandage</code></p> </li> <li> <p>Be aware of the effects and trade-offs of the parameter <code>K</code> on the     genome assembly</p> </li> <li> <p>Understand that genome assembly produces a graph structure</p> </li> </ul>"},{"location":"modules/denovo-module-bac/bac_asm/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/denovo-module-bac/bac_asm/#tools-used","title":"Tools Used","text":"<p>Velvet: https://www.ebi.ac.uk/~zerbino/velvet/</p> <p>Bandage: https://rrwick.github.io/Bandage/</p>"},{"location":"modules/denovo-module-bac/bac_asm/#data-source-and-useful-links","title":"Data Source and Useful Links","text":"<p>Data used in this tutorial is from ENA accession SRR2054105:   http://www.ebi.ac.uk/ena/data/view/SRR2054105</p> <p>Spreadsheet used in the group activity: https://docs.google.com/spreadsheets/d/1iFbCCihawpY1LClsAB-OJ66lyeW7EsdJyaMo1HCetF8/edit?usp=sharing</p>"},{"location":"modules/denovo-module-bac/bac_asm/#author-information","title":"Author Information","text":"<p>Primary Author(s): Torsten Seemann tseemann@unimelb.edu.au Paul Berkman Paul.Berkman@csiro.au Philippe Moncuquet Philippe.Moncuquet@csiro.au </p> <p>Contributor(s): Zhiliang Chen zhiliang.chen@unsw.edu.au Sonika Tyagi Sonika.Tyagi@agrf.org.au </p>"},{"location":"modules/denovo-module-bac/bac_asm/#introduction","title":"Introduction","text":"<p>In this tutorial we will take raw sequencing reads and de novo assemble them into contigs. We will also explore the internal assembly graph structure to aid our understanding of why assemblies are incomplete.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#before-the-assembly","title":"Before the assembly","text":""},{"location":"modules/denovo-module-bac/bac_asm/#sequence-data","title":"Sequence data","text":"<p>We have sequenced the genome of a bacterium using paired-end chemistry on an Illumina HiSeq 2000 instrument at the University of Oxford and publicly available as SRR2054105 the Europen Nucleotide Archive (ENA).</p> <p>Let\u2019s have a look at the datasets by doing:</p> <pre><code> cd /home/trainee/bacterial\n ls\n</code></pre> <p>You should see the following two files:</p> <pre><code>R1.fastq.gz\nR2.fastq.gz\n</code></pre>  <p>Questions</p> <p>How many reads are in this data set?</p>   Answer <p>2100778 pairs</p>   <p>What is the yield in basepairs?</p>   Answer <p>210077800 bp = 210 Mbp</p>   <p>Assuming an average bacterial genome size of 4 Mbp, what depth of coverage do we have?</p>   Answer <p>210 / 4 = 53x</p>"},{"location":"modules/denovo-module-bac/bac_asm/#trim-or-clip-reads","title":"Trim or clip reads","text":"<p>There are two distinct reasons one may wish to trim or clip the raw reads.</p> <ol> <li> <p>Low quality bases typically occur toward the end of Illumina reads.     The lower the quality score, the higher the chance that the base is     incorrect. This introduces false k-mers into the assembly process. A     good assembler should handle these gracefully.</p> </li> <li> <p>Sequencing adaptors are artificial sequence that can occur at the     end of reads that came from fragments of DNA that were shorter than     desired. They were not in the original genome and their presence in     lots of reads will totally confuse the assembler.</p> </li> </ol> <p>Normally one would use <code>fastqc</code> or similar to determine whether quality trimming is warranted and for the presence of sequencing adaptors. The data set in this exercise is 99% free of adaptors and is good quality overall so we will skip those steps.</p>  <p>Questions</p> <p>How do you know which adaptor sequence to trim?</p>   Answer <p>You either ask the sequencing provider or use a tool that tries all known adapters.</p>   <p>How are are quality values in <code>FASTQ</code> files calculated? Can we trust them?</p>   Answer <p>The are calculated using formulas calibrated to the physical measurements the sequencing instrument takes. In the case of Illumina sequencing, these measurements will include pixel light intensities from the images the camera/scanner takes of the flow cell.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#allocate-yourself-to-a-k-value-on-the-spreadsheet","title":"Allocate yourself to a <code>K</code> value on the spreadsheet","text":"<p>An important parameter to most genome assemblers is <code>K</code> which is the k-mer size used to construct the de Bruijn graph (pronounced de Brown).</p> <p>Please go to this shared online spreadsheet and choose a K value and put your name next to it.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#assemble-the-reads-using-velvet","title":"Assemble the reads using Velvet","text":"<p>The <code>Velvet</code> software performs the assembly in two steps. The first <code>velveth</code> step hashes the reads. The second <code>velvetg</code> step builds, cleans and traverses the graph.</p> <p>Choose an output folder <code>DIR</code> and use the <code>K</code> value you were allocated on the spreadsheet and assemble the reads.</p> <pre><code>velveth DIR K -shortPaired -fastq.gz -separate R1.fastq.gz R2.fastq.gz\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>DIR: Directory name you chose to write the output to K: K-mer value; it must be an odd number -shortPaired: The read types are short and paired-end -fastq.gz: The format of the input files -separate: R1 and R2 reads are in separate files input file names: input FASTQ file names  </p>  <p>Run the velvetg step now:</p> <pre><code>/usr/bin/time -f \"%e\" velvetg DIR -exp_cov auto -cov_cutoff auto\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>time: to capture the <code>velvetg</code> command run time DIR: Directory name you chose to write the velvet output to -exp_cov: expected coverage is set to <code>auto</code>. To be determined by <code>velvet</code> -cov_cutoff: coverage cutoff is set to <code>auto</code>. To be determined by <code>velvet</code> </p>"},{"location":"modules/denovo-module-bac/bac_asm/#add-your-results-to-the-spreadsheet","title":"Add your results to the spreadsheet","text":"Column Where to find the value     K-mer size You chose this earlier and used it in the velveth command   How long it took to run Look at the final output line for the user time in seconds: NN.N   Average K-mer coverage Look for Estimated Coverage = NN.N   Number of contigs Look for text Final graph has NNN nodes   N50 contig size Look for n50 of NNNNN   Largest contig size Look for max NNNNN   Total number of basepairs (sum of contig lengths) Look for total NNNNNNN"},{"location":"modules/denovo-module-bac/bac_asm/#effect-of-k-on-assembly","title":"Effect of K on assembly","text":"<p>Examine the spreadsheet and look for patterns in the tabular data and corresponding bar/line charts.</p> <p>How does K affect each of the statistics? Which value of K do you think is doing the best job? Why?</p>"},{"location":"modules/denovo-module-bac/bac_asm/#output-files","title":"Output Files","text":"<p>Table: Output Files</p>    Filename Description     contigs.fa This is a FASTA file with your contigs   Log Has most of the metrics in it that we recorded   stats.txt TSV file of length and coverage of individual contigs   Sequences A copy of the input FASTQ sequences in FASTA format   Pregraph Roadmaps Graph2 Interim assembly graph structure   LastGraph Final graph structure    <p>Let\u2019s examine the <code>stats.txt</code> file and look at the <code>short1_cov</code> column which is the k-mer coverage of each contig:</p> <pre><code>cut -f6 dir/stats.txt | less\n</code></pre> <p>Press <code>SPACE</code> and <code>b</code> to scroll up and down, and press <code>q</code> to quote.</p>  <p>Questions</p> <p>What do you notice about the k-mer coverages?</p>   Answer <p>They all appear to have a similar value, but there are some that do not fit the pattern.</p>   <p>What do the outliers correspond to?</p>   Answer <p>Repetitive elements of the genome including gene duplications. Also  replicons with differing copy numbers, like bacterial plasmids.</p>    <pre><code>grep NN dir/contigs.fa\n</code></pre>  <p>Question</p> <p>Why is there N letters in the assembly?</p>   Answer <p>Paired-end reads come from each end of the same original fragment of DNA. The unsequenced gap in the middle is variable, and unknown. The assembler sometimes struggles to resolve these gaps. So it knows two sections of the genome are connected, but it doesn\u2019t quite know what is between them. So it pads them with its best guess of how many bases there are, and uses N to denote them as unknown.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#visualising-the-assembly-graph-using-bandage","title":"Visualising the assembly graph using Bandage","text":"<p>The final graph that <code>Velvet</code> uses is stored in the file <code>LastGraph</code>. The <code>Bandage</code> software allows us to view and explore the assembly graph.</p> <ol> <li> <p>Start <code>Bandage</code>.</p> </li> <li> <p>Go to <code>File -&gt; Load Graph</code> and load the <code>LastGraph</code> from your     assembly in <code>DIR</code>. This may take a little while so be patient.</p> </li> <li> <p>Maximize your window to fill up the whole screen.</p> </li> <li> <p>Click <code>Draw graph</code> on the left hand panel.</p> </li> <li> <p>Change <code>Random colours</code> to <code>Colour by read depth</code> on the left hand     panel.</p> </li> </ol> <p>Now get up out of your chair and walks around and look at all the different graphs the other participants obtained with different values of <code>K</code>.</p>  <p>Questions</p> <p>How does <code>K</code> affect the graph?</p>   Answer <p>Small K produces higher k-mer coverage but also more connectivity as smaller K is more ambiguous.</p>   <p>What would the graph look like in an ideal situtation?</p>   Answer <p>If the genome had M replicons, we would like to see only M sub-graphs. Each sub-graph would be linear or circular, depending on the form of the replicon in the original organism.</p>   <p>Why didn\u2019t anyone achieve perfection?</p>   Answer <p>Short reads are unable to disambiguate repeats longer than the read length (or read span). Most genomes have repeats beyond 500 bp.</p>    <p>Here is another example from the Bandage web site on how K can affect assembly graphs.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#explore-the-graph","title":"Explore the graph","text":"<p><code>Bandage</code> is designed to be an interactive tool. It allows you to see relationships between parts of your genome that are lost when you only look at the FASTA file of contigs.</p> <ul> <li> <p>Zoom in using the <code>Zoom</code> up/down button in the left hand panel</p> </li> <li> <p>Pan around by holding down the Option/Windows key and dragging on     the background</p> </li> <li> <p>Move nodes out of the way by selecting and dragging</p> </li> </ul> <p>Feel free to play around a bit and ask questions.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#features-of-the-assembly-graph","title":"Features of the assembly graph","text":"<p>The graph is quite tangled. The long stretches correspond to contigs. The intersections correspond to shared k-mers in the graph, which occur due to repeated sequences within the genome.</p> <p>Select an node (rectangle) in the graph. It\u2019s length is reported in the right hand panel as <code>Length: NNN</code>.</p> <p>On the menu choose <code>Output -&gt; Web BLAST selected node</code>. Your browser should open the NCBI BLAST Website.</p> <p>Click the <code>BLAST</code> button at the bottom, and wait for the result.</p>  <p>Questions</p> <p>Did your node match anything in the Genbank database?</p>   Answer <p>You will need to examine the BLAST report on the trainee browser window.</p>   <p>Can you determine what species of bacteria was sequenced?</p>   Answer <p>The top hit of the small segment of DNA the trainee chose may not necessarily reflect the true species of bacteria. But if multiple segments produce consistent top hits to the same species you would have some confidence.</p>"},{"location":"modules/denovo-module-bac/bac_asm/#conclusion","title":"Conclusion","text":"<p>You should now:</p> <ul> <li> <p>know how to use Velvet to assemble a simple genome from Illumina     sequences</p> </li> <li> <p>understand the role of the k-mer length K in the assembly process</p> </li> <li> <p>be able to relate the graph structure to the final contigs</p> </li> <li> <p>realize the limitations of short read sequences with respect to     genome complexity</p> </li> </ul> <p>Thank you.</p>"},{"location":"modules/denovo-module-cli/commandline/","title":"Commandline","text":""},{"location":"modules/denovo-module-cli/commandline/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Familiarise yourself with the command line environment on a Linux     operating system.</p> </li> <li> <p>Run some basic linux system and file operation commands</p> </li> <li> <p>Navigration of biological data files structure and manipulation</p> </li> </ul>"},{"location":"modules/denovo-module-cli/commandline/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/denovo-module-cli/commandline/#tools-used","title":"Tools Used","text":"<p>[style=multiline,labelindent=0cm,align=left,leftmargin=0.5cm]</p> <p>Basic Linux system commands on an Ubuntu OS.</p> <p>Basic file operation commands</p>"},{"location":"modules/denovo-module-cli/commandline/#useful-links","title":"Useful Links","text":"<p>Software Carpentry: https://software-carpentry.org</p> <p>1000Genome Project data for example: http://www.1000genomes.org</p>"},{"location":"modules/denovo-module-cli/commandline/#shell-exercise","title":"Shell Exercise","text":"<p>Let\u2019s try out your new shell skills on some real data.</p> <p>The file <code>1000gp.vcf</code> is a small sample (1%) of a very large text file containing human genetics data. Specifically, it describes genetic variation in three African individuals sequenced as part of the 1000 Genomes Project(http://www.1000genomes.org). The \u2018vcf\u2019 extension lets us know that it\u2019s in a specific text format, namely \u2018Variant Call Format\u2019 The file starts with a bunch of comment lines (they start with \u2018#\u2019 or \u2018##\u2019), and then a large number of data lines. This VCF file lists the differences between the three African individuals and a standard \u2018individual\u2019 called the reference (actually based upon a few different people). Each line in the file corresponds to a difference. The line tells us the position of the difference (chromosome and position), the genetic sequence in the reference, and the corresponding sequence in each of the three Africans. Before we start processing the file, let\u2019s get a high-level view of the file that we\u2019re about to work with.</p> <p>Open the Terminal and go to the directory where the data are stored:</p> <pre><code>cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n</code></pre> <p>What\u2019s the file size (in kilo-bytes), and how many lines are in the file?. (Hint: <code>man ls</code>, <code>man wc</code>)\\</p> <p>3.6M\\ 45034 lines\\</p> <p>Because this file is so large, you\u2019re going to almost always want to pipe (<code>`) the result of any command to less (a simple text viewer, type \u2018</code>q`\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t accidentally print 45,000 lines to the screen.</p> <p>Let\u2019s start by printing the first 5 lines to see what it looks like.</p> <pre><code>head -5 1000gp.vcf\n</code></pre> <p>That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the beginning of the file (they all start with \uff40#\u2019)!</p> <p>Print the first 20 lines to see more of the file.</p> <pre><code>head -20 1000gp.vcf\n</code></pre> <p>Okay, so now we can see the basic structure of the file. A few comment lines that start with \uff40#\u2019 or \uff40##\u2019 and then a bunch of lines of data that contain all the data and are pretty hard to understand. Each line of data contains the same number of fields, and all fields are separated with TABs. These fields are:</p> <ol> <li> <p>the chromosome (which volume the difference is in)</p> </li> <li> <p>the position (which character in the volume the difference starts     at)</p> </li> <li> <p>the ID of the difference</p> </li> <li> <p>the sequence in the reference human(s)</p> </li> </ol> <p>The rest of the columns tell us, in a rather complex way, a bunch of additional information about that position, including: the predicted sequence for each of the three Africans and how confident the scientists are that these sequences are correct.</p> <p>To start analyzing the actual data, we have to remove the header.</p> <p>How can we print the first 10 non-header lines (those that don\u2019t start with a \uff40#\u2019)?(Hint: <code>man grep</code> (remember to use pipes ``))</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | head\n</code></pre> <p>How many lines of data are in the file (rather than counting the number of header lines and subtracting, try just counting the number of data lines)?\\</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | wc -l (should print 45024)\n</code></pre> <p>Where these differences are located can be important. If all the differences between two encyclopedias were in just the first volume, that would be interesting. The first field of each data line is the name of the chromosome that the difference occurs on (which volume we\u2019re on).</p> <p>Print the first 10 chromosomes, one per line.</p> <p>Hint: <code>man cut</code> (remember to remove header lines first)</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | cut -f 1 | head\n</code></pre> <p>As you should have observed, the first 10 lines are on numbered chromosomes. Every normal cell in your body has 23 pairs of chromosomes, 22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair of sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re male).</p> <p>Let\u2019s look at which chromosomes these variations are on.</p> <p>Print a list of the chromosomes that are in the file (each chromosome name should only be printed once, so you should only print 23 lines).</p> <p>Hint: remove all duplicates from your previous answer (<code>man sort</code>)</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | cut -f 1 | sort -u\n</code></pre> <p>Rather than using <code>sort</code> to print unique results, a common pipeline is to first sort and then pipe to another UNIX command, <code>uniq</code>. The <code>uniq</code> command takes sorted input and prints only unique lines, but it provides more flexibility than just using sort by itself. Keep in mind, if the input isn\u2019t sorted, <code>uniq</code> won\u2019t work properly.</p> <p>Using <code>sort</code> and <code>uniq</code>, print the number of times each chromosome occurs in the file.</p> <p>Hint: <code>man uniq</code></p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | cut -f 1 | sort | uniq -c\n</code></pre> <p>Add to your previous solution to list the chromosomes from most frequently observed to least frequently observed.</p> <p>Hint: Make sure you\u2019re sorting in descending order. By default, sort sorts in ascending order.</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n</code></pre> <p>This is great, but biologists might also like to see the chromosomes ordered by their number (not dictionary order), since different chromosomes have different attributes and this ordering allows them to find a specific chromosome more easily.</p> <p>Sort the previous output by chromosome number</p> <p>Hint: A lot of the power of sort comes from the fact that you can specify which fields to sort on, and the order in which to sort them. In this case you only need to sort on one field.</p> <pre><code>grep -v \"\\^\\#\" 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n\n</code></pre>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/","title":"Eukaryote Assembly","text":""},{"location":"modules/denovo-module-euk/eukaryotic_assem/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Assess general assembly approach, kmer spectra and biases.</p> </li> <li> <p>Visually inspect the kmer spectra and KAT plots</p> </li> <li> <p>Run a first pass eukaryotic assembly and do goal checks</p> </li> <li> <p>Develop validation metrics or tools for NGS data and assembly.</p> </li> <li> <p>Improving methods and pipelines for genome assembly.</p> </li> <li> <p>Convince the lab guys to tweak protocols.</p> </li> </ul>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/denovo-module-euk/eukaryotic_assem/#tools-used","title":"Tools Used","text":"<p>Kmer Analysis Tool kit: https://github.com/TGAC/KAT</p> <p>Nextclip: https://github.com/richardmleggett/nextclip</p> <p>Abyss: http://www.bcgsc.ca/platform/bioinfo/software/abyss</p> <p>Soap Denovo: http://soap.genomics.org.cn/soapdenovo.html</p> <p>SOAPec: http://soap.genomics.org.cn/about.html</p> <p>BLAST: http://blast.ncbi.nlm.nih.gov/Blast.cgi</p>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#author-information","title":"Author Information","text":"<p>Primary Author(s): Bernardo Clavijo, TGAC Bernardo.Clavijo@tgac.ac.uk</p> <p>Contributor(s): Sonika Tyagi, AGRF sonika.tyagi@agrf.org.au Zhiliang Chan zhiliang@unsw.edu.au</p>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#first-pass-genome-assembly","title":"First Pass Genome Assembly","text":"<p>Assuming by now you are familiar with the general concept of de novo assembly, kmers and the de Bruijn graph based assembler. In this tutorial we will use ABySS to perform the first pass assembly of a eukaryotic genome and look at various parameters to assess the information content of the input data and choice of assembly parameters. sequence data.  </p> <p>Genome assembly is a challenging problem requiring heavy computational resources, expertise and time. Before you begin the process of de novo assembly there are a number of points you need to consider:</p> <ul> <li> <p>What is the objective of your assembly experiment? What biological question(s) you have?  </p> </li> <li> <p>Is assembly strictly necessary for the purpose in question?  </p> </li> <li> <p>Do you have right kind of data and enough coverage to start with?</p> </li> <li> <p>Do you have suitable computational resources to run this assembly?</p> </li> </ul> <p>Remember that the assembly is just a probabilistic model of a genome, condensing the information from the experimental evidence. All the information is already present in the experimental results. The goal of the assembly is to find the right motifs, the correct number of times, in correct order and position.</p>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#fusarium-first-pass-with-a-goal","title":"Fusarium first pass with a goal","text":"<p>Goal: Identify a fusarium sample is \u201ccloser\u201d to F. graminearum or F. pseudograminearum</p> <ul> <li> <p>Previous knowledge</p> <ul> <li>F. graminearum has a cluster producing PKS6 and NRSP7, while F. pseudograminearum produces PKS40 and NRPS32</li> </ul> </li> <li> <p>Data</p> <ul> <li> <p>Proteins sequences for:</p> <ol> <li> <p>F. graminearum (non necrotrophic): PKS6 and NRSP7</p> </li> <li> <p>F. pseudograminearum (necrotrophic): PKS40 and NRPS32</p> </li> </ol> </li> </ul> </li> <li> <p>Strategy</p> <ul> <li>Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence.</li> </ul> </li> <li> <p>Assembly goals:</p> <ul> <li> <p>To capture a good enough representation of the protein-coding space to get blast matches.</p> </li> <li> <p>To accurately represent the relevant whole cluster loci in a single sequence.</p> </li> </ul> </li> </ul>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#prepare-the-environment","title":"Prepare the Environment","text":"<p>Open the Terminal. First, go to the right folder, where the data are stored.</p> <pre><code>cd /home/trainee/eukaryotic\nls\n</code></pre>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#task-11-first-pass-assembly-k71","title":"Task 1.1: First pass assembly, k=71","text":"<p>Let\u2019s assemble Fusarium with abyss, k=71</p> <pre><code>cd /home/trainee/eukaryotic\nmkdir abyss_k71\ncd abyss_k71\nabyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=71 name=CS3270_abyss_k71 np=4 &gt; CS3270_abyss_k71.log 2&gt;&amp;1\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>k: kmer size np: number of processors to be used sequence file names: R1 and R2 reads of a paired end sequence data  </p>  <p> Let\u2019s look at the statistics of the assembly we just did.</p> <p>Ok, there are no stats available in the folder, but we can always use <code>abyss-fac</code> to get the stats:</p> <pre><code>abyss-fac CS3270_abyss_k71-*tigs.fa  | tee CS3270_abyss_k71-stats.tab\nless CS3270_abyss_k71-stats.tab\n</code></pre> <p> Table 1: Statistics of fusarium assembly by ABySS using k=71</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     27 13 2 970 6004 13202 52602 28712 52602 112849 CS3270_abyss_k71-unitigs.fa   5 1 1 128429 128429 128429 128429 128429 128429 128429 CS3270_abyss_k71-contigs.fa      <p>Questions</p> <p>How many unitigs/contigs do you have in the assembly?</p>   Answer <p>27/5</p>   <p>What are the length statistics of your assembly?</p>   Answer <p>In the table above</p>   <p>Does it match what you think before the assembly and why?</p>   Answer <p>No</p>    <p> The assembly is looking strange! It\u2019s time for some analysis:</p> <p>Check frequencies for kmers kept/discarded/etc.</p> <p>Check spectra-cn and compare with expectations. Let\u2019s do this by the following commands:</p> <pre><code>less CS3270_abyss_k71.log\nless coverage.hist\n</code></pre> <p>We will now plot the values from the <code>coverage.hist</code>:</p> <pre><code>gnuplot &lt;Press enter&gt;\nset xrange [0:200]\nset yrange [0:5000]\nplot \"coverage.hist\"\nexit\n</code></pre> <p>Looks like we are not assembling this bit, let\u2019s have another look at the spectra</p> <pre><code>kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 '../*.fastq' CS3270_abyss_k71-contigs.fa\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>o: Path prefix for files generated by this program. t: The number of threads to use. C: Whether the jellyfish hash for input 1 contains K-mers produced for both strands D: Whether the jellyfish hash for input 2 contains K-mers produced for both strands --d1_bins: Number of bins for the first dataset. i.e. number of rows in the matrix  </p>   <pre><code>kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx\n</code></pre> <p>Description of the arguments used in the command:</p>  <p>x: Maximum value for the x-axis (default value auto calculated from matrix, otherwise 1000) y: Maximum value for the y-axis (default value auto calculated from matrix if possible, otherwise, 1000000) o: The path to the output file</p>  <p> The kmer spectra for Fusarium assembly with <code>abyss</code>, k=71 should be looking like this:</p> <p> Figure 1: Kmer spectrum for Fusarium assembly with abyss, k=71, coloured by the frequencies as found on the reference assembly.  </p> <p> Take the output and BLAST it in NCBI. What is it? Surprising?</p> <p>Choosing a wrong k value (too large in this case) and just running a typical assembly job, we can end up with something quite more interesting. It is easy by comparison to spot some missing content, alongside duplications and triplications (and quadruplications and so on) that should not be there. This assembly will get us nowhere, let\u2019s choose a lower K to gain coverage and start again.</p>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#task-12-first-pass-assembly-k27","title":"Task 1.2: First pass assembly, k=27","text":"<p>We now assemble Fusarium with <code>abyss</code> and k=27:</p> <pre><code>cd /home/trainee/eukaryotic\nmkdir abyss_k27\ncd abyss_k27\nabyss-pe in=\"../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\" k=27 name=CS3270_abyss_k27 np=4 &gt; CS3270_abyss_k27.log 2&gt;&amp;1\n</code></pre> <p>Let\u2019s look at the stats by doing:</p> <pre><code>less CS3270_abyss_k27-stats.tab\n</code></pre> <p> The stats look better:  </p> <p>Table 2: Statistics of fusarium assembly by ABySS using k=27.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     30645 2717 430 502 11354 25336 47966 31027 147694 36.14e6 CS3270_abyss_k27-unitigs.fa   21511 350 33 527 157565 338989 630228 407098 1265237 36.52e6 CS3270_abyss_k27-contigs.fa   21327 205 17 527 332444 716132 1265237 791882 1880850 36.51e6 CS3270_abyss_k27-scaffolds.fa    <p> Let\u2019s check a bit anyway:</p> <pre><code>less CS3270_abyss_k27.log\nless coverage.hist\n</code></pre> <p>How is the coverage plot looking now?</p> <pre><code>gnuplot\ngnuplot&gt; set xrange [0:50]\ngnuplot&gt; set xrange [0:4000000]\ngnuplot&gt; plot \"coverage.hist\"\ngnuplot&gt; exit\n</code></pre> <p>K-mer spectrum:</p> <p><pre><code>kat plot spectra-cn -y 1000 -x 1000 -o reads_vs_abyss1-main.mx.spectra-cn_noabsent.png reads_vs_abyss1-main.mx\nkat comp -o reads_vs_abyss_k27 -t 4 -C -D '../*.fastq' CS3270_abyss_k27-scaffolds.fa\n</code></pre>   Figure 2: Kmer spectrum for Fusarium assembly with abyss, k=27, coloured by the frequencies as found on the reference assembly. </p>  <p>Questions</p> <p>Any tools you can use to check kmer spectra at any K before assembling?</p>   Answer <p>KAT</p>   <p>Can you predict what will happen if you use KAT with larger K values?</p>  <p> Will the assembly answer the biological question?</p> <p>Use BLAST or BLAT and the databases to check.</p> <pre><code>makeblastdb -in CS3270_abyss_k27-scaffolds.fa -dbtype nucl\nblat \u2013t=dnax \u2013q=prot \u2013minIdentity=90 CS3270_abyss_k27-scaffolds.fa test_genes.fasta out.psl\n</code></pre>"},{"location":"modules/denovo-module-euk/eukaryotic_assem/#references","title":"References","text":"<ol> <li>De novo genome assembly: what every biologist should know Nature     Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935</li> </ol>"},{"location":"modules/denovo-module-scaffolding/scaffold/","title":"Scaffolding","text":""},{"location":"modules/denovo-module-scaffolding/scaffold/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Run an assembly using long mate pair (LMP) reads.</p> </li> <li> <p>Compare results of LMP assemblies with those from the short reads.</p> </li> <li> <p>Run scaffolding and assess the assembly statistics.</p> </li> </ul>"},{"location":"modules/denovo-module-scaffolding/scaffold/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/denovo-module-scaffolding/scaffold/#tools-used","title":"Tools Used","text":"<p>Kmer Analysis Tool kit: https://github.com/TGAC/KAT</p> <p>Nextclip: https://github.com/richardmleggett/nextclip</p> <p>Abyss: http://www.bcgsc.ca/platform/bioinfo/software/abyss</p> <p>Soap Denovo: http://soap.genomics.org.cn/soapdenovo.html</p> <p>SOAPec: http://soap.genomics.org.cn/about.html</p>"},{"location":"modules/denovo-module-scaffolding/scaffold/#author-information","title":"Author Information","text":"<p>Primary Author(s): Bernardo Clavijo, TGAC Bernardo.Clavijo@tgac.ac.uk</p> <p>Contributor(s): Sonika Tyagi, AGRF sonika.tyagi@agrf.org.au Zhiliang Chan zhiliang@unsw.edu.au</p>"},{"location":"modules/denovo-module-scaffolding/scaffold/#scaffolding-with-long-mate-pair-libraries-chalara-scaffolding-using-lmp","title":"Scaffolding with long mate pair libraries: Chalara scaffolding using LMP","text":"<p>Scaffold is made of two or more contigs joined together using the read pair information. In the absence of a high-quality reference genome, new genome assemblies are often evaluated on the basis of the number of scaffolds and contigs required to represent the genome. Other criteria such as the alignment of reads back to assemblies, N50, and the length of contigs and scaffolds relative to the size of the genome can also be used to assess the quality of assemblies. In this exercise we will be using a long mate pair data to perform assembly and scaffolding and we will focus on how using LMP reads can affect the assemblies. Most part of this tutorial has been precomputed and made available to you in interest of time. We will spend more time exploring and discussing the results.</p>"},{"location":"modules/denovo-module-scaffolding/scaffold/#pair-end-assembly-using-chalara-dataset","title":"Pair-end assembly using Chalara dataset","text":"<p>Before doing a scaffolding, we need to build an assembly using the pair-end reads first.</p> <p>Let\u2019s go to the correct location where the files are:</p> <pre><code>cd /home/trainee/scaffolding/cha_raw/test_1\n</code></pre>   <p>STOP</p> <p>You DO NOT need to run this command. This has already been done for you.</p> <pre><code>abyss-pe name=cha1 k=27 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4\n</code></pre>  <p> Let\u2019s look at the stats by doing:</p> <pre><code>less cha1-stats.tab\n</code></pre> <p> Table 1: Statistics of Chalara assembly by ABySS using k=27.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     394789 11681 2094 500 2880 6254 12303 7936 44414 44.07e6 cha1-unitigs.fa   394199 11673 2097 500 2887 6255 12303 7937 44414 44.11e6 cha1-contigs.fa   394161 11647 2095 500 2898 6269 12303 7944 44414 44.12e6 cha1-scaffolds.fa    <pre><code>cd /home/trainee/scaffolding/cha_raw/test_2\n</code></pre>   <p>STOP</p> <p>This is also a pre-computed example for you:</p> <pre><code> abyss-pe name=cha2 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" np=4\n</code></pre>  <p> Again, we need to look at the statistics of our new assembly using k=61:</p> <pre><code>less cha2-stats.tab\n</code></pre> <p> It should be looking like this:</p> <p>Table 2: Statistics of Chalara assembly by ABySS using k=61.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     130547 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 cha2-unitigs.fa   130210 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 cha2-contigs.fa   130182 12555 1769 500 3377 8394 16380 10534 54300 50.78e6 cha2-scaffolds.fa      <p>Question</p> <p>The assembly contiguity is worse than Fusarium, why?</p>   Answer <ul> <li>Genome characteristics.</li> <li>Data</li> <li>Coverage</li> <li>Error distributions</li> <li>Read sizes</li> <li>Fragment sizes</li> <li>Kmer spectra</li> </ul>"},{"location":"modules/denovo-module-scaffolding/scaffold/#chalara-scaffolding-using-long-mate-pair-reads-lmp","title":"Chalara scaffolding using long mate-pair reads (LMP)","text":"<p>Let\u2019s put some LMP in there.</p> <pre><code>cd /home/trainee/scaffolding/cha_raw/test_3\n</code></pre>   <p>STOP</p> <p>This is also a pre-computed example for you:</p> <pre><code>abyss-pe name=chalmp1 k=61 in=\"../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\" mp=\"lmp1\" lmp1=\"../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\" np=4\n</code></pre>  <p> <pre><code>less chalmp1-stats.tab\n</code></pre></p> <p> Table 3: Statistics of Chalara assembly by ABySS using k=27.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     130548 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 chalmp1-unitigs.fa   130211 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 chalmp1-contigs.fa   130148 12545 1769 500 3380 8400 16380 10535 54300 50.78e6 chalmp1-scaffolds.fa    <p>So\u2026 what happened? Let\u2019s check the data by:</p> <ul> <li>Kmer spectra</li> <li>Fragment sizes</li> </ul> <p> Any hints on the protocol? And a not-so-obvious property:</p> <pre><code>kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx\n</code></pre> <p> Do you remember LMP required pre-processing? Let\u2019s try with processed LMP.</p> <p>Prior task (already made) preprocess the LMP with <code>nextclip</code>.</p> <pre><code>cd /home/trainee/scaffolding/cha_proc/test_1\n</code></pre>   <p>STOP</p> <p>This is also a pre-computed example for you:</p> <pre><code>abyss-pe name=chalmpproc1 k=61 in=\"../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\" mp=\"proclmp1\" proclmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4\n</code></pre>   <pre><code>less chalmpproc1-stats.tab\n</code></pre> <p> Table 4: Statistics of Chalara assembly by ABySS using k=61 with LMP.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     130548 12596 1770 500 3352 8379 16380 10518 54300 50.75e6 chalmpproc1-unitigs.fa   130211 12575 1771 500 3363 8382 16380 10525 54300 50.78e6 chalmpproc1-contigs.fa   122061 6679 167 500 18609 87510 187171 106178 397967 51.15e6 chalmpproc1-scaffolds.fa    <p> That\u2019s much better!</p>"},{"location":"modules/denovo-module-scaffolding/scaffold/#chalara-beyond-first-pass","title":"Chalara: beyond first pass","text":"<p>Do you remember these?</p> <p>Genome characteristics. Data:</p> <ul> <li>Coverage</li> <li>Error distributions</li> <li>Read sizes</li> <li>Fragment sizes</li> <li>Kmer spectra</li> </ul> <p>Let\u2019s think a bit and try to improve the assembly.</p>  <p>Question</p> <p>If you look at the pre-processed LMP, do you notice anything peculiar?</p>  <p>Testing the inclusion of heavily pre-processed LMP coverage into the DBG</p> <pre><code>cd /home/trainee/scaffolding/cha_proc/test_2\n</code></pre>   <p>STOP</p> <p>This is also a pre-computed example for you:</p> <pre><code>abyss-pe name=chalmp2 k=61 se=\"../LIB8209_preproc_single.fastq\" lib=\"lmp1\" lmp1=\"../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\" np=4 &gt;chaproclmp2.log 2&gt;&amp;1\n</code></pre>   <pre><code>less chalmp2-stats.tab\n</code></pre> <p> Let\u2019s look at the stats.  </p> <p>Table 5: Statistics of improved Chalara assembly by ABySS using k=61 with LMP.</p>    n n:500 L50 min N80 N50 N20 E-size max sum name     128306 10150 1182 500 4954 12585 24777 15693 68684 51.03e6 chaproclmp4-unitigs.fa   118939 5772 362 500 15717 41870 82033 52819 252066 52.24e6 chaproclmp4-contigs.fa   116139 4014 141 500 41145 109273 224986 133450 460979 52.56e6 chaproclmp4-scaffolds.fa"},{"location":"modules/denovo-module-scaffolding/scaffold/#references","title":"References","text":"<ol> <li> <p>Robert Ekblom* and Jochen B. W. Wolf. \u201cA field guide to whole-genome     sequencing, assembly and annotation\u201d. Evolutionary Applications     Special Issue: Evolutionary Conservation Volume 7, Issue 9, pages     1026 \u2013 1042, November 2014</p> </li> <li> <p>De novo genome assembly: what every biologist should know. Nature     Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935</p> </li> </ol>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/","title":"Mining EBI Metagenomics (EMG) output with R","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part1/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Load requred R packages and input data for analysis from EMG</p> </li> <li> <p>Know how analysis the data with R statistical packages</p> </li> </ul>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part1/#tools-used","title":"Tools Used","text":"<p>RStudio:  https://www.rstudio.com/</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#data","title":"Data","text":"<p>America Gut Project: EMG Project ERP012803</p> <p>Author: B T.F. Alako - Cochrane team: European Nucleotide Archive. </p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#useful-links","title":"Useful Links","text":"<p>EBI Metagenomics resource (EMG) :   EBI MG Portal</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#part-1-exploratory-analysis-of-emg-portal-contingency-tables","title":"Part 1: Exploratory analysis of EMG portal contingency tables","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part1/#introduction","title":"Introduction","text":"<p>The American Gut project is the largest crowdsourced citizen science project to date. Fecal, oral, skin, and other body site samples collected from thousands of participants represent the largest human microbiome cohort in existence. Detailed health and lifestyle and diet data associated with each sample is enabling us to deeply examine associations between the human microbiome and factors such as diet (from vegan to near carnivore and everything in between), season, amount of sleep, and disease states such as IBD, diabetes, or autism spectrum disorder-as well as many other factors not listed here. The American Gut project also encompasses the British Gut and Australian Gut projects, widening the cohort beyond North America. As the project continues to grow, we will be able to identify significant associations that would not be possible with smaller, geographically and health/disease status-limited cohorts. EMG Project ERP012803</p> <p>We will explore the data generated by the study above. The data is available at: EMG Project ERP012803</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#loading-of-r-packages","title":"Loading of R packages","text":"<p>First we need to load the required R packages for analysis.</p> <pre><code>suppressWarnings(suppressMessages(require(stringr)))      # String format\nsuppressWarnings(suppressMessages(require(ade4)))         # Multivariate analysis\nsuppressWarnings(suppressMessages(require(ggplot2)))      # Fancy plotting\nsuppressWarnings(suppressMessages(require(grid)))         # Has the viewport function\nsuppressWarnings(suppressMessages(require(dplyr)))        # Data manipulation\nsuppressWarnings(suppressMessages(require(tidyr)))        # Data manipulation\nsuppressWarnings(suppressMessages(require(FactoMineR)))   # Multivariate analysis\nsuppressWarnings(suppressMessages(require(factoextra)))   # Visualize result of multivariate analysis\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#load-emg-input-data-for-analysis","title":"Load EMG input data for analysis","text":"<p>We can then load the analysis summary and metadata files for ERP0123803.</p> <pre><code>ERP012803 &lt;- \"https://www.ebi.ac.uk/metagenomics/projects/ERP012803/download/2.0/export?contentType=text&amp;exportValue=taxonomy_abundances\"\nERP012803.meta &lt;- \"https://www.ebi.ac.uk/metagenomics/projects/ERP012803/overview/doExport\";\ngutproject &lt;- tbl_df(read.delim(file=ERP012803))\nmeta &lt;-  read.csv(file=ERP012803.meta)\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#preprocess-the-data","title":"Preprocess the data","text":"<ul> <li>Filter to only retain full taxonomic lineages with a species name</li> </ul> <p><pre><code>gutproject &lt;- gutproject %&gt;% \n              rename(taxonomy=X.SampleID) %&gt;%\n                #mutate(taxonomy=gsub(\".*?s__\",\"\",taxonomy)) %&gt;% \n                mutate(taxonomy=gsub(\".*?g__\",\"\",taxonomy)) %&gt;%\n                mutate(taxonomy=gsub(\";s__\",\"_\", taxonomy)) %&gt;%\n                  filter(taxonomy !=\"\" | taxonomy=='_' ) %&gt;%\n                    filter(!grepl('_$', taxonomy))\n</code></pre>  - Merge counts of species in the same experimental sample</p> <p><pre><code>gutproject &lt;- gutproject %&gt;% \n                group_by(taxonomy) %&gt;%\n                  summarize_each(funs(sum)) %&gt;%\n                    ungroup()\n</code></pre>  - Use the species name as the rowname, this replaces the default numerical name </p> <pre><code>rownames(gutproject) &lt;- gutproject$taxonomy\n</code></pre>  <p>Note</p>  <p>You can ignore warning: Setting row names on a tibble is deprecated.</p> <ul> <li>Transform the wide format data into the long format for the purpose of appending descriptive information</li> </ul> <p><pre><code>gutproject.long  &lt;- gutproject %&gt;% \n                      gather(sampleb, freq, ERR1072624:ERR1160857) %&gt;%\n                        filter(taxonomy !=\"Root\")\n</code></pre>  - Retain only the sample description and the Run id from the metadata</p> <p><pre><code>meta &lt;- meta %&gt;% select(Sample.Description, Run.ID)\n</code></pre>  - What is the composition of the data.</p> <pre><code>meta.strip &lt;-  tbl_df(meta) %&gt;% mutate(Sample.Description=gsub(\" \",\"_\",Sample.Description),\n                         Sample.Description=gsub(\"American_Gut_Project_\",\"\", Sample.Description), \n                         Sample.Description=gsub(\"American_\",\"\", Sample.Description)\n                         )\nmeta.strip &lt;- meta.strip %&gt;% group_by(Sample.Description) %&gt;%\n              mutate(count=n()) %&gt;% ungroup() %&gt;%\n              select(Sample.Description, count)  %&gt;% unique()\n\nmeta.strip %&gt;% ggplot(aes(x=Sample.Description, y=count)) +  geom_bar(stat='identity') + \n                geom_text(aes(label=count))  + theme_bw() +\n                      theme(axis.text.x=element_text(angle=45, hjust=1)) + \n                      ggtitle(\"American gut project\")\n</code></pre>  <p>Question</p>  <p>What is the data mostly composed of?</p> <ul> <li>Append metadata to the gut data</li> </ul> <p><pre><code>gutproject.df &lt;- merge(gutproject.long, meta, by.x='sampleb', by.y='Run.ID')\ngutproject.df &lt;- tbl_df(gutproject.df)\n</code></pre>  - Clean up the description of sample</p> <p><pre><code>gutproject.df &lt;- gutproject.df %&gt;% \n                  mutate(Sample.Description=gsub(\" \",\"_\",Sample.Description), \n                   Sample.Description=gsub(\"American_Gut_Project_\",\"\", Sample.Description),\n                   Sample.Description=gsub(\"American_\",\"\", Sample.Description)) %&gt;%\n                    ungroup() \n</code></pre>  - Merge taxonomy occurences count per data source</p> <p><pre><code>gutproject.df &lt;- gutproject.df %&gt;%\n                  group_by(Sample.Description,taxonomy) %&gt;% \n                    summarise(cum=sum(freq)) %&gt;% ungroup()\ngutproject.df &lt;- gutproject.df %&gt;%\n                  select(taxonomy,Sample.Description,cum)\n                  #filter(Sample.Description !=\"Stool_sample\") %&gt;%\n                  #filter(Sample.Description !=\"Stool_sample\" &amp; Sample.Description !=\"Mouth_sample\" )\n                  #filter( Sample.Description     !=\"Vaginal_mucus_sample\" )\n</code></pre>  - Transform the long format to wide format for the purpose of Mutivariate analysis</p> <pre><code>gutproject.df &lt;- gutproject.df %&gt;%\n                    spread(Sample.Description, cum, fill=0) %&gt;%\n                      as.data.frame()\n</code></pre> <p><pre><code>gutproject.mat &lt;- gutproject.df %&gt;% select(-c(1)) %&gt;% as.matrix()\nrownames(gutproject.mat) &lt;- gsub(\"\\\\[|\\\\]\",\"\", perl=TRUE, gutproject.df$taxonomy)\n</code></pre>  - Clean up the matrix</p> <pre><code>gutproject.mat = gutproject.mat[!rownames(gutproject.mat)==\"\",]\nas.matrix(table(rownames(gutproject.mat)))[as.matrix(table(rownames(gutproject.mat)))&gt;1,]\ntodiscard &lt;- (as.matrix(table(rownames(gutproject.mat)) &gt; 1))\nmyselect = names(todiscard[todiscard[,1]==TRUE,])\ngutproject.mat&lt;- gutproject.mat[!rownames(gutproject.mat) %in% myselect,]\ngutproject.mat &lt;- gutproject.mat[which(rowSums(gutproject.mat)!=0),]\n</code></pre>  <p>Question</p>  <p>Is there any association between the species and the data source?</p> <ul> <li>Perform a chi-square test of independence, this evaluates whether there is a significant dependence between species and biological samples</li> </ul> <p><pre><code>chisq &lt;- chisq.test(gutproject.mat)\nchisq\n</code></pre> The results should look like this. <pre><code>Pearson's Chi-squared test\ndata:  gutproject.mat\nX-squared = 106730000, df = 8442, p-value &lt; 2.2e-16\n</code></pre> The species and biological samples are statistically significantly associated (p-value=0)</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#perform-correspondence-analysis-ca","title":"Perform Correspondence Analysis (CA)","text":"<p>Correspondence Analysis (CA) is a generalized form of Principal Component Analysis for categorical data. We use CA to reduce the dimension of the data without loosing the most important information. CA is used to graphically visualize rows points and column points in a low dimensional space</p> <pre><code>gutproject.ca &lt;- CA(gutproject.mat, graph=FALSE)\n</code></pre> <ul> <li>Explore the content of the CA output</li> </ul> <pre><code>summary(gutproject.ca, nb.dec = 2, nbelements = 4, ncp= 3)\n</code></pre> <p>The result of the function summary() contains the chi-square statistics and 3 tables:</p> <ul> <li>Table 1- Eigenvalues, displays the variances and the percentage of vaiances retained by each dimension.</li> <li>Table 2, Contains the coordinates, the contribution and the cos2 (quality of representation [0-1] of the first 4 active rows variable on the dimension 1, 2 and 3</li> <li>Table 3: displays the coordinates, the contribution and the cos2 of the first 4 active column variables on the dimension 1, 2 and 3</li> </ul>  <p>Question</p>  <p>Can you interprete the CA result?</p>  <p>Hint</p>  <p>correlation coef., chi-square statistic</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#correlation-coefficient","title":"Correlation coefficient","text":"<p>Eigen values is the amount of information retained by each axis.</p> <pre><code>eig &lt;- get_eigenvalue(gutproject.ca)\ntrace &lt;- sum(eig$eigenvalue)\ncor.coef &lt;- sqrt(trace)\ncor.coef\n</code></pre> <pre><code>[1] 1.378614\n</code></pre> <p>As a rule of thumb a correlation above 0.2 can be considered important (Bendixen 1995, 567; Healey 2013, 289-290) The correlation coef. is 1.38 in the American gut project dataset, indicating a strong association between row (species) and column(biological samples variables.).  A more rigorous approach for examining the association is to use Chi-square statistics.  The association is highly significant (p-value=0) </p> <p>Explore the percentages of inertia explained by the CA dimensions, the scree plot</p> <p><pre><code>fviz_screeplot(gutproject.ca, barfill='white', addlabels=TRUE) + theme_bw()\n</code></pre> The point at which the scree plot shows a bend mignt indicate an optimal dimensionality.</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part1/#hierarchical-clustering-of-principal-components","title":"Hierarchical Clustering of principal components","text":"<p>Compute hierarchical clustering of species of CA results</p> <p><pre><code>gutproject.hcpc &lt;- HCPC(gutproject.ca, graph = TRUE , nb.clust=4, order=TRUE )\n</code></pre>  - Visualize the CA results</p> <p>Symmetric biplot, whereby both rows (blue) and columns (red) are represented in the same space using the principal coordinates. Only the distance between row points or the distance between column points can be reliably interpreted.  With a symmetric plot, only general statements can be drawn about the pattern.  The inter-distance between rows and column can not be interpreted.</p> <p><pre><code>fviz_ca_biplot(gutproject.ca, repel=TRUE, select.row = list(contrib = 95), select.col = list(contrib = 10), geom=\"text\") + theme_minimal()\n</code></pre>  - Remove labels and add cluster centers</p> <p>A 3D map of the hierarchical clustering of the principal component.</p> <p><pre><code>plot(gutproject.hcpc, choice =\"3D.map\", draw.tree = FALSE,\n     ind.names = FALSE, centers.plot = TRUE, \n     title='Hierarchical clustering of the Principal Components')\n</code></pre>  * Hierarchical Clustering of principal component.</p> <p>Compute hierarchical clustering of biological samples of CA results</p> <p><pre><code>res.hcpc &lt;- HCPC(gutproject.ca, cluster.CA = \"columns\", graph = TRUE , nb.clust=4, order=TRUE )\n</code></pre> Biological sample cluster in the dataset  using fviz_cluster and theme_bw()</p> <p>Transpose the row and column in the gut data and perform a new correspondance analysis</p> <p><pre><code>gutproject.ca &lt;- CA(t(gutproject.mat), ncp=4)\n</code></pre> Symmetric biplot </p> <p><pre><code>fviz_ca_biplot(gutproject.ca, repel=TRUE, select.row = list(contrib = 12), \n               select.col = list(contrib = 95), geom=\"text\") +\n  theme_minimal()\n</code></pre>  - Remove labels and add cluster centers</p> <p>3D hierarchical clustering of the principal components.</p> <pre><code>plot(res.hcpc, choice =\"3D.map\", draw.tree = FALSE,\n     ind.names = FALSE, centers.plot = TRUE, title='Hierarchical clustering of the Principal Components')\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part2A/","title":"Part 2: Functional analysis of EMG genomic and transcriptomic data using R","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part2A/#gut-microbiota-disturbance-during-antibiotic-therapy-a-multi-omic-approach","title":"Gut microbiota disturbance during antibiotic therapy: a multi-omic approach","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part2A/#introduction","title":"Introduction","text":"<p>Antibiotic usage strongly affects microbial intestinal metabolism and thereby impacts human health. Understanding this process and the underlying mechanisms remains a major research goal. Accordingly, we conducted the first comparative omic investigation of gut microbial communities in faecal samples taken at multiple time points from an individual subjected to Beta-lactam therapy. Perez-Cobas AE, et al. 2013 Gut 62(11), 1591-1601</p> <p>We will explore the data generated by the study above. The data is available at: https://www.ebi.ac.uk/metagenomics/projects/ERP001506</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2A/#gather-the-necessary-analysis-results-file-from-emg-portal","title":"Gather the necessary analysis results file from EMG portal","text":"<p>Read in the different contigency tables. R provides a very versatile data structure that allows us to store various data types, namely the list.</p> <pre><code>analysis_files &lt;- c(\"BP_GO_abundances\",\"BP_GO-slim_abundances\",\"CC_GO_abundances\",\"CC_GO-slim_abundances\",\"GO_abundances\",\"GO-slim_abundances\",\"IPR_abundances\",\"MF_GO_abundances\",\"MF_GO-slim_abundances\",\"phylum_taxonomy_abundances\",\"taxonomy_abundances\")\n</code></pre> <p>Read in all the contingency tables in a list making use of the lapply function</p> <pre><code>baseanalysis = \"https://www.ebi.ac.uk/metagenomics/projects/ERP001506/download/2.0/export?contentType=text&amp;exportValue=\"\nmetadata &lt;- \"https://www.ebi.ac.uk/metagenomics/projects/ERP001506/overview/doExport\";\nfilenames &lt;- paste(baseanalysis, analysis_files, sep=\"\")\nresults &lt;- lapply(filenames,function(i){\nprint(i)\nread.delim(i, header=TRUE)\n})\nmetadata &lt;- tbl_df(read.csv(file=metadata))\n\nmetadata &lt;- metadata %&gt;%\nfilter(Release.version&gt;1) %&gt;%\nmutate(Sample.Name= gsub(\"Human gut metagenome, |Human gut metatranscriptome \\\\(mRNA\\\\), | after antibiotics\",\"\",Sample.Name)) %&gt;%\nmutate(Sample.Name= paste(\"days\",gsub(\" days\", \"\", Sample.Name), sep=\"\")) %&gt;%\nmutate(genomictype= ifelse(Experiment.type==\"metatranscriptomic\", \"mRNA\", \"DNA\")) %&gt;%\nrename(days=Sample.Name, id=Sample.ID, datatype=Experiment.type,genomictype=genomictype) %&gt;%\nselect(days,id,datatype,genomictype) %&gt;%\nmutate(id=ifelse(datatype==\"amplicon\", paste(id, \"_A\", sep=\"\"), ifelse(datatype==\"metatranscriptomic\", paste(id,\"_T\", sep=\"\"), paste(id, \"_G\", sep=\"\")))) %&gt;%\nmutate(datatype=str_to_title(datatype))\n</code></pre>  <p>Question</p>  <p>How can we access elements in the list?</p> <p><pre><code>attributes(results)\n</code></pre> This approach enables us to assign a name to each entry in the list for unambiguous access to different contingency tables <pre><code>colheader &lt;- gsub(analysis_files, pattern=\"-\", replacement =\"_\")\nnames(results) &lt;- colheader\nattributes(results)\n</code></pre> Let\u2019s have a look at results object</p> <pre><code>str(results, list.len=4)\n</code></pre> <p>Each entry in the list can now be accessed by name. Give each row an explicit name before manipulating the table contents with dplyr functions. The main advantage of tbl_df (return a dataframe from a dataframe) is that it only displays a few rows and all the columns fit on the current screen, with the rest of the data displayed as text. We will be making extensive use of the dplyr package, which allows operations to be chained (\u201c%&gt;%\u201d); it is analogous to the linux pipe command (|), see here for more details: https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html </p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2A/#1-taxonomy-who-is-there-and-in-what-proportion_","title":"1. Taxonomy: who is there and in what proportion?_","text":"<p>We can optimise the display of the first few columns or so\u2026</p> <p><pre><code>taxonomy &lt;- tbl_df(results$phylum_taxonomy_abundances)\ntaxonomy &lt;- taxonomy %&gt;% mutate(phylum=as.character(phylum))\ntax_g &lt;- taxonomy %&gt;%\nselect(matches(\"_G|um\")) %&gt;%\nmutate(DataType=\"Metagenomics\")\ntax_t &lt;- taxonomy %&gt;%\nselect(matches(\"_T|um\")) %&gt;%\nmutate(DataType=\"Metatranscriptomics\")\ntax_a &lt;- taxonomy %&gt;%\nselect(matches(\"_A|um\")) %&gt;%\nmutate(DataType=\"Amplicon\")\n\nrenameSample &lt;- function(metadata=metadata, localdf=localdf){\nstrippedcol &lt;- colnames(localdf)\nfor (i in 1:nrow(metadata)){\nstrippedcol[which(strippedcol==as.character(metadata[i,2]))] &lt;- as.character(metadata[i,1])\n}\nreturn(strippedcol)\n}\n\ncolnames(tax_g) &lt;- renameSample(metadata=metadata, localdf=tax_g)\ncolnames(tax_t) &lt;- renameSample(metadata=metadata, localdf=tax_t)\ncolnames(tax_a) &lt;- renameSample(metadata=metadata, localdf=tax_a)\n</code></pre> Then transform a wide contingency table into a long one for the purposes of plotting.</p> <p><pre><code>tax_metag &lt;- tax_g %&gt;% gather(condition, count, 3:ncol(tax_g)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\ntax_metat &lt;- tax_t %&gt;% gather(condition, count, 3:ncol(tax_t)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n</code></pre> We can merge both metatranscriptomic and metagenomic relative occurrences of operational taxonomic units (OTU) per condition to visualise the community dynamic during treatment.</p> <p><pre><code>genomic_transcriptomics &lt;- rbind(tax_metag, tax_metat)\n</code></pre> Bar plot of relative abundance of species <pre><code>genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=phylum)) +\ngeom_bar(stat='identity', position='fill', aes(fill = phylum)) +\nfacet_grid(~DataType) +\nggtitle('Species abundance through treatment per datatype') +\nylab('proportion abundance') +\ntheme_light() + theme(axis.text.x=element_text(angle=45, hjust=1)) +\ncoord_flip()\n</code></pre></p>  <p>Question</p>  <p>What conclusion would you draw from this distribution?</p> <p>The heatmap is of absolute Species counts per datatypes</p> <pre><code>genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=phylum))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=count)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part2A/#2-function-who-does-what-in-what-relative-proportion","title":"2. Function: who does what, in what relative proportion?","text":"<p>Recall the various functional annotations in the created list.</p> <pre><code>attributes(results)\n</code></pre> <p>As an initial exploration, we will focus on a broad GO annotation (GO-slim)</p> <p>** (A) GO Biological processes**: relative occurrence during treatment</p> <pre><code>bp_slim &lt;- tbl_df(results$BP_GO_slim_abundances)\nbp_slim &lt;- bp_slim %&gt;% mutate(description=as.character(description))\nbp_slim_g &lt;- bp_slim %&gt;%\nselect(matches(\"_G|description\")) %&gt;%\nmutate(DataType=\"Metagenomics\")\nbp_slim_t &lt;- bp_slim %&gt;%\nselect(matches(\"_T|description\")) %&gt;%\nmutate(DataType=\"Metatranscriptomics\")\nbp_slim_a &lt;- bp_slim %&gt;%\nselect(matches(\"_A|description\")) %&gt;%\nmutate(DataType=\"Amplicon\")\n</code></pre> <p>Give meaningful names to conditions, as for the taxonomic case above.</p> <pre><code>colnames(bp_slim_g) &lt;- renameSample(metadata=metadata, localdf=bp_slim_g)\ncolnames(bp_slim_t) &lt;- renameSample(metadata=metadata, localdf=bp_slim_t)\ncolnames(bp_slim_a) &lt;- renameSample(metadata=metadata, localdf=bp_slim_a)\nbp_slim_metagenomics &lt;- bp_slim_g %&gt;%\ngather(condition, count, 3:ncol(bp_slim_g)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n\nbp_slim_metatransciptomics &lt;- bp_slim_t %&gt;%\ngather(condition, count, 3:ncol(bp_slim_t)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n</code></pre> <p>Merge both metatranscriptomic and metagenomic relative occurences of biological process terms per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.</p> <p><pre><code>bp_genomic_transcriptomics &lt;- rbind(bp_slim_metagenomics, bp_slim_metatransciptomics)\n</code></pre> Bar plot of relative abundance of Biological Processes</p> <pre><code>bp_genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=description)) +\ngeom_bar(stat='identity', position='fill', aes(fill = description)) +\nfacet_wrap(~DataType, ncol=1) +\nggtitle('Biological processess relative abundance through \\n treatment per datatype') +\nylab('proportion abundance') +\ntheme_light() + theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_flip() +\ntheme(legend.position=\"none\")\n</code></pre>  <p>Question</p>  <p>What can you conclude from this plot?</p> <p>Heat map of absolute biological process counts per datatype</p> <p><pre><code>bp_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=count)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> Heat map of biological process proportions per datatype <pre><code>bp_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=prop)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> (B) GO Cellular Compartment: relative occurence during treatment <pre><code>cc_slim &lt;- tbl_df(results$CC_GO_slim_abundances)\ncc_slim &lt;- cc_slim %&gt;% mutate(description=as.character(description))\ncc_slim_g &lt;- cc_slim %&gt;%\nselect(matches(\"_G|description\")) %&gt;%\nmutate(DataType=\"Metagenomics\")\ncc_slim_t &lt;- cc_slim %&gt;%\nselect(matches(\"_T|description\")) %&gt;%\nmutate(DataType=\"Metatranscriptomics\")\ncc_slim_a &lt;- cc_slim %&gt;%\nselect(matches(\"_A|description\")) %&gt;%\nmutate(DataType=\"Amplicon\")\n</code></pre> Give meaningful names to conditions as for the taxonomic case above</p> <p><pre><code>colnames(cc_slim_g) &lt;- renameSample(metadata=metadata, localdf=cc_slim_g)\ncolnames(cc_slim_t) &lt;- renameSample(metadata=metadata, localdf=cc_slim_t)\ncolnames(cc_slim_a) &lt;- renameSample(metadata=metadata, localdf=cc_slim_a)\n</code></pre> Calculate the relative occurrence of cellular compartments per day of treatment for both metagenomic and transcriptomic data types. <pre><code>cc_slim_metagenomics &lt;- cc_slim_g %&gt;%\ngather(condition, count, 3:ncol(cc_slim_g)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n\ncc_slim_metatransciptomics &lt;- cc_slim_t %&gt;%\ngather(condition, count, 3:ncol(cc_slim_t)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n</code></pre> Merge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundance of biological process terms that are dynamic during treatment.</p> <pre><code>cc_genomic_transcriptomics &lt;- rbind(cc_slim_metagenomics, cc_slim_metatransciptomics)\n</code></pre> <p>Bar plot of relative abundances of cellular compartment</p> <pre><code>cc_genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=description)) +\ngeom_bar(stat='identity', position='fill', aes(fill = description)) +\nfacet_wrap(~DataType, ncol=1) +\nggtitle('Cellular compartment relative abundance through \\n\ntreatment per datatype') +\nylab('proportion abundance') + theme_light() +\ntheme(axis.text.x=element_text(angle=45, hjust=1)) +\ncoord_flip() + theme(legend.position=\"none\")\n</code></pre>  <p>Question</p>  <p>What can you conclude from this plot?</p> <p>Heat map of absolute cellular compartment term counts per datatype</p> <pre><code>cc_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=count)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> <p>Heat map of Cellular compartment term proportions per datatype</p> <pre><code>cc_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=prop)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> <p>(C) GO Molecular function: relative occurence during treatment</p> <p>Extract the data from the data structure \u201cresults\u201d created earlier.</p> <pre><code>mf_slim &lt;- tbl_df(results$MF_GO_slim_abundances)\nmf_slim &lt;- mf_slim %&gt;% mutate(description=as.character(description))\nmf_slim_g &lt;- mf_slim %&gt;% \n            select(matches(\"_G|description\")) %&gt;% \n            mutate(DataType=\"Metagenomics\")\nmf_slim_t &lt;- mf_slim %&gt;% \n            select(matches(\"_T|description\")) %&gt;% \n            mutate(DataType=\"Metatranscriptomics\") \nmf_slim_a &lt;- mf_slim %&gt;% \n            select(matches(\"_A|description\"))  %&gt;% \n            mutate(DataType=\"Amplicon\")\n</code></pre> <p>Give meaningful names to conditions as for the taxonomic case above</p> <pre><code>colnames(mf_slim_g) &lt;- renameSample(metadata=metadata, localdf=mf_slim_g)\ncolnames(mf_slim_t) &lt;- renameSample(metadata=metadata, localdf=mf_slim_t)\ncolnames(mf_slim_a) &lt;- renameSample(metadata=metadata, localdf=mf_slim_a)\n\nmf_slim_metagenomics &lt;- mf_slim_g %&gt;% \n                        gather(condition, count, 3:ncol(mf_slim_g)-1) %&gt;%\n                        group_by( condition) %&gt;% \n                        mutate(prop=count/sum(count))\n\nmf_slim_metatransciptomics &lt;- mf_slim_t %&gt;% \n                              gather(condition, count, 3:ncol(mf_slim_t)-1) %&gt;%\n                              group_by( condition) %&gt;%\n                              mutate(prop=count/sum(count))\n</code></pre> <p>Merge both metatranscriptomic and metagenomic relative occurrences of biological processes per condition to visualise the relative abundances of biological process terms that are dynamic during treatment.  </p> <pre><code>mf_genomic_transcriptomics &lt;- rbind(mf_slim_metagenomics, mf_slim_metatransciptomics)\n</code></pre> <p>Barplot of relative abundance Molecular function</p> <p><pre><code>mf_genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=description)) +\ngeom_bar(stat='identity', position='fill', aes(fill = description)) +\nfacet_wrap(~DataType, ncol=1) +\nggtitle('Molecular function relative abundance through\n\\n treatment per datatype') +\nylab('proportion abundance') +\ntheme_light() +\ntheme(axis.text.x=element_text(angle=45, hjust=1)) +\ncoord_flip() +\ntheme(legend.position=\"none\")# + guides(fill=guide_legend(ncol=2))\n</code></pre> Heatmap of Absolute Molecular Function term counts through treatment</p> <p><pre><code>mf_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+\ngeom_tile(aes(fill=count)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> Heat map of relative occurrence of molecular function term counts throughout treatment</p> <pre><code>mf_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+\ngeom_tile(aes(fill=prop)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre>  <p>Question</p>  <p>What can you conclude from this plot?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/","title":"emgr part2B","text":""},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#3-interproscan-relative-occurence-during-treatment","title":"3. InterproScan: relative occurence during treatment.","text":"<p>Extract the data from the data structure \u201cresults\u201d created earlier and partition the extracted data per datatype.</p> <pre><code>ipr &lt;- tbl_df(results$IPR_abundances)\nipr&lt;- separateDataType(mydf=ipr, metadata=metadata)\nipr_g &lt;- ipr$genome\nipr_t &lt;- ipr$transcript\nipr_a &lt;- ipr$amplicon\n</code></pre> <p>Calculate the relative occurrence of IPR enrichment per day of treatment for both metagenomic and transcriptomic data types.</p> <p><pre><code>ipr_metagenomics &lt;- ipr_g %&gt;% gather(condition, count, 3:ncol(ipr_g)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\nipr_metatransciptomics &lt;- ipr_t %&gt;% gather(condition, count, 3:ncol(ipr_t)-1) %&gt;%\ngroup_by( condition) %&gt;%\nmutate(prop=count/sum(count))\n</code></pre> Merge both metatranscriptomic and metagenomic relative occurrences of IPR per condition to visualise the relative abundance of IPR terms that are dynamic during treatment. <pre><code>ipr_genomic_transcriptomics &lt;- rbind(ipr_metagenomics, ipr_metatransciptomics)\nipr_genomic_transcriptomics &lt;- ipr_genomic_transcriptomics %&gt;%\nmutate(description=as.character(description), count=log2(count+1))\n</code></pre></p> <p>Bar plot of relative abundance of IPR</p> <pre><code>ipr_genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=description)) +\ngeom_bar(stat='identity', position='fill', aes(fill = description)) +\nfacet_wrap(~DataType, ncol=1) +\nggtitle('IPR relative abundance through treatment\\n per datatype') +\nylab('proportion abundance') +\ntheme_light() +\ntheme(axis.text.x=element_text(angle=45, hjust=1),\naxis.text.y=element_blank()) +\ncoord_flip() +\ntheme(legend.position=\"none\")\n</code></pre> <p>Heat map of absolute InterproScan hit counts throughout treatment</p> <pre><code>ipr_genomic_transcriptomics %&gt;%\nggplot( aes(x=condition,y=description))+ facet_grid(~DataType) +\ngeom_tile(aes(fill=count)) + scale_fill_gradient(low=\"white\", high=\"darkblue\") +\nxlab(\"\") + ylab(\"\") + theme(axis.text.x=element_text(angle=45, hjust=1),\naxis.text.y=element_blank())\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#4-go-all-category-relative-occurence-during-treatment","title":"4. GO (all category): relative occurence during treatment.","text":"<p>The goal here is to create a composite plot of all GO category occurence during treatments. Give meaningful names to conditions as for the taxonomic case</p> <pre><code>GO &lt;- tbl_df(results$GO_abundances)\nGO &lt;- GO %&gt;% mutate(description=as.character(description), category=as.character(category))\n\nGO_g &lt;- GO %&gt;%\nselect(matches(\"_G|description|category\")) %&gt;%\nmutate(DataType=\"Metagenomics\")\nGO_t &lt;- GO %&gt;% select(matches(\"_T|description|category\")) %&gt;%\nmutate(DataType=\"Metatranscriptomics\")\nGO_a &lt;- GO %&gt;% select(matches(\"_A|description|category\")) %&gt;%\nmutate(DataType=\"Amplicon\")\n\n\ncolnames(GO_g) &lt;- renameSample(metadata=metadata, localdf=GO_g)\ncolnames(GO_t) &lt;- renameSample(metadata=metadata, localdf=GO_t)\ncolnames(GO_a) &lt;- renameSample(metadata=metadata, localdf=GO_a)\n</code></pre> <p>Transform the data from a wide format to a long format, making use of the tidyr package function gather, and compute the relative frequency of occurrence per days of treatment</p> <pre><code>GO_metagenomics &lt;- GO_g %&gt;% gather(condition, count, 4:ncol(GO_g)-1) %&gt;%\ngroup_by( condition, category) %&gt;%\nmutate(prop=count/sum(count))\nGO_metatransciptomics &lt;- GO_t %&gt;% gather(condition, count, 4:ncol(GO_t)-1) %&gt;%\ngroup_by( condition, category) %&gt;%\nmutate(prop=count/sum(count))\n</code></pre> <p>Merge both metatranscriptomic and metagenomic relative occurrences of GO per condition to visualise the relative abundance of all GO category terms that are dynamic during treatment.</p> <pre><code>GO_genomic_transcriptomics &lt;- rbind(GO_metagenomics, GO_metatransciptomics)\n</code></pre> <p>Barplot of relative abundance of all GO terms</p> <pre><code>GO_genomic_transcriptomics %&gt;% arrange(desc(prop)) %&gt;%\nggplot( aes(x=condition, y=prop,fill=description)) +\ngeom_bar(stat='identity', position='fill', aes(fill = description)) +\nfacet_grid(category~DataType) +\nggtitle('Gene Ontology relative abundance through days \\n of treatment per datatype') +\nylab('proportion abundance') +\ntheme_light() + theme(axis.text.x=element_text(angle=45, hjust=1)) +\ncoord_flip() +\ntheme(legend.position=\"none\") #+ guides(fill=guide_legend(ncol=2))\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#50-correspondance-analysisis-of-the-contingency-tables","title":"5.0 Correspondance Analysisis of the contingency tables.","text":"<p>The goal of this analysis is to elucidate the relationship between species and GO terms with respect to various treatment conditions (days). We will make use of the variable created above.</p> <pre><code>head(genomic_transcriptomics)\ntaxonomy_long &lt;- genomic_transcriptomics %&gt;% ungroup() %&gt;%\nmutate(condition=paste(DataType, condition, sep=\"_\")) %&gt;%\nselect(phylum, condition, count)\nhead(taxonomy_long)\ntaxonomy_wide &lt;- spread(taxonomy_long, condition, count)\nrownames(taxonomy_wide) &lt;- taxonomy_wide$Phylum\ntaxonomy_wide\ntaxonomy_wide.df &lt;- select(taxonomy_wide, -phylum) %&gt;%\nas.data.frame()\n</code></pre> <p>Transpose the matrix for the purpose of plotting</p> <p><pre><code>taxonomy&lt;- t(taxonomy_wide.df)\n</code></pre> Define a main plot function to set up a plot for later use</p> <p><pre><code>setup.plot &lt;- ggplot() + coord_fixed() +\nlabs(x=\"Comp1, Axis1\", y=\"Comp2, Axis2\") +\ngeom_hline(yintercept=0, col=\"darkgrey\") +\ngeom_vline(xintercept=0, col=\"darkgrey\")\n</code></pre> Make the scree plot in a viewport</p> <p><pre><code>myscree &lt;- function(eigs, x=0.8, y=0.1, just=c(\"right\",\"bottom\")){\nvp &lt;- viewport(x=x, y=y, width=0.2, height=0.2, just=just)\nmm &lt;- data.frame(x=factor(1:length(eigs)), y=eigs)\nsp &lt;- ggplot(mm, aes(x=x, y=y)) + geom_bar(stat=\"identity\") +\nlabs(x = NULL, y = NULL) + theme_light()\nprint(sp, vp=vp)\n}\n</code></pre> Extract various datatype</p> <p><pre><code>taxonomy.dna &lt;- taxonomy_wide.df %&gt;% select(matches('Metagenomics'))\ntaxonomy.rna &lt;- taxonomy_wide.df %&gt;% select(matches('Metatranscriptomics'))\ntaxonomy.dna&lt;- t(taxonomy.dna)\ntaxonomy.rna&lt;- t(taxonomy.rna)\n</code></pre> Read in a file describing the data we loaded previously into R</p> <pre><code>data.dna &lt;- metadata[metadata$datatype==\"Metagenomic\" &amp;\n!(metadata$datatype==\"Amplicon_DNA\"| metadata$datatype==\"Amplicon_RNA\"),]\ndata.rna &lt;- metadata[metadata$datatype==\"Metatranscriptomic\" &amp;\n!(metadata$datatype==\"Amplicon_DNA\"| metadata$datatype==\"Amplicon_RNA\"),]\n</code></pre> <p>Perform a Correspondance analysis on Metagenomic data</p> <pre><code>taxonomy.coa &lt;- dudi.coa(taxonomy.dna, scannf=F, nf=2)\n</code></pre> <p>Plot the correspondence analysis with Ade4 native plotter</p> <pre><code>scatter(taxonomy.coa)\n</code></pre> <p>The plot can be made more visually appealing and easier to interpret with ggplot2, making use of data computed for us by Ade4 (dudi.coa). These are in the object return by the dudi.coa function.</p> <pre><code>taxonomy.dna.plot&lt;- setup.plot + geom_point(data=data.frame(taxonomy.coa$li, data.dna),\naes(x=Axis1, y=Axis2, col=days, shape=days, size=3)) +\ngeom_text(data=taxonomy.coa$co,\naes(x=Comp1, y=Comp2, label=rownames(taxonomy.coa$co)),\nposition = \"jitter\", alpha=0.2) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=F) +\nlabs(title=\"Correspondence Analysis: Metagenomics\\nTaxonomy abundance\") + theme_light()\ntaxonomy.dna.plot\nmyscree(taxonomy.coa$eig / sum(taxonomy.coa$eig))\n</code></pre>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#51-perform-a-correspondence-analysis-on-the-metatranscriptomic-data","title":"5.1 Perform a Correspondence analysis on the Metatranscriptomic data","text":"<pre><code>taxonomy.coa &lt;- dudi.coa(taxonomy.rna, scannf=F, nf=2)\ntaxonomy.rna.plot &lt;- setup.plot + geom_point(data=data.frame(taxonomy.coa$li, data.rna),\naes(x=Axis1, y=Axis2, col=days, shape=days, size=3)) +\ngeom_text(data=taxonomy.coa$co,\naes(x=Comp1, y=Comp2, label=rownames(taxonomy.coa$co)),\nposition = \"jitter\", alpha=0.2) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=F) +\nlabs(title=\"Correspondence Analysis: Metatranscriptomic\\nTaxonomy abundance\") +\ntheme_light()\ntaxonomy.rna.plot\nmyscree(taxonomy.coa$eig / sum(taxonomy.coa$eig))\n</code></pre>  <p>Question</p>  <p>What can you conclude despite the very sparse data?</p> <p>Merge taxonomic and GO_slim_abundances for correspondence analysis._ Organise the taxonomy data</p> <pre><code>taxonomy &lt;- tbl_df(results$phylum_taxonomy_abundances) %&gt;% select(-kingdom)\nphylum &lt;- taxonomy$phylum\ntaxonomy &lt;- taxonomy %&gt;% select(-phylum) %&gt;% t()\ncolnames(taxonomy) &lt;- phylum\ntaxonomy &lt;- taxonomy[!grepl('_A$',rownames(taxonomy)),]\n</code></pre> <p>Organise the ontologies data</p> <pre><code>ontology.meta &lt;- tbl_df(results$GO_slim_abundances) %&gt;%select(GO,description, category)\n</code></pre> <p>Organise the Biological processes data</p> <pre><code>go.bp &lt;- tbl_df(results$BP_GO_slim_abundances) %&gt;%select(-GO, -category)\ndescription.go &lt;- go.bp$description\ngo.bp &lt;- go.bp %&gt;% select(-description) %&gt;% t()\ncolnames(go.bp) &lt;- description.go\n</code></pre> <p>Organise the Molecular function data <pre><code>go.mf &lt;- tbl_df(results$MF_GO_slim_abundances) %&gt;%select(-GO, -category)\ndescription.go &lt;- go.mf$description\ngo.mf &lt;- go.mf %&gt;% select(-description) %&gt;% t()\ncolnames(go.mf) &lt;- description.go\n</code></pre></p> <p>Organise the Cellular compartment data <pre><code>go.cc &lt;- tbl_df(results$CC_GO_slim_abundances) %&gt;%select(-GO, -category)\ndescription.go &lt;- go.cc$description\ngo.cc &lt;- go.cc %&gt;% select(-description) %&gt;% t()\ncolnames(go.cc) &lt;- description.go\n</code></pre></p> <p>Organise the IPR data</p> <pre><code>ipr &lt;- tbl_df(results$IPR_abundances) %&gt;% select(-IPR)\ndescription.ipr &lt;- ipr$description\nipr &lt;- ipr %&gt;% select(-description) %&gt;% t()\ncolnames(ipr) &lt;- description.ipr\n</code></pre> <p>Read in the annotation material</p> <pre><code>metainfo &lt;- metadata\n</code></pre> <p>Filter out the amplicons data</p> <pre><code>metainfo &lt;- metainfo %&gt;% filter(!grepl('_A$', id))\nmetainfo\n</code></pre> <p>Merge the Taxonomy and Molecular Functions and perform Correspondence Analysis (CA) on the merge set</p> <pre><code>mydf &lt;- cbind(taxonomy, go.mf)\nmydf &lt;- mydf[rownames(mydf) %in% metainfo$id,]\n</code></pre> <p>Call on dudi.coa function from Ade4 package to perform CA</p> <pre><code>mydf.coa &lt;- dudi.coa(mydf, scannf=F, nf=2)\n</code></pre> <p>Visualize the CA analysis with ggplot2</p> <pre><code>mydf.plot&lt;- setup.plot +\ngeom_point(data=data.frame(mydf.coa$li, metainfo),\naes(x=Axis1, y=Axis2, col=days, shape=datatype, size=3)) +\ngeom_text(data=mydf.coa$co, aes(x=Comp1, y=Comp2, label=rownames(mydf.coa$co)),\nposition = \"jitter\", alpha=.1) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=T) +\nlabs(title=\"Correspondence Analysis: Metagenomics\\n\nTaxonomy and Molecular Function abundance\") +\ntheme_light()\nprint(mydf.plot)\nmyscree(mydf.coa$eig / sum(mydf.coa$eig))\n</code></pre>  <p>Question</p>  <p>What can you conclude from this analysis?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#52-merge-taxonomy-and-biological-processess-and-perform-correspondence-analysis-ca-on-the-merge-set","title":"5.2 Merge Taxonomy and Biological processess and perform Correspondence Analysis (CA) on the merge set","text":"<p><pre><code>mydf &lt;- cbind(taxonomy, go.bp)\nmydf &lt;- mydf[rownames(mydf) %in% metainfo$id,]\nmydf.coa &lt;- dudi.coa(mydf, scannf=F, nf=2)\n</code></pre> Visualize the CA analysis with ggplot2</p> <pre><code>mydf.plot&lt;- setup.plot +\ngeom_point(data=data.frame(mydf.coa$li, metainfo),\naes(x=Axis1, y=Axis2, col=days, shape=datatype, size=3)) +\ngeom_text(data=mydf.coa$co, aes(x=Comp1, y=Comp2, label=rownames(mydf.coa$co)),\nposition = \"jitter\", alpha=.1) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=F) +\nlabs(title=\"Correspondence Analysis: Metagenomics\\n\nTaxonomy and Biological Processess abundance\") +\ntheme_light()\nprint(mydf.plot)\nmyscree(mydf.coa$eig / sum(mydf.coa$eig))\n</code></pre>  <p>Question</p>  <p>What can you conclude from this analysis?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#53-merge-taxonomy-and-cellular-compartment-and-perform-correspondence-analysis-ca-on-the-merge-set","title":"5.3 Merge Taxonomy and Cellular Compartment and perform Correspondence Analysis (CA) on the merge set","text":"<pre><code>mydf &lt;- cbind(taxonomy, go.cc)\nmydf &lt;- mydf[rownames(mydf) %in% metainfo$id,]\nmydf.coa &lt;- dudi.coa(mydf, scannf=F, nf=2)\n</code></pre> <p>Visualize the CA analysis with ggplot2</p> <pre><code>mydf.plot&lt;- setup.plot +\ngeom_point(data=data.frame(mydf.coa$li, metainfo),\naes(x=Axis1, y=Axis2, col=days, shape=datatype, size=3)) +\ngeom_text(data=mydf.coa$co, aes(x=Comp1, y=Comp2, label=rownames(mydf.coa$co)),\nposition = \"jitter\", alpha=.1) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=T) +\nlabs(title=\"Correspondence Analysis: Metagenomics\\n\nTaxonomy and Cellular Compartment abundance\") +\ntheme_light()\nprint(mydf.plot)\nmyscree(mydf.coa$eig / sum(mydf.coa$eig))\n</code></pre>  <p>Question</p>  <p>What can you conclude from this analysis?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#54-merge-taxonomy-cellular-compartments-and-molecular-functions-then-perform-correspondence-analysis-ca-on-the-merge-set","title":"5.4 Merge Taxonomy, Cellular Compartments and Molecular functions then perform Correspondence Analysis (CA) on the merge set","text":"<p>Merging Taxonomy and Cellular compartment and Molecular function</p> <p><pre><code>mydf &lt;- cbind(taxonomy, go.cc, go.mf)\nmydf &lt;- mydf[rownames(mydf) %in% metainfo$id,]\nmydf.coa &lt;- dudi.coa(mydf, scannf=F, nf=2)\n</code></pre> Visualize the CA analysis with ggplot2</p> <pre><code>mydf.plot&lt;- setup.plot +\ngeom_point(data=data.frame(mydf.coa$li, metainfo),\naes(x=Axis1, y=Axis2, col=days, shape=datatype, size=3)) +\ngeom_density2d(colour = \"red\") +\ngeom_text(data=mydf.coa$co, aes(x=Comp1, y=Comp2, label=rownames(mydf.coa$co)),\nposition = \"jitter\", alpha=.2) +\nscale_size_area(breaks=c(0,3,6,11,14,40)) +\nscale_shape(solid=T) +\nlabs(title=\"Correspondence Analysis: Metagenomics\\n\nTaxonomy, Cellular Compartment and Molecular Function abundance\") +\ntheme_light()\nprint(mydf.plot)\nmyscree(mydf.coa$eig / sum(mydf.coa$eig))\n</code></pre>  <p>Question</p>  <p>What can you conclude from this analysis?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#60-clustering-of-the-principal-component","title":"6.0 Clustering of the Principal Component","text":"<p><pre><code>compositedata &lt;- as.data.frame(mydf)\nrownames(compositedata) &lt;- paste(metainfo$days, metainfo$genomictype, sep=\"_\" )\n</code></pre> Remove columns that only contain zero as they are not contributing to the creation of the component when using FactoMineR</p> <p><pre><code>compositedata&lt;- compositedata[, which(!apply(compositedata,2,FUN=function(x){all(x==0)}))]\n</code></pre> Perform a correspondence analysis using FactoMineR. This automatically generates a perceptual map, showing the relationship between GO terms, species, and days of treatment and various datatypes. <pre><code>ca.analysis &lt;- CA(compositedata)\n</code></pre> Perform a hierarchical clustering of the principal component created above to see the relationship between days of experiment and datatypes. <pre><code>clustering&lt;- HCPC(ca.analysis, nb.clust=-1, order=TRUE)\n</code></pre> <pre><code>plot(clustering)\n</code></pre></p>  <p>Question</p>  <p>What do you conclude from this graph?</p>"},{"location":"modules/metagenomics-module-emgr/emgr-part2B/#61-to-find-out-the-relationship-between-annotationsbpcc-species-we-transpose-the-above-table-prior-to-performing-a-ca","title":"6.1 To find out the relationship between annotations(BP,CC), species we transpose the above table prior to performing a CA.","text":"<pre><code>annoSpecies &lt;- as.data.frame(t(compositedata))\nrownames(annoSpecies) &lt;- rownames(t(compositedata))\n</code></pre> <p>CA</p> <pre><code>anno.ca.analysis &lt;- CA(annoSpecies)\n</code></pre> <p>Perform a hierarchical clustering of the following principal component created above to see the relationship between species and annotation. <pre><code>anno.clustering&lt;- HCPC(anno.ca.analysis, nb.clust=-1, order=TRUE)\n</code></pre></p> <pre><code>plot(anno.clustering)\n</code></pre>  <p>Question</p>  <p>What can you conclude from this clustering?</p> <p>APPENDIX</p> <p>To reproduce this hands on session, your R sessionInfo() must be identical to the following:</p> <pre><code>sessionInfo()\n</code></pre>"},{"location":"modules/metagenomics-module-fda/fda/","title":"Functional Data Analysis","text":""},{"location":"modules/metagenomics-module-fda/fda/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Understand how EMG provides functional analysis of metagenomic data sets</p> </li> <li> <p>Know where to find and how to interpret analysis results for samples on the EMG website</p> </li> <li> <p>Know how to download the raw sample data and analysis results for use with 3rd party visualisation and statistical analysis packages</p> </li> </ul>"},{"location":"modules/metagenomics-module-fda/fda/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/metagenomics-module-fda/fda/#tools-used","title":"Tools Used","text":"<p>Megan6 :  http://ab.inf.uni-tuebingen.de/software/megan6/</p>"},{"location":"modules/metagenomics-module-fda/fda/#useful-links","title":"Useful Links","text":"<p>EBI Metagenomics resource (EMG) :  www.ebi.ac.uk/metagenomics/</p>"},{"location":"modules/metagenomics-module-fda/fda/#introduction","title":"Introduction","text":"<p>The EBI Metagenomics resource (EMG) provides functional analysis of predicted coding sequences (pCDS) from metagenomic data sets using the InterPro database. InterPro is a sequence analysis resource that predicts protein family membership, along with the presence of important domains and sites. It does this by combining predictive models known as protein signatures from a number of different databases into a single searchable resource. InterPro curators manually integrate the different signatures, providing names and descriptive abstracts and, whenever possible, adding Gene Ontology (GO) terms. Links are also provided to pathway databases, such as KEGG, MetaCyc and Reactome, and to structural resources, such as SCOP, CATH and PDB.</p>"},{"location":"modules/metagenomics-module-fda/fda/#what-are-protein-signatures","title":"What are protein signatures?","text":"<p>Protein signatures are obtained by modelling the conservation of amino acids at specific positions within a group of related proteins (i.e., a protein family), or within the domains/sites shared by a group of proteins. InterPro\u2019s different member databases use different computational methods to produce protein signatures, and they each have their own particular focus of interest: structural and/or functional domains, protein families, or protein features, such as active sites or binding sites (see Figure 1).</p>  <p>Figure 1. InterPro member databases grouped by the methods, indicated in white coloured text, used to construct their signatures. Their focus of interest is shown in blue text.</p> <p>Only a subset of the InterPro member databases are used by EMG: Gene3D, TIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were selected since, together, they provide both high coverage and offer detailed functional analysis, and have underlying algorithms that can cope with the vast amounts of fragmentary sequence data found in metagenomic datasets.</p>"},{"location":"modules/metagenomics-module-fda/fda/#assigning-functional-information-to-metagenomic-sequences","title":"Assigning functional information to metagenomic sequences","text":"<p>Whilst InterPro matches to metagenomic sequence sets are informative in their own right, EMG offers an additional type of analysis in the form of Gene Ontology (GO) terms. The Gene Ontology is made up of 3 structured controlled vocabularies that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner. By using GO terms, scientists working on different species or using different databases can compare datasets, since they have a precisely defined name and meaning for a particular concept. Terms in the Gene Ontology are ordered into hierarchies, with less specific terms towards the top and more specific terms towards the bottom (see Figure 2).</p> <p> Figure 2. An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding is a type of cytoskeletal binding, which is a type of protein binding). Note that a GO term can have more than one parent term. The Gene Ontology also allows for different types of relationships between terms (such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only uses the straightforward \u2018is a\u2019 relationships.</p> <p>More information about the GO project can be found http://www.geneontology.org/GO.doc.shtml</p> <p>As part of the EMG analysis pipeline, GO terms for molecular function,biological process and cellular component are associated to pCDS in a sample via the InterPro2GO mapping service. This works as follows: InterPro entries are given GO terms by curators if the terms can be accurately applied to all of the proteins matching that entry. Sequences searched against InterPro are then associated with GO terms by virtue of the entries they match - a protein that matches one InterPro entry with the GO term \u2018kinase activity\u2019 and another InterPro entry with the GO term \u2018zinc ion binding\u2019 will be annotated with both GO terms.</p>"},{"location":"modules/metagenomics-module-fda/fda/#finding-functional-information-about-samples-on-the-emg-website","title":"Finding functional information about samples on the EMG website","text":"<p>Functional analysis of samples within projects on the EMG website www.ebi.ac.uk/metagenomics/ can be accessed by clicking on the Functional Analysis tab found toward the top of any sample page (see Figure 3 below).</p> <p> Figure 3. A Functional analysis tab can be found towards the top of each run page</p> <p>Clicking on this tab brings up a page displaying sequence features (the number of reads with pCDS, the number of pCDS with InterPro matches, etc), InterPro match information and GO term annotation for the sample, as shown in Figure 4 and 5 below.</p> <p>InterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.</p> <p> Figure 4. Functional analysis of metagenomics data, as shown on the EMG website.</p> <p> Figure 5. The GO terms predicted for the sample are displayed. Different graphical representations are available, and can be selected by clicking on the \u2018Switch view\u2019 icons.</p> <p>The Gene Ontology terms displayed graphically on the web site have been \u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO slims are cut-down versions of the Gene Ontology, containing a subset of the terms in the whole GO. They give a broad overview of the ontology content without the detail of the specific fine-grained terms.</p> <p>The full data sets used to generate both the InterPro and GO overview charts, along with a host of additional data (processed reads, pCDS, reads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for further analysis by clicking the Download tab, found towards the top of the page (see Figure 6).</p> <p> Figure 6. Each sample has a download tab, where the full set of sequences, analyses, summaries and raw data can be downloaded.</p>"},{"location":"modules/metagenomics-module-fda/fda/#practical","title":"Practical","text":""},{"location":"modules/metagenomics-module-fda/fda/#browsing-analysed-data-via-the-emg-website","title":"Browsing analysed data via the EMG website","text":"<p>For this session, we are going to look at the Ocean Sampling Day (OSD) 2014 project, which involved simultaneously sampling from geographically diverse oceanographic sites on Solstice 2014. A map of all of the sampling sites is shown on the project page: https://www.ebi.ac.uk/metagenomics/projects/ERP009703</p> <p>To get the OSD project page, either follow the above link or open the Metagenomics Portal home page (https://www.ebi.ac.uk/metagenomics/).</p> <ol> <li>enter \u2018OSD\u2019 in the search box on the top right hand side of the page, and follow the link to project ERP009703. You should now have a Project overview page, which describes the project, submitter contact details, and links to the samples and runs that the project contains.</li> <li>Scroll down to Associated Runs and use the \u2018Filter\u2019 search box to find the OSD80_2014-06-21_0m_NPL022 sample (ERS667582). </li> <li>Click on the Sample Name link (not the Run link) to arrive at the overview page, describing various contextual data, such as the geographic location from which the material was isolated, its collection date, and so on.</li> </ol>  <p>Question 1:</p> <p>What is the latitude, longitude and depth at which the sample was collected?</p>   <p>Question 2:</p> <p>What geographic location does this correspond to?</p>   <p>Question 3:</p> <p>What environmental ontology (ENVO) identifer and name has the sample material been annotated with?</p>  <p>Now scroll down to the \u2018Associated runs\u2019 section of the page. Some samples can have a number of sequencing runs associated with them (for example, corresponding to 16S rRNA amplicon analyses and WGS analyses performed on the same sample). In this case, there is only 1 associated run: ERR770971. Click on the Run ID to go to the Run page.</p> <p>This page has a number of tabs towards the top (Overview, Quality control, Taxonomy analysis, Functional analysis, and Download). Click on the \u2018Download\u2019 tab. Click the file labelled \u2018Predicted CDS\u2019 link to save this file to your computer. Find the file (it should be in your Downloads folder), unzip it and examine it using \u2018less\u2019 by typing the following commands in a terminal window:</p> <p>Click on the \u2018Download\u2019 tab. Right click the file labelled \u2018Predicted CDS (FASTA)\u2019 link, and save this file to your desktop. Find the file, and either double click on it to open it, or examine it using \u2018less\u2019 by typing the following commands in a terminal window:</p> <pre><code>cd ~/Downloads\ngzip \u2013d ERR770971_MERGED_FASTQ_pCDS.faa.gz \nless ERR770971_MERGED_FASTQ_pCDS.faa\n</code></pre> <p>Have a look at one or two of the many sequences it contains. </p> <p>You can count the total number of sequences in the file by grepping the number of header lines that start with \u201c&gt;\u201d</p> <pre><code>grep -c \"^&gt; \"ERR770971_MERGED_FASTQ_pCDS.faa\n</code></pre> <p>In a moment, we will look at the analysis results for this entire batch of sequences, displayed on the EMG website. First, we will attempt to analyse just one of the predicted coding sequences using InterPro (the analysis results on the EMG website summarise these kind of results for hundreds of thousands of sequences).</p> <p>In a new tab or window, open your web browser and navigate to http://www.ebi.ac.uk/interpro/. Copy and paste the following sequence into the text box on the InterPro home page where it says \u2018Analyse your sequence\u2019:</p> <pre><code>&gt;HWI-M02024:110:000000000-A8H0K:1:1101:23198:21331-1:N:0:TCAGAGAC_1_267\nHLLSYRYAYGKFSSTHEATIGGCFLTKDEELDDHIVKYEIWDTAGKNGTIHLPRCTTSKAYXIQVTWYRNAIAAVVVFDVTSRDSFEK\n</code></pre> <p>Press Search and wait for your results. Your sequence will be run through the InterProScan analysis software, which attempts to match it against all of the different signatures in the InterProScan database.</p>  <p>Question 4:</p> <p>Which protein family does InterProScan predict your sequence belongs to, and what GO terms are predicted to describe its function?</p>  <p>Clicking on the InterPro entry name or IPR accession number will take you to the InterPro entry page for your result, where more information can be found.</p> <p>Return to the overview page for ERR770971.</p> <p>Now we are going to look at the functional analysis results for all of the pCDS in the sample. First, we will find the number of sequences that made it through to the functional analysis section of the pipeline.</p> <p>Click on the Quality control tab. This page displays a series of charts, showing how many sequences passed each quality control step, how many reads were left after clustering, and so on.</p>  <p>Question 5:</p> <p>After all of the quality filtering steps are complete, how many reads were submitted for analysis by the pipeline?</p>  <p>Next, we will look at the results of the functional predictions for the pCDS. These can be found under the Functional analysis tab.</p> <p>Click on the Functional analysis tab and examine the InterPro match section. The top part of this page shows a sequence feature summary, showing the number of reads with predicted coding sequences (pCDS), the number of pCDS with InterPro matches, etc.</p>  <p>Question 6:</p> <p>How many predicted coding sequences (pCDS) are in the run?</p>   <p>Question 7:</p> <p>How many pCDS have InterProScan hits?</p>  <p>Scroll down the page to the InterPro match summary section</p>  <p>Question 8:</p> <p>How many different InterPro entries were matched by the pCDS?</p>   <p>Question 9:</p> <p>Why is this figure different to the number of pCDS that have InterProScan hits?</p>  <p>Next we will examine the GO terms predicted by InterPro for the pCDS in the sample.</p> <p>Scroll down to the GO term annotation section of the page and examine the 3 bar charts, which correspond to the 3 different components of the Gene Ontology.</p>  <p>Question 10:</p> <p>What are the top 3 biological process terms predicted for the pCDS from the sample?</p>  <p>Selecting the pie chart representation of GO terms makes it easier to visualise the data to find the answer.</p> <p>Now we will look at the taxonomic analysis for this run.</p> <p>Click on the Taxonomy Analysis tab and examine the phylum composition graph and table.</p>  <p>Question 11:</p> <p>How many of the WGS reads are predicted to encode 16S rRNAs?</p>   <p>Question 12:</p> <p>What are the top 3 phyla in the run, according to 16S rRNA analysis?</p>  <p>Select the Krona chart view of the data icon. This brings up an interactive chart that can be used to analyse data at different taxonomic ranks.</p>  <p>Question 13:</p> <p>What is the proportion of Polaribacter in the population?</p>   <p>Note</p> <p>If the cyanobacteria section of the chart looks strange, this is because the version of GreenGenes used for analysis lists chloroplastic organisms under the cyanobacteria category; some of the cyanobacterial counts are, in fact, derived from photosynthetic eukaryotic organisms.</p>  <p>Now we will compare these analyses with those for a sample taken at 2 m depth from the same geographical location.</p> <p>In a new tab or window, find and open the Ocean Sampling Day (OSD) 2014 project page again. Find the sample OSD80_2014-06-21_2m_NPL022 and examine metadata on the Overview page.</p>  <p>Question 14:</p> <p>Other than sampling depth, what are the differences between this sample and OSD80_2014-06-21_0m_NPL02?</p>  <p>Scroll to the Assocated runs section, and click on ERR770970. Open the Functional analysis tab and examine the Sequence feature and InterPro match summary information for this run.</p>  <p>Question 15:</p> <p>How many pCDS were in this run? </p>   <p>Question 16:</p> <p>How many of the pCDSs have an InterPro match?</p>   <p>Question 17:</p> <p>How many different InterPro entries are matched by this run?</p>   <p>Question 18:</p> <p>Are these figures broadly comparable to those for the previous sample?</p>  <p>Now we are going to look at the differences in slimmed GO terms between the 2 runs. There are to ways to do this. First, you can simply scroll to the bottom of the page and examine the GO term annotation (note - selecting the bar chart representation of GO terms makes it easier to compare different data sets). Alternatively, you can use the comparison tool, which allows direct comparison of runs within a project. The tool can be accessed by clicking on the \u2018Comparison Tool\u2019 tab, illustrated in Figure 6 below. At present, the tool only compares slimmed GO terms, but will be expanded to cover full GO terms, InterPro annotations, and taxonomic profiles as development of the site continues.</p> <p>Click on the Comparison tool tab and choose the Ocean Sampling Day (OSD) 2014 project from the sample list and select the OSD80_2014-06- 21_0m_NPL022 - ERR770971 and OSD80_2014-06-21_2m_NPL022 - ERR770970.</p>  <p>Question 19:</p> <p>Are there visible differences between the GO terms for these runs. Could there be any biological explanation for this?</p>  <p>Navigate back and open the taxonomic analysis results tab for each run.</p>  <p>Question 20:</p> <p>How does the taxonomic composition differ between runs? Are any trends in the data consistent with your answer to question 19?</p>"},{"location":"modules/metagenomics-module-fda/fda/#visualising-taxonomic-data-using-megan","title":"Visualising taxonomic data using MEGAN","text":"<p>Next, we are going to look at the taxonomic predictions for all of the runs. To do this, we are going to load them into MEGAN. MEGAN is a tool suite that provides metagenomic data analysis and visualization. We are going to use only a small subset of its features, relating to taxonomic comparison. Detailed information on MEGAN and the analyses and visualisations it offers can be found here:  <pre><code>http://ab.inf.uni-tuebingen.de/data/software/megan5/download/manual.pdf\n</code></pre></p> <p>MEGAN can be downloaded from  <pre><code>http://ab.inf.uni-tuebingen.de/software/megan6/\n</code></pre> (there are 2 editions Community and Ultimate. The Community edition is open source and free to download. We will use this version for the tutorial). However, it should already be installed on your Desktop.</p> <p>Click on the MEGAN icon on your desktop to load the s/w.</p> <p>We now need to download the full taxonomic predictions for all of the runs in the Ocean Sampling Day project.</p> <p>Navigate to the Project: Ocean Sampling Day (OSD) page, using the breadcrumb URL link at the top of the page. Click on the Analysis summary tab, which will take you to a set of tab separated result matrix files, summarising the taxonomic and functional observations for all runs in the project.</p> <p>Click on the Taxonomic assignments (TSV) link, which will download the corresponding file to your computer. When prompted, choose to save the file in the Downloads folder.</p> <p>Open the Terminal, navigate to the Downloads directory and take a look at the file you have just downloaded (hit q to exit less):</p> <pre><code>cd ~/Downloads\nless -S ERP009703_taxonomy_abundances_v2.0.tsv\n</code></pre> <p>You will see it is a large matrix file, with abundance counts for each taxonomic lineage for each run in the project. From the MEGAN menu, choose \u2018File\u2019 and \u2018Import\u2019. Select \u2018CSV Import\u2019 and then find the file you have just downloaded and edited.</p> <p>From the pop up menu, go with the default options in the Format and Separator sections (which should have the \u2018Class, Count\u2019 option selected under format, and the separator set as \u2018Tab\u2019) and select \u2018Taxonomy\u2019 in classification section, then press \u2018Apply\u2019.</p> <p>There are many different visualisations and comparisons that can be performed using MEGAN, so feel free to explore the data using the tool.</p> <p>Click on the \u2018Show chart\u2019 icon and choose \u2018Stacked Bar Chart\u2019. This should give you a bar chart, showing the abundance reads for taxonomic lineages across all of the sampling sites.</p>  <p>Question 21:</p> <p>Are the number of classified 16S reads roughly equivalent across all of the different sampling sites?</p>   <p>Question 22:</p> <p>Which run appears particularly enriched in Polaribacter?</p>  <p>You can change between abundance counts and relative abundance using the Options drop menu and choosing % Percentage Scale or Linear Scale.</p> <p>Change the chart view to \u2018Bubble Chart\u2019. This visualisation can be useful when comparing a large number of samples.</p>  <p>Question 23:</p> <p>Do any samples contain taxa not found in other samples? </p>  <p>Take a look at the Schlegelella genus.</p>  <p>Question 24:</p> <p>Can you discern any patterns in the geographical distribution of certain species (for example, the cluster of samples enriched for the lactobacillus genus compared to other samples)?</p>  <p>We are now going to take a look at which other datasets in EBI Metagenomics that lactobacilli are found in. Point your browser at https://www.ebi.ac.uk/metagenomics/search/</p> <p>This interface allows you to search the project, sample and run related metadata and analysis results for all of the publicly available datasets in the EBI Metagenomics resource.</p> <p>Click on the \u2018Runs\u2019 tab. You should now see a number of run-related metadata search facets on the left hand side of the page, including \u2018Organism\u2019. Click on the \u2018More\u2026\u2019 option under Organism, scroll down to \u2018Lactobacillus\u2019 in the pop-up window and select the check box next to it. Now click \u2018Filter\u2019. The results page should now show all of the runs that have taxonomic matches to lactobacilli in their datasets. The \u2018Biome\u2019 facet on the left hand side of the page now shows the number of matching datasets in each biome category (to save space, the 10 biome categories with the most matching datasets are shown by default).</p>  <p>Question 25:</p> <p>Which biome category has the most datasets that contain lactobacilli?</p>   <p>Question 26:</p> <p>How well does this correlate with what\u2019s known about these bacteria?</p>  <p>Finally, we will try to use the interface to find functional proteins present under certain environmental conditions. For example, InterPro entry IPR001087 represents a domain found in GDSL esterases and lipases, which are hydrolytic enzymes with multifunctional properties.</p>  <p>Question 27:</p> <p>Using the search interface, can you identify the metagenomics datasets sampled from ocean sites at between 10 and 15 degrees C that contain these enzymes?</p>   <p>Question 28:</p> <p>Can you envisage ways in which this kind of search functionality could be used for target / enzyme discovery?</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/","title":"Data Quality Control","text":""},{"location":"modules/metagenomics-module-qc/ngs-qc/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Assess the overall quality of NGS (FastQ format) sequence reads</p> </li> <li> <p>Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis</p> </li> <li> <p>Clean up adaptors and pre-process the sequence data for further analysis</p> </li> </ul>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/metagenomics-module-qc/ngs-qc/#tools-used","title":"Tools Used","text":"<p>FastQC: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</p> <p>Skewer: http://sourceforge.net/projects/skewer/</p> <p>FASTX-Toolkit: http://hannonlab.cshl.edu/fastx_toolkit/&gt;</p> <p>Picard: http://picard.sourceforge.net/</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#useful-links","title":"Useful Links","text":"<p>FASTQ Encoding: http://en.wikipedia.org/wiki/FASTQ_format#Encoding</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#introduction","title":"Introduction","text":"<p>Going on a blind date with your read set? For a better understanding of the consequences please check the data quality!</p> <p>For the purpose of this tutorial we are focusing only on Illumina sequencing which uses \u2019sequence by synthesis\u2019 technology in a highly parallel fashion. Although Illumina high throughput sequencing provides highly accurate sequence data, several sequence artifacts, including base calling errors and small insertions/deletions, poor quality reads and primer/adapter contamination are quite common in the high throughput sequencing data. The primary errors are substitution errors. The error rates can vary from 0.5-2.0% with errors mainly rising in frequency at the 3\u2019 ends of reads.</p> <p>One way to investigate sequence data quality is to visualize the quality scores and other metrics in a compact manner to get an idea about the quality of a read data set. Read data sets can be improved by pre processing in different ways like trimming off low quality bases, cleaning up any sequencing adapters, removing PCR duplicates and screening for contamination. We can also look at other statistics such as, sequence length distribution, base composition, sequence complexity, presence of ambiguous bases etc. to assess the overall quality of the data set.</p> <p>Highly redundant coverage (&gt;15X) of the genome can be used to correct sequencing errors in the reads before assembly. Various k-mer based error correction methods exist but are beyond the scope of this tutorial.</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#quality-value-encoding-schema","title":"Quality Value Encoding Schema","text":"<p>In order to use a single character to encode Phred qualities, ASCII characters are used (http://shop.alterlinks.com/ascii-table/ascii-table-us.php). All ASCII characters have a decimal number associated with them but the first 32 characters are non-printable (e.g. backspace, shift, return, escape). Therefore, the first printable ASCII character is number 33, the exclamation mark (!). In Phred+33 encoded quality values the exclamation mark takes the Phred quality score of zero.</p> <p>Early Solexa (now Illumina) sequencing needed to encode negative quality values. Because ASCII characters $&lt;$ 33 are non-printable, using the Phred+33 encoding was not possible. Therefore, they simply moved the offset from 33 to 64 thus inventing the Phred+64 encoded quality values. In this encoding a Phred quality of zero is denoted by the ASCII number 64 (the @ character). Since Illumina 1.8, quality values are now encoded using Phred+33.</p> <p>FASTQ does not provide a way to describe what quality encoding is used for the quality values. Therefore, you should find this out from your sequencing provider. Alternatively, you may be able to figure this out by determining what ASCII characters are present in the FASTQ file. E.g the presence of numbers in the quality strings, can only mean the quality values are Phred+33 encoded. However, due to the overlapping nature of the Phred+33 and Phred+64 encoding schema it is not always possible to identify what encoding is in use. For example, if the only characters seen in the quality string are (<code>@ABCDEFGHI</code>), then it is impossible to know if you have really good Phred+33 encoded qualities or really bad Phred+64 encoded qualities.</p> <p>For a graphical representation of the different ASCII characters used in the two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#prepare-the-environment","title":"Prepare the Environment","text":"<p>To investigate sequence data quality we will demonstrate tools called FastQC and Skewer. FastQC will process and present the reports in a visual manner. Based on the results, the sequence data can be processed using the Skewer. We will use one data set in this practical, which can be found in the QC directory on your desktop.</p> <p>Open the Terminal and go to the directory where the data are stored:</p> <pre><code>ls\ncd qc\npwd\n</code></pre> <p>At any time, help can be displayed for FastQC using the following command:</p> <pre><code>fastqc -h\n</code></pre> <p>Look at SYNOPSIS (Usage) and options after typing fastqc -h</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#quality-visualisation","title":"Quality Visualisation","text":"<p>We have a file for a good quality and bad quality statistics. FastQC generates results in the form of a zipped and unzipped directory for each input file.</p> <p>Execute the following command on the two files:</p> <pre><code>fastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz\n</code></pre> <p>View the FastQC report file of the bad data using a web browser such as firefox. The \u2019&amp;\u2019 sign puts the job in the background.</p> <pre><code>firefox qcdemo_R2_fastqc.html &amp;\n</code></pre> <p>The report file will have a Basic Statistics table and various graphs and tables for different quality statistics. E.g.:</p>    Filename qcdemo_R2.fastq.gz     File type Conventional base calls   Encoding Sanger / Illumina 1.9   Total Sequences 1000000   Filtered Sequences 0   Sequence length 150   %GC 37    <p>FastQC Basic Statistics table</p>  <p>Per base sequence quality plot for <code>qcdemo_R2.fastq.gz</code>.</p>  <p>A Phred quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities. The probability that base <code>A</code> is wrong <code>P(sim A)</code> is expressed by a quality score, <code>Q(A)</code>, according to the relationship:</p> <p><code>Q(A) =-10 log10(P(sim A))</code></p> <p>The relationship between the quality score and error probability is demonstrated with the following table:</p>    Quality score Error probability Accuracy of the base call     10 0.1 90%   20 0.01 99%   30 0.001 99.9%   40 0.0001 99.99%   50 0.00001 99.999%    <p>Error probabilities associated with various quality (Q) values</p> <p>[tab:quality_error_probs]</p>  <ol> <li>How many sequences were there in your file? What is the read length?</li> </ol> <p>This is a spoiler: {%s%}Hello World.{%ends%}</p> <pre><code>&gt; 1,000,000. read length=150bp\n</code></pre> <ol> <li> <p>Does the quality score values vary throughout the read length? (hint: look at the \u2019per base sequence quality plot\u2019)</p>  <p>Yes. Quality scores are dropping towards the end of the reads.</p>  </li> <li> <p>What is the quality score range you see?</p>  <p>2-40</p>  </li> <li> <p>At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?</p>  <p>Around 30 bp position</p>  </li> <li> <p>How can we trim the reads to filter out the low quality data?</p>  <p>By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.</p>  </li> </ol>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#good-quality-data","title":"Good Quality Data","text":"<p>View the FastQC report files <code>fastqc_report.html</code> to see examples of a good quality data and compare the quality plot with that of the <code>bad_example_fastqc</code>.</p> <pre><code>firefox qcdemo_R1_fastqc.html &amp;\n</code></pre> <p>Sequencing errors can complicate the downstream analysis, which normally requires that reads be aligned to each other (for genome assembly) or to a reference genome (for detection of mutations). Sequence reads containing errors may lead to ambiguous paths in the assembly or improper gaps. In variant analysis projects sequence reads are aligned against the reference genome. The errors in the reads may lead to more mismatches than expected from mutations alone. But if these errors can be removed or corrected, the read alignments and hence the variant detection will improve. The assemblies will also improve after pre-processing the reads to remove errors.</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#read-trimming","title":"Read Trimming","text":"<p>Read trimming can be done in a variety of different ways. Choose a method which best suits your data. Here we are giving examples of fixed-length trimming and quality-based trimming.</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#quality-based-trimming","title":"Quality Based Trimming","text":"<p>Base call quality scores can be used to dynamically determine the trim points for each read. A quality score threshold and minimum read length following trimming can be used to remove low quality data.</p> <p>The previous FastQC results show R1 is fine but R2 has low quality at the end. There is no adaptor contamination though. We will be using Skewer to perform the quality trimming.</p> <p>Run the following command to quality trim a set of paired end data.</p> <pre><code>cd ~/qc\nskewer -t 20 -l 50  -q 30 -Q 25 -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n</code></pre> <pre><code>-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode\n</code></pre> <p>Run FastQC on the quality trimmed file and visualise the quality scores.</p> <pre><code>fastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html &amp;\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html&amp;\n</code></pre> <p>Let\u2019s look at the quality from the second reads. The output should look like:</p> <p>FastQC Basic Statistics table</p>    Filename qcdemo_R1.fastq-trimmed-pair2.fastq     File type Conventional base calls   Encoding Sanger / Illumina 1.9   Total Sequences 742262   Filtered Sequences 0   Sequence length 50   %GC 37    <p>Per base sequence quality plot for the quality-trimmed <code>qcdemo_R2.fastq.gz</code></p>   <ol> <li>Did the number of total reads in R1 and R2 change after trimming?</li> </ol>  <p>Quality trimming discarded &gt;1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.</p>  <ol> <li>What reads lengths were obtained after quality based trimming?</li> </ol>  <p>50-150 Reads &lt;50 bp, following quality trimming, were discarded.</p>  <ol> <li>Did you observe adapter sequences in the data?</li> </ol>  <p>No. (Hint: look at the overrepresented sequences.</p>  <ol> <li>How can you use -a option with fastqc ? (Hint: try fastqc -h).</li> </ol>  <p>Adaptors can be supplied in a file for screening.</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#adapter-clipping","title":"Adapter Clipping","text":"<p>Sometimes sequence reads may end up getting the leftover of adapters and primers used in the sequencing process. It\u2019s good practice to screen your data for these possible contamination for more sensitive alignment and assembly based analysis.</p> <p>This is particularly important when read lengths can be longer than the molecules being sequenced. For example when sequencing miRNAs.</p> <p>Various QC tools are available to screen and/or clip these adapter/primer sequences from your data. Apart from skewer which will be using today the following two tools are also useful for trimming and removing adapter sequence.</p> <p>Cutadapt: (http://code.google.com/p/cutadapt/) Trimmomatic: (http://www.usadellab.org/cms/?page=trimmomatic)</p> <p>Here we are demonstrating <code>Skewer</code> to trim a given adapter sequence.</p> <pre><code>cd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n</code></pre> <pre><code>-x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n</code></pre> <p>Run FastQC on the adapter trimmed file and visualise the quality scores. Fastqc now shows adaptor free results.</p> <pre><code>fastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html &amp;\n</code></pre> <p>An alternative tool, not installed on this system, for adapter clipping is <code>fastq-mcf</code>. A list of adapters is provided in a text file. For more information, see FastqMcf at http://code.google.com/p/ea-utils/wiki/FastqMcf.</p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#fixed-length-trimming","title":"Fixed Length Trimming","text":"<p>We will not cover Fixed Length Trimming but provide the following for your information. Low quality read ends can be trimmed using a fixed-length trimming. We will use the <code>fastx_trimmer</code> from the FASTX-Toolkit. Usage message to find out various options you can use with this tool. Type <code>fastx_trimmer -h</code> at anytime to display help.</p> <p>We will now do fixed-length trimming of the <code>bad_example.fastq</code> file using the following command. You should still be in the qc directory, if not cd back in.</p> <pre><code>cd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n</code></pre> <p>We used the following options in the command above: <pre><code>-Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name\n</code></pre></p> <p>Run FastQC on the trimmed file and visualise the quality scores of the trimmed file.</p> <pre><code>fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html &amp;\n</code></pre> <p>The output should look like:</p>    Filename bad_example_trimmed01.fastq     File type Conventional base call   Encoding Sanger / Illumina 1.9   Total Sequences 40000   Filtered Sequences 0   Sequence length 80   %GC 48    <p>: FastQC Basic Statistics table</p> <p>  </p> <p>![Per base sequence quality plot for the fixed-length trimmed <code>bad_example.fastq</code> </p> <p>What values would you use for <code>-f</code> if you wanted to trim off 10 bases at the 5\u2019 end of the reads?</p> <p><code>-f 11</code></p>"},{"location":"modules/metagenomics-module-qc/ngs-qc/#removing-duplicates","title":"Removing Duplicates","text":"<p>Duplicate reads are the ones having the same start and end coordinates. This may be the result of technical duplication (too many PCR cycles), or over-sequencing (very high fold coverage). It is very important to put the duplication level in context of your experiment. For example, duplication level in targeted or re-sequencing projects may mean something different in RNA-seq experiments. In RNA-seq experiments oversequencing is usually necessary when detecting low abundance transcripts.</p> <p>The duplication level computed by FastQC is based on sequence identity at the end of reads. Another tool, Picard, determines duplicates based on identical start and end positions in SAM/BAM alignment files.</p> <p>We will not cover Picard but provide the following for your information.</p> <p>Picard is a suite of tools for performing many common tasks with SAM/BAM format files. For more information see the Picard website and information about the various command-line tools available:</p> <p>http://picard.sourceforge.net/command-line-overview.shtml</p> <p>A good list of tools for filtering PCR duplication can also be found at http://omictools.com/duplicate-reads-removal-c495-p1.html</p> <p>Picard is installed on this system in <code>/usr/share/java</code></p> <p>One of the Picard tools (MarkDuplicates) can be used to analyse and remove duplicates from the raw sequence data. The input for Picard is a sorted alignment file in BAM format. Short read aligners such as, bowtie, BWA and tophat can be used to align FASTQ files against a reference genome to generate SAM/BAM alignment format.</p> <p>Interested users can use the following general command to run the MarkDuplicates tool at their leisure. You only need to provide a BAM file for the INPUT argument (not provided):</p> <pre><code>cd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT=&lt;alignment_file.bam&gt; VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true\n</code></pre>"},{"location":"modules/metagenomics-module-tax/tax/","title":"An introduction to taxonomic analysis of amplicon and shotgun data using QIIME","text":""},{"location":"modules/metagenomics-module-tax/tax/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this practical the trainee should be able to:</p> <ul> <li> <p>Understand the open source software package QIIME for analysis</p> </li> <li> <p>Perform a taxonomic analysis on a 16S rRNA amplicon dataset</p> </li> <li> <p>Conduct 16S taxonomic analysis on shotgun data</p> </li> </ul>"},{"location":"modules/metagenomics-module-tax/tax/#resources-you-will-be-using","title":"Resources you will be using","text":""},{"location":"modules/metagenomics-module-tax/tax/#tools-used-part-1-16s-analysis","title":"Tools Used: Part 1. 16S Analysis","text":"<p>QIIME :  http://qiime.org/</p> <p>PEAR: http://sco.h-its.org/exelixis/web/software/pear/doc.html</p> <p>FASTX toolkit (v0.0.14): http://hannonlab.cshl.edu/fastx_toolkit/download.html</p> <p>BBMap: http://sourceforge.net/projects/bbmap</p> <p>VSEARCH: https://github.com/torognes/vsearch</p> <p>SortMeRNA: http://bioinfo.lifl.fr/RNA/sortmerna/</p> <p>STAMP: http://kiwi.cs.dal.ca/Software/STAMP</p>"},{"location":"modules/metagenomics-module-tax/tax/#tools-used-part-2-16s-analysis-of-whole-genome-shotgun-sequencing","title":"Tools Used: Part 2. 16S analysis of whole genome shotgun sequencing","text":"<p>rRNASelector :   http://www.ezbiocloud.net/sw/rrnaselector</p> <p>MEGAN6 : http://ab.inf.uni-tuebingen.de/software/megan6</p>"},{"location":"modules/metagenomics-module-tax/tax/#useful-links","title":"Useful Links","text":"<p>FASTQ Encoding:   </p> <p>http://en.wikipedia.org/wiki/FASTQ_format#Encoding</p> <p>QIIME Tutorials: http://qiime.org/tutorials/tutorial.html</p> <p>16S Tutorials: https://github.com/mlangill/microbiome_helper/wiki/16S-tutorial-(chemerin)</p> <p>Lee et al. (2011). rRNASelector: a computer program for selecting ribosomal RNA encoding sequences from metagenomic and metatranscriptomic shotgun libraries. J. Microbiol. 49(4):689-691.</p>"},{"location":"modules/metagenomics-module-tax/tax/#sources-of-data","title":"Sources of Data","text":""},{"location":"modules/metagenomics-module-tax/tax/#part-1-16s-analysis","title":"Part 1: 16S Analysis","text":"<ul> <li> <p>Data : Mouse gut microbial composition affected by the protein chemerin.  https://www.dropbox.com/s/4fqgi6t3so69224/Sinal_Langille_raw_data.tar.gz https://www.dropbox.com/s/r2jqqc7brxg4jhx/16S_chemerin_tutorial.zip</p> </li> <li> <p>RDP_trainset16_022016.fa (20 MB) is a subset of the Ribosome Database Project (RDP) filtered to include only bacteria. https://www.dropbox.com/s/qnlsv7ve2lg6qfp/RDP_trainset16_022016.fa?dl=1 </p> </li> </ul>"},{"location":"modules/metagenomics-module-tax/tax/#part-2-16s-analysis-of-whole-genome-shotgun-sequencing","title":"Part 2: 16S analysis of whole genome shotgun sequencing","text":"<ul> <li>Li et al. (2013). Draft Genome Sequence of Thermoanaerobacter sp. Strain A7A, Reconstructed from a Metagenome Obtained from a High-Temperature Hydrocarbon Reservoir in the Bass Strait, Australia. Genome Announc. 1(5): e00701-13.</li> </ul>"},{"location":"modules/metagenomics-module-tax/tax/#overview","title":"Overview","text":"<p>In this tutorial we will look at the open source software package QIIME (pronounced \u2019chime\u2019). QIIME stands for Quantitative Insights Into Microbial Ecology. The package contains many tools that enable users to analyse and compare microbial communities. </p> <p>After completion of this tutorial, you should be able to perform a taxonomic analysis on a Illumina pair end 16S rRNA amplicon dataset. In addition you should be able to do 16S rRNA taxonomic analysis on shotgun data using the tool rRNASelector in combination with QIIME and other third party tools.</p>"},{"location":"modules/metagenomics-module-tax/tax/#part-1-16s-analysis_1","title":"Part 1: 16S Analysis","text":""},{"location":"modules/metagenomics-module-tax/tax/#de-novo-otu-picking-and-diversity-analysis-using-illumina-data","title":"De novo OTU picking and diversity analysis using Illumina data","text":""},{"location":"modules/metagenomics-module-tax/tax/#introduction","title":"Introduction","text":"<p>The workflow for 16S analysis in general is as follows:</p> <ol> <li>Split multiplexed reads to samples</li> <li>Join overlapping read pairs</li> <li>Filter reads on quality and length</li> <li>Filter Chimera sequences</li> <li>Assign reads to samples</li> <li>Pick operational taxonomic units (OTUs) for each sample</li> <li>Alpha diversity analysis and rarefaction</li> <li>Beta diversity analysis and Taxonomic composition</li> <li>PCA analysis</li> </ol> <p>16S analysis is a method of microbiome analysis (compared to shotgun metagenomics) that targets the 16S ribosomal RNA gene, as this gene is present in all prokaryotes. It features regions that are conserved among these organisms, as well as variable regions that allow distinction among organisms. These characteristics make this gene useful for analyzing microbial communities at reduced cost compared to metagenomic techniques. A similar workflow can be applied to eukaryotic micro-organisms using the 18S rRNA gene.</p> <p>The tutorial dataset was originally used in a project to determine whether knocking out the protein chemerin affects gut microbial composition. Originally 116 mouse samples acquired from two different facilities were used for this project (only 24 samples were used in this tutorial dataset, for simplicity). </p>"},{"location":"modules/metagenomics-module-tax/tax/#the-mapping-file","title":"The Mapping file","text":"<p>Metadata associated with each sample is indicated in the mapping file (map.txt). The mapping file associates the read data files for a sample to it\u2019s metadata. The mapping file can contain information on your experimental design. The format is very strict; columns are separated with a single TAB character; the header names have to be typed exactly as specified in the documentation. A good sample description is useful as it is used in the legends of the figures QIIME generates.</p> <p>In the mapping (map.txt) file the genotypes of interest can be seen: wildtype (WT), chemerin knockout (chemerin_KO), chemerin receptor knockout (CMKLR1_KO) and a heterozygote for the receptor knockout (HET). Also of importance are the two source facilities: \u201cBZ\u201d and \u201cCJS\u201d. It is important to include as much metadata as possible, so that it can be easily explored later on.</p> <p>Open Terminal and go to the dataset\u2019s directory:</p> <pre><code>cd ~/Desktop/16S_chemerin_tutorial\nls -lhtr\n</code></pre> <p>\u201cfastq\u201d is the directory containing all the sequencing files, which we are going to process. The file \u201cmap.txt\u201d contains metadata about the samples. We can look at it with the less command (hit \u201cq\u201d to exit):</p> <pre><code>less -S map.txt\n</code></pre> <p>The first column is the sample IDs, the next 2 are blank (note the file is tab-delimited, meaning each column is separated by a tab, not just whitespace) and the 4th column contains FASTA filenames (these filenames are based on what we will produce in this pipeline). The rest of the columns are metadata about the samples.</p> <p>Here is what the first 4 lines should look like:</p> <pre><code>SampleID       BarcodeSequence LinkerPrimerSequence    FileInput       Source  Mouse#  Cage#   genotype        SamplingWeek\n105CHE6WT                       105CHE6WT_S325_L001.assembled_filtered.nonchimera.fasta BZ      BZ25    7       WT      wk6\n106CHE6WT                       106CHE6WT_S336_L001.assembled_filtered.nonchimera.fasta BZ      BZ26    7       WT      wk6\n107CHE6KO                       107CHE6KO_S347_L001.assembled_filtered.nonchimera.fasta BZ      BZ27    7       chemerin_KO     wk6\n</code></pre>  <p>Note</p>  <p>The Barcode and LinkerPrimerSequence are absent for this tutorial, these would be used for assigning multiplexed reads to samples and for quality control. Our tutorial data set is already de-multiplexed. What QIIME could be used? </p> <p>Execute the following command and test the mapping file for potential errors:</p> <pre><code>validate_mapping_file.py -m map.txt -o map_output\n</code></pre> <p>There shouldn\u2019t be any errors, but if errors occur a corrected mapping file will be written to the directory map_output</p>"},{"location":"modules/metagenomics-module-tax/tax/#join-directory-of-pe-reads","title":"Join directory of PE reads","text":"<p>First we need to join the overlapping Illumina Pair End (PE) reads contained in the fastq directory for each sample.</p> <pre><code>cd ~/Desktop/16S_chemerin_tutorial  \nrun_pear.pl -p 4 -o stitched_reads fastq/*fastq\n</code></pre> <pre><code>\"-p 4\" indicates this job should be run on 4 CPU \n\"-o stitched_reads\" indicates that the output folder)\n</code></pre> <p>Four FASTQ files will be generated for each set of paired-end reads:     (1) assembled reads (used for downstream analyses)     (2) discarded reads (often abnormal reads, e.g. large run of Ns).     (3) unassembled forward reads     (4) unassembled reverse reads</p> <p>The default log file \u201cpear_summary_log.txt\u201d contains the percent of reads either assembled, discarded or unassembled.</p>  <p>Question 1</p> <p>What percent of reads were successfully stitched for sample 40CMK6WT?</p>"},{"location":"modules/metagenomics-module-tax/tax/#filtering-reads-by-quality-and-length","title":"Filtering reads by quality and length","text":"<p>We will now filter our reads on a quality score cut-off of 30 over 90% of bases and at a maximum length of 400 bp, which are considered reasonable filtering criteria (~2 min on 1 CPU):</p> <pre><code>cd ~/Desktop/16S_chemerin_tutorial\nread_filter.pl -q 30 -p 90 -l 400 -thread 4 -c both stitched_reads/*.assembled*fastq\n</code></pre> <p>The \u201c-c both\u201d option above checks for the default forward and reverse degenerate primer sequences to match exactly in each sequences (You can use whatever primer sequences you want. However, the primer sequences in this case are set by default in the script, which you can look at with \u201cread_filter.pl -h\u201d). If you don\u2019t set the \u201c-c\u201d option to either \u201cboth\u201d or \u201cforward\u201d then there wont be any primer matching.</p> <p>By default this script will output filtered FASTQs in a folder called \u201cfiltered_reads\u201d and the percent of reads thrown out after each filtering step is recorded in \u201cread_filter_log.txt\u201d. This script is just a convenient way to run two popular tools for read filtering: FASTX-toolkit and BBMAP.</p> <p>If you look in this logfile you will note that ~40% of reads were filtered out for each sample. You can also see the counts and percent of reads dropped at each step.</p>  <p>Question 2</p> <p>How many reads of sample 36CMK6WT were filtered out for not containing a match to the forward primer (which is the default setting in this case).</p>"},{"location":"modules/metagenomics-module-tax/tax/#conversion-to-fasta-and-removal-of-chimeric-reads","title":"Conversion to FASTA and removal of chimeric reads","text":"<p>The next steps in the pipeline require the simple conversion of sequences from FASTQ to FASTA format, using the following command (&lt; 1 min on 1 CPU):</p> <pre><code>cd ~/Desktop/16S_chemerin_tutorial\nrun_fastq_to_fasta.pl -p 4 -o fasta_files filtered_reads/*fastq\n</code></pre> <p>Note that this command removes any sequences containing \u201cN\u201d (a fully ambiguous base read), which is &lt;&lt; 1% of the reads after the read filtering steps above.</p> <p>During PCA amplification 16S rRNA sequences from different organisms can sometimes combine to form hybrid molecules called chimeric sequences. It\u2019s important to remove these so they aren\u2019t incorrectly called as novel Operational Taxonomic Units (OTUs). Unfortunately, not all chimeric reads will be removed during this step, which is something to keep in mind during the next steps.</p> <p>You can run chimera checking with VSEARCH with this command (~3 min on 1 CPU):</p> <pre><code>chimera_filter.pl -type 1 -thread 4 -db /home/shared/rRNA_db/Bacteria_RDP_trainset15_092015.fa fasta_files/*fasta\n</code></pre> <p>This script will remove any reads called either ambiguously or as chimeric, and output the remaining reads in the \u201cnon_chimeras\u201d folder by default.</p> <p>By default the logfile \u201cchimeraFilter_log.txt\u201d is generated containing the counts and percentages of reads filtered out for each sample.</p>  <p>Question 3</p> <p>What is the mean percent of reads retained after this step, based on the output in the log file (\u201cnonChimeraCallsPercent\u201d column)?</p>   <p>Question 4</p> <p>What percent of stitched reads was retained for sample 75CMK8KO after all the filtering steps </p>   <p>Hint</p> <p>\u201cyou will need to compare the original number of reads to the number of reads output by chimera_filter.pl)?\u201d</p>"},{"location":"modules/metagenomics-module-tax/tax/#assign-samples-to-the-reads","title":"Assign samples to the reads","text":"<p>Now that we have adequately prepared the reads, we can now run OTU picking using QIIME. An Operational Taxonomic Unit (OTU) defines a taxonomic group based on sequence similarity among sampled organisms. QIIME software clusters sequence reads from microbial communities in order to classify its constituent micro-organisms into OTUs. QIIME requires FASTA files to be input in a specific format (specifically, sample names need to be at the beginning of each header line). We have provided the mapping file (\u201cmap.txt\u201d), which links filenames to sample names and metadata.</p> <p>As we saw the map.txt (e.g. with less -S) had 2 columns without any data: \u201cBarcodeSequence\u201d and \u201cLinkerPrimerSequence\u201d. We don\u2019t need to use these columns today. These would normally be populated for multiplexed data.</p> <p>Also, you will see that the \u201cFileInput\u201d column contains the names of each FASTA file, which is what we need to specify for the command below.</p> <p>This command will correctly format the input FASTA files and output a single FASTA:</p> <pre><code>add_qiime_labels.py -i non_chimeras/ -m map.txt -c FileInput -o combined_fasta \n</code></pre> <p>If you take a look at combined_fasta/combined_seqs.fna you can see that the first column of header line is a sample name taken from the mapping file.</p>"},{"location":"modules/metagenomics-module-tax/tax/#de-novo-otu-picking","title":"De novo OTU picking","text":"<p>Now that the input file has been correctly formatted we can run the actual OTU picking program.</p> <p>Several parameters for this program can be specified into a text file, which will be read in by \u201cpick_open_reference_otus.py\u201d:</p> <pre><code>echo \"pick_otus:threads 1\" &gt;&gt; clustering_params.txt\necho \"pick_otus:sortmerna_coverage 0.8\" &gt;&gt; clustering_params.txt\necho \"pick_otus:sortmerna_db /home/shared/pick_otu_indexdb_rna/97_otus\" &gt;&gt; clustering_params.txt\n</code></pre> <p>We will be using the uclust method of open-reference OTU picking. In open-reference OTU picking, reads are first clustered against a reference database; then, a certain percent (10% in the below command) of those reads that failed to be classified are sub-sampled to create a new reference database and the remaining unclassified reads are clustered against this new database. This de novo clustering step is repeated again by default using the below command (can be turned off to save time with the \u201c\u2013suppress_step4\u201d option).</p> <p>We are actually also retaining singletons (i.e. OTUs identified by 1 read), which we will then remove in the next step. Note that \u201c$PWD\u201d is just a variable that contains the path to your current directory. This command takes ~7 min with 1 CPU. Lowering the \u201c-s\u201d parameter\u2019s value will greatly affect running speed.</p> <pre><code>pick_open_reference_otus.py -i $PWD/combined_fasta/combined_seqs.fna -o $PWD/clustering/ -p $PWD/clustering_params.txt -m sortmerna_sumaclust -s 0.1 -v --min_otu_size 1\n</code></pre>"},{"location":"modules/metagenomics-module-tax/tax/#remove-low-confidence-otus","title":"Remove low confidence OTUs","text":"<p>We will now remove low confidence OTUs, i.e. those that are called by a low number of reads. It\u2019s difficult to choose a hard cut-off for how many reads are needed for an OTU to be confidently called, since of course OTUs are often at low frequency within a community. A reasonable approach is to remove any OTU identified by fewer than 0.1% of the reads, given that 0.1% is the estimated amount of sample bleed-through between runs on the Illumina Miseq:</p> <pre><code>remove_low_confidence_otus.py -i $PWD/clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o $PWD/clustering/otu_table_high_conf.biom\n</code></pre> <p>Since we are just doing a test run with few sequences, the threshold is 1 OTU regardless. However, this is an important step with real datasets. Note: Sequence errors can give rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.</p> <p>We can compare the summaries of these two BIOM files:</p> <pre><code>biom summarize-table -i clustering/otu_table_mc1_w_tax_no_pynast_failures.biom -o clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt\n\nbiom summarize-table -i clustering/otu_table_high_conf.biom -o clustering/otu_table_high_conf_summary.txt\n</code></pre> <p>The first four lines of clustering/otu_table_mc1_w_tax_no_pynast_failures_summary.txt are:</p> <pre><code>Num samples: 24\nNum observations: 2420\nTotal count: 12014\nTable density (fraction of non-zero values): 0.097\nThis means that for the 24 separate samples, 2420 OTUs were called based on 12014 reads. Only 9.7% of the values in the sample x OTU table are non-zero, meaning that most OTUs are in a small number of samples.\n</code></pre> <p>In contrast, the first four lines of clustering/otu_table_high_conf_summary.txt are:</p> <pre><code>Num samples: 24\nNum observations: 884\nTotal count: 10478\nTable density (fraction of non-zero values): 0.193\n</code></pre> <p>After removing low-confidence OTUs, only 36.5% were retained: the number of OTUs dropped from 2420 to 884. This effect is generally even more drastic for bigger datasets. However, the numbers of reads only dropped from 12014 to 10478 (so 87% of the reads were retained). You can also see that the table density increased, as we would expect.</p> <p>The pipeline creates a Newick-formatted phylogenetic tree (*.tre) in the clustering directory. You can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by typing \u2019figtree\u2019 then hit the return key. </p> <p><pre><code>figtree\n</code></pre> View the tree by opening the file \u2019.tre\u2019 in the \u2019clustering\u2019 folder *(Desktop-&gt;Taxonomy-&gt;otus)**. The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 6, which produces a far more useful tree. </p> <p>Megan can be opened from the terminal by typing MEGAN. If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN5_registration_for_academic_use.txt. From the File menu select Import -&gt; BIOM format. Find your biom file and import it.</p> <p>Megan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.</p> <p>The Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.</p>"},{"location":"modules/metagenomics-module-tax/tax/#view-otu-statistics","title":"View OTU statistics","text":"<p>You can generate some statistics, e.g. the number of reads assigned, distribution among samples. Some of the statistics are useful for further downstream analysis, e.g. beta-diversity analysis.  Write down the minimum value under Counts/sample summary. We need it for beta-diversity analysis. You can look at the read depth per sample in clustering/otu_table_high_conf_summary.txt, here are the first five samples (they are sorted from smallest to largest):</p> <pre><code>Counts/sample detail:\n106CHE6WT: 375.0\n111CHE6KO: 398.0\n39CMK6KO: 410.0\n113CHE6WT: 412.0\n108CHE6KO: 413.0\n</code></pre>  <p>Question 5</p> <p>What is the read depth for sample \u201c75CMK8KO\u201d?</p>  <p>We need to subsample the number of reads for each sample to the same depth, which is necessary for several downstream analyses. This is called rarefaction, a technique that provides an indication of species richness for a given number of samples. First it indicates if you have sequence enough to identify all species. Second we want to rarify the read depth of samples to a similar number of reads for comparative analysis. There is actually quite a lot of debate about whether rarefaction is necessary (since it throws out data), but it is still the standard method used in microbiome studies. We want to rarify the read depth to the sample with the lowest \u201creasonable\u201d number of reads. Of course, a \u201creasonable\u201d read depth is quite subjective and depends on how much variation there is between samples.</p>"},{"location":"modules/metagenomics-module-tax/tax/#rarify-reads","title":"Rarify reads","text":"<pre><code>mkdir final_otu_tables\nsingle_rarefaction.py -i clustering/otu_table_high_conf.biom -o final_otu_tables/otu_table.biom -d 355\n</code></pre>"},{"location":"modules/metagenomics-module-tax/tax/#visualize-taxonomic-composition","title":"Visualize taxonomic composition","text":"<p>We will now group sequences by taxonomic assignment at various levels. The following command produces a number of charts that can be viewed in a browser. The command takes about 5 minutes to complete</p> <pre><code>summarize_taxa_through_plots.py -i final_otu_tables/otu_table.biom -o  wf_taxa_summary -m map.txt \n</code></pre> <p>To view the output, open a web browser from the Applications -&gt;Internet menu. You can use Google chrome, Firefox or Chromium. In  Firefox use the File menu to select </p> <pre><code>Desktop -&gt;;Taxonomy -&gt;; wf_taxa_summary -&gt;; taxa_summary_plots \nand open either area_charts.html or bar_chars.html. \n</code></pre> <p>I prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the samples. The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the top) are very useful for discovering how the communities in your samples differ from each other. </p>"},{"location":"modules/metagenomics-module-tax/tax/#alpha-diversity-within-samples-and-rarefaction-curves","title":"Alpha diversity within samples and rarefaction curves","text":"<p>Alpha diversity is the microbial diversity within a sample. QIIME can calculate a lot of metrics, but for our tutorial, we generate 3 metrics from the alpha rarefaction workflow: chao1 (estimates species richness); observed species metric (the count of unique OTUs); phylogenetic distance. The following workflow generates rarefaction plots to visualize alpha diversity.</p> <p>Run the following command from within your taxonomy directory, this should take a few minutes:</p> <pre><code>alpha_rarefaction.py -i final_otu_tables/otu_table.biom -o plots/alpha_rarefaction_plot -t clustering/rep_set.tre --min_rare_depth 40 --max_rare_depth 355 -m map.txt  --num_steps 10\n</code></pre> <p>First we are going to view the rarefaction curves in a web browser by opening the resulting HTML file to view the plots: </p> <pre><code>plots/alpha_rarefaction_plot/alpha_rarefaction_plots/rarefaction_plots.html\n</code></pre> <p>Choose \u201cobserved_otus\u201d as the metric and \u201cSource\u201d as the category. You should see this plot:</p>  <p>There is no difference in the number of OTUs identified in the guts of mice from the BZ facility than the CJS facility, based on this dataset. However, since the rarefaction curves have not reached a plateau, it is likely that this comparison is just incorrect to make with so few reads. Indeed, with the full dataset you do see a difference in the number of OTUs.</p> <p>In general the more reads you have, the more OTUs you will observe. If a rarefaction curve start to flatten, it means that you have probably sequenced at sufficient depth, in other words, producing more reads will not significantly add more OTUs. If on the other hand hasn\u2019t flattened, you have not sampled enough to capture enough of the microbial diversity and by extrapolating the curve you may be able to estimate how many more reads you will need. Consult the QIIME overview tutorial for further information.</p> <p>Run the following command from within your taxonomy directory, this should take a few minutes to generate a heatmap of the level three taxonomy:</p> <pre><code>make_otu_heatmap.py -i final_otu_tables/otu_table_L3.biom -o final_otu_tables/otu_table_L3_heatmap.pdf -c Treatment -m map.txt\n</code></pre>"},{"location":"modules/metagenomics-module-tax/tax/#beta-diversity-and-beta-diversity-plots","title":"Beta diversity and beta diversity plots","text":"<p>Beta diversity analysis, is the assessment of differences between microbial communities/samples. As we have already observed, our samples contain different numbers of sequences. The first step is to remove sample heterogeneity by randomly selecting the same number of reads from every sample. This number corresponds to the \u2019minimum\u2019 number recorded when you looked at the OTU statistics.  Now run the following command</p> <pre><code>beta_diversity_through_plots.py -i final_otu_tables/otu_table.biom -m map.txt -o bdiv_even -t otus/rep_set.tre -e 355\n</code></pre> <p>Good data quality and sample metadata is important for visualising metagenomics analysis. The output of these comparisons is a square matrix where a distance or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering.</p>"},{"location":"modules/metagenomics-module-tax/tax/#testing-for-statistical-differences","title":"Testing for statistical differences","text":"<p>So what\u2019s the next step? Since we know that source facility is such an important factor, we could analyze samples from each facility separately. This will lower our statistical power to detect a signal, but otherwise we cannot easily test for a difference between genotypes.</p> <p>To compare the genotypes within the two source facilities separately we fortunately don\u2019t need to re-run the OTU-picking. Instead, we can just take different subsets of samples from the final OTU table. First though we need to make two new mapping files with the samples we want to subset:</p> <pre><code>head -n 1 map.txt &gt;&gt; map_BZ.txt; awk '{ if ( $3 == \"BZ\" ) { print $0 } }' map.txt &gt;&gt;map_BZ.txt\nhead -n 1 map.txt &gt;&gt; map_CJS.txt; awk '{ if ( $3 == \"CJS\" ) { print $0 } }' map.txt &gt;&gt;map_CJS.txt\n</code></pre> <p>These commands are split into 2 parts (separated by \u201c;\u201d). The first part writes the header line to each new mapping file. The second part is an awk command that prints any line where the 3rd column equals the id of the source facility. Note that awk splits by any whitespace by default, which is why the source facility IDs are in the 3rd column according to awk, even though we know this isn\u2019t true when the file is tab-delimited.</p> <p>The BIOM \u201csubset-table\u201d command requires a text file with 1 sample name per line, which we can generate by these quick bash commands:</p> <pre><code>tail -n +2  map_BZ.txt | awk '{print $1}' &gt; samples_BZ.txt\ntail -n +2  map_CJS.txt | awk '{print $1}' &gt; samples_CJS.txt\n</code></pre> <p>These commands mean that the first line (the header) should be ignored and then the first column should be printed to a new file. We can now take the two subsets of samples from the BIOM file:</p> <p><pre><code>biom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_BZ.txt -o final_otu_tables/otu_table_BZ.biom\nbiom subset-table -i final_otu_tables/otu_table.biom -a sample -s samples_CJS.txt -o final_otu_tables/otu_table_CJS.biom\n</code></pre> We can now re-create the beta diversity plots for each subset:</p> <p><pre><code>beta_diversity_through_plots.py -m map_BZ.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_BZ.biom -o plots/bdiv_otu_BZ \nbeta_diversity_through_plots.py -m map_CJS.txt -t clustering/rep_set.tre -i final_otu_tables/otu_table_CJS.biom -o plots/bdiv_otu_CJS\n</code></pre> We can now take a look at whether the genotypes separate in the re-generated weighted beta diversity PCoAs for each source facility separately.</p> <p>For the BZ source facility:</p>  <p>And for the CJS source facility:</p>  <p>Just by looking at these PCoA plots it\u2019s clear that if there is any difference it\u2019s subtle. To statistically evaluate whether the weighted UniFrac beta diversities differ among genotypes within each source facility, you can run an analysis of similarity (ANOSIM) test. These commands will run the ANOSIM test and change the output filename:</p> <p><pre><code>compare_categories.py --method anosim -i plots/bdiv_otu_BZ/weighted_unifrac_dm.txt -m map_BZ.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_BZ.txt \n</code></pre> <pre><code>compare_categories.py --method anosim -i plots/bdiv_otu_CJS/weighted_unifrac_dm.txt -m map_CJS.txt -c genotype -o beta_div_tests\nmv beta_div_tests/anosim_results.txt  beta_div_tests/anosim_results_CJS.txt\n</code></pre></p> <p>You can take a look at the output files to see significance values and test statistics. The P-values for both tests are &gt; 0.05, so there is no significant difference in the UniFrac beta diversities of different genotypes within each source facility.</p>"},{"location":"modules/metagenomics-module-tax/tax/#unifrac-beta-diversity-analysis","title":"UniFrac beta diversity analysis","text":"<p>UniFrac is a particular beta-diversity measure that analyzes dissimilarity between samples, sites, or communities. We will now create UniFrac beta diversity (both weighted and unweighted) principal coordinates analysis (PCoA) plots. PCoA plots are related to principal components analysis (PCA) plots, but are based on any dissimilarity matrix rather than just a covariance/correlation matrix. </p> <p>Note the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then it\u2019s likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).</p> <p>QIIME \u201cbeta_diversity_through_plots.py\u201d takes the OTU table as input, as well as file which contains the phylogenetic relatedness between all clustered OTUs. One HTML file will be generated for the weighted and unweighted beta diversity distances:</p> <pre><code>plots/bdiv_otu/weighted_unifrac_emperor_pcoa_plot/index.html\nplots/bdiv_otu/unweighted_unifrac_emperor_pcoa_plot/index.html\n</code></pre> <p>Open the weighted HTML file in your browser and take a look, you should see a PCoA very similar to this:</p>  <p>The actual metadata we are most interested in for this dataset is the \u201cgenotype\u201d column of the mapping file, which contains the different genotypes I described at the beginning of this tutorial. Go to the \u201cColors\u201d tab of the Emperor plot (which is what we were just looking at) and change the selection from \u201cBarcodeSequence\u201d (default) to \u201cgenotype\u201d. You should see a similar plot to this:</p>  <p>The WT genotype is spread out across both knockout genotypes, which is not what we would have predicted.</p> <p>You\u2019ll see what\u2019s really driving the differences in beta diversity when you change the selection under the \u201cColors\u201d tab from \u201cgenotype\u201d to \u201cSource\u201d:</p>"},{"location":"modules/metagenomics-module-tax/tax/#using-stamp-to-test-for-particular-differences","title":"Using STAMP to test for particular differences","text":"<p>Often we\u2019re interested in figuring out which particular taxa (or other feature such as functions) differs in relative abundance between groups. There are many ways this can be done, but one common method is to use the easy-to-use program STAMP. We\u2019ll run STAMP on the full OTU table to figure out which genera differ between the two source facilities as an example.</p> <p>Before running STAMP we need to convert our OTU table into a format that STAMP can read:</p> <pre><code>biom_to_stamp.py -m taxonomy final_otu_tables/otu_table.biom &gt;final_otu_tables/otu_table.spf\n</code></pre> <p>If you take a look at \u201cfinal_otu_tables/otu_table.spf\u201d with less you\u2019ll see that it\u2019s just a simple tab-delimited table.</p> <p>Now we\u2019re ready to open up STAMP, which you can either do by typing STAMP on the command-line or by clicking the side-bar icon.</p> <p>Load \u201cotu_table.spf\u201d as the Profile file and \u201cmap.txt\u201d as the Group metadata file.</p> <p>As a reminder, the full paths of these files should be: <pre><code>/home/mh_user/Desktop/16S_chemerin_tutorial/final_otu_tables/otu_table.spf and /home/mh_user/Desktop/16S_chemerin_tutorial/map.txt\n</code></pre></p> <p>Change the Group field to \u201cSource\u201d and the profile level to \u201cLevel_6\u201d (which corresponds to the genus level). Change the heading from \u201cMultiple groups\u201d to \u201cTwo groups\u201d. The statistical test to \u201cWelch\u2019s t-test\u201d and the multiple test correction to \u201cBenjamini-Hochberg FDR\u201d</p> <p>Change the plot type to \u201cBar plot\u201d. Look at the barplot for Prevotella and save it to a file.</p>  <p>Question</p> <p>Can you see how many genera are significant by clicking \u201cShow only active features\u201d?</p>"},{"location":"modules/metagenomics-module-tax/tax/#part-2-16s-analysis-of-whole-genome-shotgun-sequencing_1","title":"Part 2: 16S analysis of whole genome shotgun sequencing","text":""},{"location":"modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-of-16s-ribosomal-rrna-fragments-selected-from-a-shotgun-data-set","title":"Closed reference OTU picking of 16S ribosomal rRNA fragments selected from a shotgun data set","text":"<p>In a closed-reference OTU picking process, reads are clustered against a reference sequence collection and any reads, which do not hit a sequence in the reference sequence collection, are excluded from downstream analyses. In QIIME, pick_closed_reference_otus.py is the primary interface for closed-reference OTU picking in QIIME. If the user provides taxonomic assignments for sequences in the reference database, those are assigned to OTUs. We could use this approach to perform taxonomic analysis on shotgun data. We need to perform the following steps:</p> <ol> <li>Extract those reads from the data set that contain 16S ribosomal RNA sequence. If there are less than (e.g.) 100 nucleotides of rRNA sequence, the read should be discarded.</li> <li>Remove non-rRNA sequence (flanking regions) from those reads</li> <li>Run closed-reference OTU picking workflow</li> <li>Visualise the results, e.g. in Megan</li> </ol>"},{"location":"modules/metagenomics-module-tax/tax/#extraction-of-16s-rrna-sequence-containing-reads-with-rrnaselector","title":"Extraction of 16S rRNA sequence-containing reads with rRNASelector","text":"<p>We will analyze an Illumina paired-end dataset that has been drastically reduced in size for this tutorial, while preserving the majority of the 16S containing reads. The dataset is from the metagenome described at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772140/. There is a pdf in the working directory for this part of the tutorial. This is a paired end dataset, and where read pairs overlapped, they were merged into a single sequence. If read pairs did not overlap, both reads were included in the analysis. QC was performed using the EBI Metagenomics pipeline. We will use a tool called rRNASelector, which is freely available (http://www.ncbi.nlm.nih.gov/pubmed/21887657) to select our 16S rRNA sequence containing reads. The tool invokes hmmsearch and uses trained hidden Markov models to detect reads with 16S rRNA sequence. The tool also trims the reads so that only 16S rRNA sequence is present in the fasta file we will feed into the QIIME workflow.</p> <p>First, we need to go to our working directory. You will find a file called A7A-paired.fasta containing the sequence reads. Fire up rRNASelector from the command line.</p> <pre><code>cd ~/Desktop/Taxonomy/A7A/\nrRNASelector\n</code></pre> <p>A graphical interface should appear. Note interaction with the interface may have a few seconds lag. </p> <ol> <li>Load the sequence file by clicking on \u2019File Choose\u2019 at the top and navigate to the file A7A-paired.fasta. </li> <li>Select the file and click \u2019Open\u2019. The tool will automatically fill in file names for the result files.</li> <li>Change the Number of CPUs to \u20194\u2019 </li> <li>Select Prokaryote 16S (to include both bacterial and archaeal 16S sequences) </li> <li>Specify the location of the hmmsearch file by clicking the second \u2019File Choose\u2019 button. Type in manually the location \u2018/usr/bin/hmmsearch\u2019, </li> <li>Then click process. The run should take a few minutes to complete.</li> </ol> <p>If all went well, you can close rRNASelector by clicking on Exit. You will have 3 new files in your directory, one containing untrimmed 16S reads, one containing trimmed 16S reads (A7A-paired.prok.16s.trim.fasta; that\u2019s the one we want) and a file containing reads that do not contain (sufficient) 16S sequence.</p>"},{"location":"modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-workflow-and-visualization-of-results-in-megan-6","title":"Closed-reference OTU picking workflow and visualization of results in Megan 6","text":"<p>We are now ready to pick our OTUs. We do that by running the following command (all on one line and no space after gg_otus-12-10):</p> <pre><code>pick_closed_reference_otus.py -i A7A-paired.prok.16s.trim.fasta -o ./cr_uc -r /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/rep_set/97_otus.fasta -t /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/taxonomy/97_otu_taxonomy.txt\n</code></pre> <p>We need to specify the following options. The command will take several minutes to run. When finished open Megan as described before, import the otu_table.biom file and explore the results.</p> <pre><code>-i input_file.fasta\n-o output_directory\n-r /path/to/reference_sequences\n-t /path/to/reference_taxonomy\n</code></pre>"},{"location":"modules/metagenomics-module-tax/tax/#bonus","title":"Bonus","text":"<p>The QIIME overview tutorial at (http://qiime.org/tutorials/tutorial.html) has a number of additional steps that you may find interesting; so feel free to try some of them out. Note hat we have not installed Cytoscape, so we cannot visualize OTU networks.</p> <p>We will end this tutorial with a summary of what we have done and how well our analysis compares with the one in the paper.</p> <p>Hopefully you will have acquired new skills that allow you to tackle your own taxonomic analyses. There are many more tutorials on the QIIME website that can help you pick the best strategy for your project (http://qiime.org/tutorials/) and https://github.com/mlangill/microbiome_helper/wiki/. There are alternatives that might suit your need better (e.g. VAMPS at http://vamps.mbl.ed&gt;; mothur at http://www.mothur.org) and others.</p>"},{"location":"modules/metagenomics-module-vis/vis/","title":"Analysis Visualisation","text":""},{"location":"modules/metagenomics-module-vis/vis/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Visualise between sample comparisons, using Emperor PCA</p> </li> <li> <p>Understand the difference between weighted and unweighted analysis</p> </li> </ul>"},{"location":"modules/metagenomics-module-vis/vis/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/metagenomics-module-vis/vis/#tools-used","title":"Tools Used","text":"<p>Emperor:  http://qiime.org/emperor/tutorial_index.html</p>"},{"location":"modules/metagenomics-module-vis/vis/#data-sets","title":"Data sets","text":"<p>Sutton et al. (2013). Impact of Long-Term Diesel Contamination on Soil Microbial Community Structure:  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553749/pdf/zam619.pdf</p> <p>Fierer et al. (2010). Forensic identification using skin bacterial communities:  http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2852011/</p>"},{"location":"modules/metagenomics-module-vis/vis/#introduction","title":"Introduction","text":"<p>Good data quality and sample metadata is important for visualising metagenomics analysis.</p> <p>For this tutorial we are using Emperor a browser enabled scatter plot visualisation tool. We will be using the Sutton and Fierer data sets for viewing the principal components analysis (PCoA) from the QIIME Beta diversity analysis (16S).</p>"},{"location":"modules/metagenomics-module-vis/vis/#beta-diversity-computation-and-plots","title":"Beta Diversity computation and plots","text":"<p>Beta diversity represents between sample comparisons based on their composition. The output of these comparisons is a square matrix where a \u201cdistance\u201d or dissimilarity is calculated between every pair of community samples, reflecting the dissimilarity between those samples. The data distance matrix can be then visualized with analyses such as PCoA and hierarchical clustering. PCoA is a technique that maps the samples in the distance matrix to a set of axes that show the maximum amount of variation explained. The principal coordinates can be plotted in two or three dimensions to provide an intuitive visualization of the data structure to look at differences between the samples, and look for similarities by sample conditions.</p> <p>The Beta diversity workflow you ran earlier contained a number of steps:</p> <ul> <li> <p>Rarifying the OTU table to remove sample heterogeneity. A rarified     OTU table should be used so that artificial diversity induced due to     different sampling effort is removed.</p> </li> <li> <p>Calculating the Beta diversity, the unifrac weighted and unweighted     generated principal coorodinates are created here by default.</p> </li> </ul> <p>The following are the required options and inputs for Beta diversity analysis:</p> <pre><code>1.  Option -i, OTU table (*.biom)\n2.  Option \u2013t, phylogeny tree (*.tre) from pick_de_novo_otus.py\n3.  Option \u2013m, the user-defined mapping file\n4.  Option \u2013o, the output directory\n5.  Option \u2013e, the number of sequences per sample (sequencing depth)\n</code></pre> <p>You do not need to run the following command, it may overwrite the pre-computed analysis. This is the command you ran on the Sutton sub dataset.</p> <pre><code>beta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o wf_bdiv_even122/ -t otus/rep_set.tre -e 122\n</code></pre>"},{"location":"modules/metagenomics-module-vis/vis/#prepare-the-environment","title":"Prepare the environment","text":"<p>The data for this practical can be found in the Taxonomy directory on your desktop. Go to the pre-computed Beta diversity analysis in the Taxonomy directory.</p> <p>Open the Terminal and go to where the data is stored. Investigate the directories available.</p> <pre><code>cd ~/Desktop/Taxonomy/sutton_full_denoised/\nls \u2013lhtr\n</code></pre> <p>Open the following files: wf_bdiv_even394/unweighted_unifrac_pc.txt and wf_bdiv_even394/weighted_unifrac_pc.txt using the \u2018less\u2019 command. The option \u2013S with \u2018less\u2019 allows you to view files with unwrapped lines, to escape type q (for quit). To scroll left and right, use the arrow keys.</p> <pre><code>less \u2013S wf_bdiv_even394/unweighted_unifrac_pc.txt\nless \u2013S wf_bdiv_even394/weighted_unifrac_pc.txt\n</code></pre> <p>In each file every sample is listed in the first column, and the subsequent columns contain the value for the sample against the noted principal coordinate. At the bottom of each Principal Coordinate column, you will find the eigen value and percent of variation explained by the coordinate. Note the major difference between weighted and unweighted analysis is the inclusion of OTU abundance when calculating distances between communities. You should use weighted if the biological question you are trying to ask takes OTU abundance of your groups into consideration. If some samples are forming groups with weighted, then likely the larger or smaller abundances of several OTUs are the primary driving force in PCoA space, but when all OTUs are considered at equal abundance, these differences are lost (unweighted).</p> <p>Note discrete vs. continuous only has to do with coloring of the points, and not the calculation of UniFrac distances.</p>"},{"location":"modules/metagenomics-module-vis/vis/#emperor","title":"Emperor","text":"<p>We can now run Emperor to prepare the PCoA plots for visualisation.</p> <pre><code>make_emperor.py -i wf_bdiv_even394/unweighted_unifrac_pc.txt -m sutton_mapping_file.txt \u2013b \"Source,Condition,Source&amp;&amp;Condition\" \u2013o emporer_output_unweighted\n\nls \u2013l emporer_output_unweighted\n</code></pre> <p>Run the last make_emporer.py command with the weighted_unifrac, using the same command. Make sure you rename the output directory to \u201cemporer_output_weighted\u201c. Look in the Emporer output directories and note the index.html file outputs.</p> <p>To view the index.html files created from the \u2018make_emperor.py\u2019 steps above a browser like Firefox or Chrome can be used.</p> <p>Important - To view the Emporer plots we need to switch from the VM to your local machine. Open the Firefox browser and view the pre-computed Emporer directories available at the following URL. http://www.ebi.ac.uk/~sterk/emperor/</p> <p>View the unweighted analysis on the Sutton sub data set:</p> <p>http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_unweighted/</p> <p>Check the \u2018Colors\u2019 options to view your samples. What can you determine from the PCoA plot?</p> <p>To change the colouring scheme click the \u2018Colors\u2019 tab. The available metadata categories are sorted in alphabetical order. Extra information on the features can be found at http://qiime.org/emperor/description_index.html Try changing the scaling, visibility, label etc. to improve the display. Note that Emperor generates publication quality figures in scalable vector graphics (SVG) format.</p> <p>Now look at the weighted unifrac analysis, http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_weighted/</p> <p>You can compare the sub data analysis against the full-denoised dataset. </p> <p>What are the main differences between the weighted and unweighted plots?</p> <p>Did you notice any differences between the analysis on the sub data set and the full denoised?</p> <p>What are your final conclusions on the Sutton dataset?</p> <p>What did each of the visualisation methods describe?</p> <p>Taxonomy summary plots:</p> <p>Alpha diversity plots:</p> <p>Beta diversity PCoA plots:</p> <p>Now that you are familiar with Emperor we will next view the forensic analysis of the Fierer et. al. data. This experiment matches the bacteria on surfaces to the skin-associated bacteria of the individual who touched the surface. This study shows that skin-associated bacteria can be readily recovered from surfaces (including single computer keys and computer mice) and that the structure of these communities can be used to differentiate objects handled by different individuals, even if those objects have been left untouched for up to 2 weeks at room temperature.</p> <p>Please look the information contained in the mapping file and identify those fields that can be plotted.</p> <pre><code>cd ~/Desktop/Taxonomy/fierer/\nless \u2013S fierer_mapping_file.txt\n</code></pre> <p>List any fields that can be plotted for this study. What is the remainder of the information for?</p> <p>Run Emperor to prepare the PCoA plots for visualisation</p> <pre><code>make_emperor.py -i fierer_unweighted_unifrac_pc.txt -m fierer_mapping_file.txt -b \\ \"HOST_SUBJECT_ID,ENV_FEATURE,HOST_SUBJECT_ID&amp;&amp;ENV_FEATURE\u201d -o emporer_output_unweighted\n</code></pre> <p>Go to http://www.ebi.ac.uk/~sterk/emperor/fierer_emporer_output_unweighted/</p> <p>What did you observe about the clustering?</p> <p>What fields have been selected?</p> <p>The on-line Qiime tutorials are very good and growing in numbers. It is highly recommended checking out in future the many analysis options.</p>"},{"location":"modules/metagenomics-module-wga/wga/","title":"Whole Genome Analysis","text":""},{"location":"modules/metagenomics-module-wga/wga/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>After completing this module the trainee should be able to:</p> <ul> <li> <p>Understand the main approaches to perform metagenomics assembly</p> </li> <li> <p>Be able to perform assembly on your data and assess the quality of your assembly</p> </li> </ul>"},{"location":"modules/metagenomics-module-wga/wga/#resources-youll-be-using","title":"Resources You\u2019ll be Using","text":""},{"location":"modules/metagenomics-module-wga/wga/#tools-used","title":"Tools Used","text":"<p>Meta-Velvet:  https://github.com/hacchy/MetaVelvet</p>"},{"location":"modules/metagenomics-module-wga/wga/#useful-links","title":"Useful Links","text":"<p>Meta-Velvet:  http://metavelvet.dna.bio.keio.ac.jp/</p>"},{"location":"modules/metagenomics-module-wga/wga/#introduction","title":"Introduction","text":"<p>Performing genomic assembly aims at generating a genome-length sequence using the sequence information obtained from short reads. In the case of metagenomics sample, the task is complicated by the number of different genomes present in the sample and the fact that their sequences are sometimes very similar to each other. There are two main approaches to perform de novo assembly (genomic or metagenomic): building a consensus and generating De Bruijn k-mer graph.</p> <p>The k-mers represent the nodes of the de Bruijn graph. Nodes are linked together if they overlap by k-1 nucleotides. Determining the correct k-mer is important. You can use tools such as Velvet Advisor: http://dna.med.monash.edu.au/~torsten/velvet_advisor/</p> <p>Building a de Bruijn graph is computationally demanding but navigation through it to identify path (to generate contigs of continuous sequences) is quick and memory efficient. Ideally other information, biological or distance-based, would be used to help build the contigs.</p>"},{"location":"modules/metagenomics-module-wga/wga/#assessing-the-quality-of-an-assembly","title":"Assessing the quality of an assembly","text":"<p>For genomic assembly, the accepted criterion of assembly quality is the number of contigs obtained: the lower this number, the longer the contigs and therefore the higher genome reconstitution. This number is often expressed as N50, which is defined as the weighted median such that 50% of the entire assembly is contained in contigs equal to or larger than this value. It is calculated by ranking the contigs by decreasing length and adding their size sequentially until 50% of the total number of nucleotides is reached: the N50 is defined by the number of contigs included in this sum. The N50 is also generally used for metagenomics</p>"},{"location":"modules/metagenomics-module-wga/wga/#practical","title":"Practical","text":"<p>The purpose of this exercise is to perform an assembly using Meta-Velvet and illustrate how k-mer choices influence the output quality. The starting dataset will be a metagenomic dataset. To ensure better assembly, rather than using the raw reads, we will only assemble the sequences having passed the EMG QC steps. Meta-Velvet is an extension of Velvet, a popular genomic assembler. Therefore to perform our assembly, we will first run Velvet and then Meta-Velvet using the Velvet output as input. Both programs are run from the command line.</p>"},{"location":"modules/metagenomics-module-wga/wga/#investigation-of-the-input-sequence-file","title":"Investigation of the input sequence file","text":"<p>Open a terminal window (Applications/Accessories/Terminal, a link is also provided on your desktop) and navigate to the \u201cdata\u201d folder in \u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The file is in fasta format and contains sequences of at least 100 nt each</p> <pre><code>cd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c \"&gt;\" A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta\n</code></pre> <p>The output indicates that the input file contains about 2.2 billion (2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212) sequences. The average sequence length was 103.2 nucleotides. The sequence file also contains 12,942 \u201cN\u201d indicating that some sequences have ambiguous bases. In addition, the script displays the N50 to N100 values: N50 = 101, n = 10256045 for example means that a cumulated sum of, at least, half of the total nucleotides is reached after adding the length of 10,256,045 sequences and that the last sequence added had a length of 101 nucletotides.</p>"},{"location":"modules/metagenomics-module-wga/wga/#performing-the-assembly-using-velvet-and-meta-velvet","title":"Performing the assembly using Velvet and Meta-Velvet","text":"<p>Velvet and Meta-Velvet had already been installed on your computer. However they need to be configured by indicating the k-mer value and the number of read categories to use. We already have seen what k-mer is (reminder in page 5 above). The number of read categories is the maximum number of libraries of different insert lengths. As in our case the reads are all coming from the same library, we will use the default value (2). The version of Velvet and MetaVelvet installed on the virtual machine you will be using as already been configured with k-mer = 59.</p> <p>We will run the applications from the AssemblyTutorial folder. The software has been installed in your path so no need to copy/link these files:</p> <pre><code># first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta\n</code></pre> <p>Then run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d parameter indicates to the software that the sequence coverage is considered uniform across the submitted set and that the expected coverage (i.e. number of reads per sequence) should be the median coverage value:</p> <pre><code>velvetg A7A-59 \u2013exp_cov auto\n</code></pre> <p>Finally run meta-velvetg to generate the assembly output in the A7A-59 directory:</p> <pre><code>meta-velvetg A7A-59 | tee logfile\n</code></pre>"},{"location":"modules/metagenomics-module-wga/wga/#assessing-the-quality-of-the-assembly","title":"Assessing the quality of the assembly","text":"<p>The main assembly output is a list of contigs provided as a fasta file. We will know look at these in more details. First we need to navigate to the output directory:</p> <p>Finally run meta-velvetg to generate the assembly output in the A7A-59 directory. We can obtain the number of contigs by running the function grep to only count the lines containing the contig names (identified by \u201c&gt;\u201d). Then run the stats script, seen earlier, to also obtain the N50 value:</p> <pre><code>cd A7A-59\ngrep -c \"&gt;\" meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa\n</code></pre> <p>It shows that the sequences had been assembled in 9,182 contigs of an average length equal to about 1,230 nucleotides. The longest contigs contains 95,305 nucleotides. The N50 line indicates that half of the nucleotides are comprised in the first 275 longest contigs and that the 275th contigs is 9,145 nucleotides long. Comparing these stats to the one obtained before assembly also reveal that the number of nucleotides involved in the assembly represents only slightly more than 0.5% of the nucleotides submitted. This reflects, of course, the amount of overlapping sequences identified by Velvet and MetaVelvet. This also explains the reduction of the number of ambiguous base (N_count).</p> <p>Changing the k-mer value can have a dramatic effect on the quality of the assembly. Reducing the k-mer to 31 for example yields the following statistics:</p> <pre><code>sum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263\n</code></pre> <p>The number of contigs is almost 5 times larger than with k-mer equal 59. However, increasing the k-mer alone does not systematically lead to better stats. A k-mer of 63 produces an assembly with higher number of contigs (Note that the N50 value is also increased):</p> <pre><code>sum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850\n</code></pre> <p>Without extra information, it could be challenging, using the N50 parameter, to judge the quality of a metagenomics assembly. We could use blast or other tools to infer taxonomy to different sections of the contigs: obtaining similar affiliation for all fragments originating from the same contigs would be indicative of a good assembly.</p>"},{"location":"timetables/timetable_cancer/","title":"Cancer Genomics Course","text":"<p>University of Sydney - 25th to 27th July 2017</p>"},{"location":"timetables/timetable_cancer/#instructors","title":"Instructors","text":"<ul> <li>Katherine Champ (KC)- Bioplatforms Australia, Sydney</li> <li>Ann-Marie Patch (AMP) - QIMR Berghofer, Brisbane</li> <li>Gayle Philip (GP) - Melbourne Bioinformatics, Melbourne</li> <li>Erdahl Teber (ET) - CMRI, Sydney</li> <li>Sonika Tyagi (ST) - AGRF, Melbourne</li> </ul>"},{"location":"timetables/timetable_cancer/#timetable","title":"Timetable","text":""},{"location":"timetables/timetable_cancer/#day-1","title":"Day 1","text":"<p>25th July - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     9:00 Welcome  KC   9:15 Introduction to cancer genomics and NGS techniques \u2013 focus on DNA  AMP   10:15 Break     10:30 Experimental design (interactive/ice breaker/group activity) and caveats; considerations on processing capacities; 10 ways to ruin your experiment  AMP   11:30 Command line intro (Unix, R) \u2013 L(15\u2019) / P(30\u2019) Practical    12:30 Lunch     13:30 Raw data - FASTQ format and QC [Slides]    14:00 Alignment (L)  ST/GP   14:30 Practical: Manipulation of BAM files and QC  ST/GP   15:00 Coffee break     15:15 Practical: Manipulation of BAM files and QC (cont.)  ST/GP   17:00 Q&amp;A  All"},{"location":"timetables/timetable_cancer/#day-2","title":"Day 2","text":"<p>26th July - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     9:00 SNV detection (review, germline vs somatic, tools, pitfalls, data visualization) \u2013 L/P. Indels \u2013 cover in SNV lecture + bonus exercises (P), specific challenges of indels analysis, tools  ST   9:45 Practical: SNV detection  ST   10:30 Coffee break     10:45 Variants annotation and filtration (L) \u2013 tools landscape  ST   11:00 Practical: Variants visualization (IGV), annotation and filtration  ST   12:30 Lunch + coffee     13:30 CNV analysis using NGS data (L)  GP/AMP   14:15 Practical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation  GP/AMP   15:00 Break     15:15 Practical: CNV analysis \u2013 deletion/amplification, calling CNVs, visualization, interpretation (cont.)  GP/AMP   17:00 Q&amp;A  All"},{"location":"timetables/timetable_cancer/#day-3","title":"Day 3","text":"<p>27th July - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     9:00 SV analysis \u2013 breakpoints/fusion, tools  AMP   9:45 Practical: SV analysis \u2013 breakpoints/fusion  ET   10:30 Coffee break     10:45 Practical: SV analysis \u2013 breakpoints/fusion (cont.)  ET   12:30 Lunch     13:30 Downstream analysis and interpretation \u2013 Exploration of resources that can be used for this. E.g. databases (COSMIC, TCGA, etc.), integration with clinical information  AMP   14:15 Practical: Downstream analysis and interpretation  AMP   15:00 Coffee break     15:15 Practical: Downstream analysis and interpretation(cont.)  AMP   16:00 How does it all link together? Integration of different data types  Mark Cowley   16:45 Q&amp;A, wrap up (how to access course\u2019s VM) &amp; survey  All"},{"location":"timetables/timetable_introNGS/","title":"Introduction to NGS Data Analysis Workshop","text":"<p>The University of Sydney, NSW - 27th - 29th June 2017</p>"},{"location":"timetables/timetable_introNGS/#instructors","title":"Instructors","text":"<ul> <li>Katherine Champ (KC) - Bioplatforms Australia, Sydney</li> <li>Sonika Tyagi (ST) - AGRF, Melbourne</li> <li>Matthew Field (MF) - Australian National University/James Cook University, Cairns</li> <li>Xi (Sean) Li (SL) - Australian National University, Canberra</li> <li>Susan Corley (SC) - UNSW Systems Biology Initiative, Sydney</li> </ul>"},{"location":"timetables/timetable_introNGS/#timetable","title":"Timetable","text":""},{"location":"timetables/timetable_introNGS/#day-1-introduction-to-the-command-line-data-quality-alignment-chip-seq","title":"Day 1 - Introduction to the command line, data quality &amp; alignment &amp; ChIP-Seq","text":"<p>27th June - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     09:00 Introductions and course orientation  K   09:45 Practical: Introduction to the command line     10:15 Morning Tea     10:40 Practical: Introduction to the command line course and R course     11:20 Introduction to NGS- technology, data formats and introduction to quality control     12:30 Lunch     13:15 Quality control: Intro to practical     13:25 Practical: Quality control     14:05 Introduction to sequence alignment     14:15 Practical: Sequence alignment     15:00 Afternoon Tea     15:25 Introduction to ChIP-Seq     15:55 Practical: ChIP-Seq analysis - Peak calling and annotation     16:30 Q&amp;A and day 1 wrap-up  All"},{"location":"timetables/timetable_introNGS/#day-2-chip-seq-motif-analysis-and-rna-seq-analysis","title":"Day 2 - ChIP-Seq motif analysis and RNA-Seq analysis","text":"<p>28th June - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     09:00 Practical: Motif analysis     09:40 Introduction to RNA-Seq     10:30 Morning Tea     10:50 Practical: Alignment and splice junction identification     12:30 Lunch     13:30 Practical: Differential gene expression with Bio-conductor package: EdgeR and Voom     15:00 Afternoon Tea     15:30 Practical: Biological interpretation     16:30 Q&amp;A and day 2 wrap-up  All"},{"location":"timetables/timetable_introNGS/#day-3-de-novo-assembly","title":"Day 3 - de novo Assembly","text":"<p>29th June - Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW</p>    Time Topic Links Instructor     09:00 Introduction to de novo assembly     09:40 Practical: de novo assembly using Illumina reads     10:30 Morning Tea     10:50 Practical: de novo assembly using Illumina reads (cont.)     11:30 Practical: de novo assembly using PacBio \u2013 Canu workflow     12:30 Lunch     13:30 Practical: de novo assembly using PacBio \u2013 Canu workflow     15:30 Afternoon Tea     15:50 Practical: Polishing PacBio de novo assembly with Illumina reads     16:30 Q&amp;A and workshop wrap-up  All   17:00 Workshop Survey"},{"location":"timetables/timetable_longRead/","title":"Long-read Data Analysis Workshop","text":"<p>Sydney - 18th - 19th July 2017</p>"},{"location":"timetables/timetable_longRead/#instructors","title":"Instructors","text":"<ul> <li>Katherine Champ (KC) - Bioplatforms Australia, Sydney</li> <li>Tonia Russell (TR) - Ramaciotti Centre for Genomics, Sydney</li> <li>Anna Syme (AS) - Melbourne Bioinformatics, Melbourne</li> <li>Torsten Seemann (TS) - Melbourne Bioinformatics, Melbourne</li> <li>Nandan Deshpande - UNSW System Biology Initiative, Sydney</li> </ul>"},{"location":"timetables/timetable_longRead/#timetable","title":"Timetable","text":""},{"location":"timetables/timetable_longRead/#day-1","title":"Day 1","text":"<p>18th July - Room M020, The Red Centre, Kensington Campus, UNSW, Sydney</p>    Time Topic Links Instructor     9:00 Welcome and registration  Workshop Host and KC   9:30 Introduction to long-read sequencing (Lecture)  TS   10:30 Break     10:30 Long-read data \u2013 Practical considerations (Lecture)  TR   11:30 Introduction to Command-line (Practical)  ND   13:00 Lunch     14:00 Introduction to NGS \u2013 Technology, data formats and quality control  ND   14:30 Illumina de novo assembly (Velvet) (Practical)  ND   15:00 Coffee break     15:30 Illumina de novo assembly (Velvet) (Practical)  ND   17:00 Q&amp;A  All"},{"location":"timetables/timetable_longRead/#day-2","title":"Day 2","text":"<p>19th July - Room M020, The Red Centre, Kensington Campus, UNSW, Sydney</p>    Time Topic Links Instructor     9:00 Long-read data \u2013 Quality control (Practical)  TR   10:30 Coffee break     11:00 PacBio assembly + Illimuna polishing (Practical)  TS and AS   12:30 Lunch     13:30 PacBio assembly + Illimuna polishing (Continue) (Practical)  TS and AS   15:00 Break     15:30 Comparison of Illimuna and PacBio assembly  TS   16:45 Q&amp;A, wrap up (how to access course\u2019s VM) &amp; survey  All"},{"location":"trainers/trainers/","title":"The Trainers","text":"<p> Dr. Dan Andrews Bioinformatics Fellow, John Curtin School of Medical Research, Australian National University, Canberra   </p> <p> Ms. Katherine Champ Workshop Coordinator, Project Officer, Bioplatform Autralia Ltd.  </p> <p> Dr. Zhiliang Chen Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  </p> <p> Dr. Susan Corley Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  </p> <p> Dr. Nandan Deshpande Postdoctoral Research Associate, The University of New South Wales (UNSW), Sydney  </p> <p> Dr. Konsta Duesing Research Team Leader - Statistics &amp; Bioinformatics, CSIRO Animal, Food and Health Science, Sydney  </p> <p> Dr. Matthew Field Senior Research Fellow, Australian National University/James Cook University, Cairns</p> <p> Dr. Velimir Gayevskiy Translational Bioinformatics Officer, KCCG, Garvan Institute of Medical Research NSW  </p> <p> Paul Greenfield Principal Experimental Scientist, CSIRO, Sydney  </p> <p> Dr. Xi (Sean) Li The Australian National University, Canberra  </p> <p> Dr. Annette McGrath Principal Research Scientist, Team Leader, Life Science Informatics DATA61, CSIRO, Canberra  </p> <p> Mr. Sean McWilliam Bioinformatics Analyst, CSIRO Agriculture, Brisbane  </p> <p> Dr. Philippe Moncuquet Research Project Officer, Cotton Disease Markers, CSIRO, Canberra</p> <p> Dr. Paula Moolhuijzen Bioinformatics Analyst, Centre for Crop Disease Management, Curtin University, Perth  </p> <p> Dr. Ann-Marie Patch Senior Research Officer, Medical Genomics QIMR Berghofer Medical Research Institute, Brisbane  </p> <p> Dr. Gayle Philip Research Fellow (Bioinformatics), Melbourne Bioinformatics, Carlton, Melbourne  </p> <p> Mr. Jerico Revote Software Developer Monash eResearch Centre, Monash University, Clayton Melbourne  </p> <p> A/Prof. Torsten Seemann Lead Bioinformatician, Melbourne Bioinformatics and MDU-PHL, The University of Melbourne, VIC  </p> <p> Dr Anna Syme Bioinformatician, Melbourne Bioinformatics, Melbourne  </p> <p> Dr. Erdahl Teber Senior Research Officer (Bioinformatics), Children Medical Research Institute, Kids Cancer Alliance, University of Sydney, Sydney  </p> <p> Dr. Sonika Tyagi Bioinformatics Supervisor, Australian Genome Research Facility Ltd, The Walter and Eliza Hall Institute, Melbourne   </p> <p> Dr. Nathan S. Watson-Haigh Research Fellow in Bioinformatics, The Australian Centre for Plant Functional Genomics (ACPFG), Adelaide  </p>"}]}